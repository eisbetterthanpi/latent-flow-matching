{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/latent-flow-model/blob/main/flow_matching_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "vVWgSlV22V7M",
        "outputId": "74f67a91-f289-4e8c-dd5d-a016e0459294"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'zero_module' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8ece3d616801>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# sptf = SpatialTransformer(in_ch=32, n_heads=8, d_head=4, depth=1, drop=0., cond_dim=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0msptf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcond_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-8ece3d616801>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_ch, n_heads, d_head, depth, drop, cond_dim)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBasicTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzero_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m# self.proj_out = nn.Conv2d(d_model, in_ch, kernel_size=1, stride=1, padding=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# nn.init.zeros_(self.proj_out.weight)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'zero_module' is not defined"
          ]
        }
      ],
      "source": [
        "# @title SpatialTransformer down\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0.):\n",
        "        super().__init__()\n",
        "        d_model = d_head * n_heads\n",
        "        self.n_heads, self.d_head, self.d_model = n_heads, d_head, d_model\n",
        "        if cond_dim == None: cond_dim = query_dim\n",
        "        self.scale = d_head ** -0.5\n",
        "        self.n_heads = n_heads\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.k = nn.Linear(cond_dim, d_model, bias=False)\n",
        "        self.v = nn.Linear(cond_dim, d_model, bias=False)\n",
        "        self.lin = nn.Sequential(nn.Linear(d_model, query_dim), nn.Dropout(drop),)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, h*w, n_heads*d_head]\n",
        "        # print(\"crossattn fwd x\", x.shape)\n",
        "        h = self.n_heads\n",
        "        if cond==None: cond=x\n",
        "        Q, K, V = self.q(x), self.k(cond), self.v(cond)\n",
        "        Q, K, V = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (Q, K, V)) # [batch*n_heads, h*w, d_head]\n",
        "        sim = Q @ K.transpose(1, 2) * self.scale\n",
        "        if mask!=None: sim.masked_fill_(~mask.flatten(1).repeat(h,1).unsqueeze(1), -torch.finfo(sim.dtype).max) # -1e10, 3.4028234663852886e+38\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = attn @ V # [batch*n_heads, h*w, d_head]\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.lin(out) # [batch, h*w, n_heads*d_head]\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, h*w, n_heads*d_head]\n",
        "        h = self.n_heads\n",
        "        batch_size = x.shape[0]\n",
        "        if cond==None: cond=x\n",
        "        Q = self.q(x).reshape(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, n_heads, h*w, d_head]\n",
        "        K = self.k(cond).reshape(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.v(cond).reshape(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        sim = Q @ K.transpose(2, 3) * self.scale\n",
        "        # if mask!=None: sim.masked_fill_(~mask.unsqueeze(1).repeat(1,self.n_heads), -torch.finfo(sim.dtype).max) # -1e10, 3.4028234663852886e+38 [batch, h,w]?-> [batch,n_heads, h*w]?\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = attn @ V # [batch*n_heads, h*w, d_head]\n",
        "        out = out.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        return self.lin(out) # [batch, h*w, n_heads*d_head]\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_head, drop=0., cond_dim=None):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=d_model, n_heads=n_heads, d_head=d_head, drop=drop)  # is a self-attention\n",
        "        self.attn2 = CrossAttention(query_dim=d_model, cond_dim=cond_dim, n_heads=n_heads, d_head=d_head, drop=drop)  # is self-attn if cond is none\n",
        "        self.norm1, self.norm2, self.norm3 = nn.RMSNorm(d_model), nn.RMSNorm(d_model), nn.RMSNorm(d_model) # RMSNorm LayerNorm\n",
        "        inner_dim = d_model * 4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, inner_dim), nn.GELU(), # nn.GELU() GEGLU(d_model, inner_dim)\n",
        "            nn.Dropout(drop), nn.Linear(inner_dim, d_model),)\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), cond=cond) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "# norm selfatt res; norm crossatt res; norm lin res\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"Transformer block for image-like data.\n",
        "    First, project the input (aka embedding) and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\"\"\"\n",
        "    def __init__(self, in_ch, n_heads, d_head, depth=1, drop=0., cond_dim=None):\n",
        "        super().__init__()\n",
        "        self.cond=True\n",
        "        self.in_ch = in_ch\n",
        "        d_model = n_heads * d_head\n",
        "        self.norm = nn.GroupNorm(num_groups=min(32,in_ch), num_channels=in_ch)\n",
        "        self.proj_in = nn.Conv2d(in_ch, d_model, kernel_size=1, stride=1, padding=0)\n",
        "        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(d_model, n_heads, d_head, drop=drop, cond_dim=cond_dim) for d in range(depth)])\n",
        "        self.proj_out = zero_module(nn.Conv2d(d_model, in_ch, kernel_size=1, stride=1, padding=0))\n",
        "        # self.proj_out = nn.Conv2d(d_model, in_ch, kernel_size=1, stride=1, padding=0)\n",
        "        # nn.init.zeros_(self.proj_out.weight)\n",
        "\n",
        "    def forward(self, x, cond=None): # note: if no cond is given, cross-attention defaults to self-attention\n",
        "        b, c, h, w = x.shape\n",
        "        if cond!=None: cond = cond.unsqueeze(1).repeat(1,self.in_ch,1)\n",
        "        x_in = x\n",
        "        x = self.proj_in(self.norm(x))\n",
        "        x = x.flatten(2).transpose(1, 2) # b c h w -> b (h w) c\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, cond=cond)\n",
        "        x = x.transpose(1, 2).reshape(b, -1, h, w) # [b,d_model, h,w]\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "\n",
        "batch=4\n",
        "in_ch=32 # assert in_ch % 32 == 0 for groupnorm32\n",
        "cond_dim=16\n",
        "# h=w=16\n",
        "h,w=3,5\n",
        "n_heads, d_head = 2, 8\n",
        "# sptf = SpatialTransformer(in_ch=32, n_heads=8, d_head=4, depth=1, drop=0., cond_dim=16)\n",
        "sptf = SpatialTransformer(in_ch, n_heads, d_head, depth=1, cond_dim=cond_dim)\n",
        "x = torch.rand(batch,in_ch,h,w)\n",
        "cond = torch.rand(batch,cond_dim)\n",
        "# cond = torch.rand(batch,cond_dim).unsqueeze(1).repeat(1,in_ch,1)\n",
        "out = sptf(x, cond)\n",
        "print(out.shape) # batch,in_ch,h,w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Pm-Fw6jn4A",
        "outputId": "44cba435-cab5-4343-de0f-fd0eb9fef55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 5, 6])\n"
          ]
        }
      ],
      "source": [
        "# @title mha me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, dropout=0):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, query_dim=None, cond_dim=None, dropout=0):\n",
        "    # def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # if query_dim == None: query_dim = d_model\n",
        "        if cond_dim == None: cond_dim = d_model\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(cond_dim, d_model, bias=False)\n",
        "        self.v = nn.Linear(cond_dim, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    # def forward(self, query, key, value, mask=None):\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model/cond_dim] or [batch, h*w, c]\n",
        "        batch = x.shape[0]\n",
        "        if cond==None: cond=x # equivalent to self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        K = self.k(cond).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.v(cond).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch, n_heads, T]\n",
        "        if mask is not None: attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        # if mask!=None: attn*=mask.int() # attn.masked_fill_(~mask.unsqueeze(1).repeat(1,self.n_heads), -torch.finfo(attn.dtype).max) # -1e10, 3.4028234663852886e+38 [batch, h,w]?-> [batch,n_heads, h*w]?\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        x = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "        x = x.transpose(1, 2).reshape(batch, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x, attention # [batch, T, d_model],\n",
        "\n",
        "\n",
        "class selfattnblock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.self_attn = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,c,h,w]\n",
        "        b,c,h,w = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        x = self.norm1(x + self.drop(self.self_attn(x)[0]))\n",
        "        x = self.norm2(x + self.drop(self.ff(x)))\n",
        "        return x.transpose(1,2).reshape(b,c,h,w)\n",
        "\n",
        "class crossattnblock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [b,cond_dim]\n",
        "        b,c,h,w = x.shape\n",
        "        if cond!=None: cond = cond.unsqueeze(1).repeat(1,self.d_model,1)\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        x = self.norm1(x + self.drop(self.cross_attn(x, cond)[0]))\n",
        "        x = self.norm2(x + self.drop(self.ff(x)))\n",
        "        return x.transpose(1,2).reshape(b,c,h,w)\n",
        "\n",
        "\n",
        "ch=8\n",
        "batch=4\n",
        "x=torch.rand(batch,ch,5,6)\n",
        "# # attn = AttentionBlock(ch, n_heads=1, d_head=-1)\n",
        "# attn = selfattnblock(ch,4)\n",
        "# out = attn(x)\n",
        "cond_dim=10\n",
        "cross = crossattnblock(ch,4,cond_dim)\n",
        "cond=torch.rand(batch,cond_dim)\n",
        "out = cross(x, cond)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8V_kbZc8imad"
      },
      "outputs": [],
      "source": [
        "# @title stable diffusion util\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/util.py\n",
        "# adopted from\n",
        "# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n",
        "# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n",
        "# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import repeat\n",
        "\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False): # [batch], [temb_dim]\n",
        "    \"\"\"\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        # freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=device)\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half) / half).to(device)\n",
        "        # args = timesteps[:, None].float() * freqs[None]\n",
        "        args = timesteps[:, None] * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2: embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    else: embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
        "    return embedding # [batch, temb_dim]\n",
        "\n",
        "# # timesteps = torch.arange(0, 10)\n",
        "# timesteps = torch.linspace(0, 1, 10)\n",
        "# print(timesteps)\n",
        "# model_channels=8\n",
        "# t_emb = timestep_embedding(timesteps, model_channels, repeat_only=False)\n",
        "# print(t_emb)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "def conv_nd(dims, *args, **kwargs):\n",
        "    \"\"\"Create a 1D, 2D, or 3D convolution module.\"\"\"\n",
        "    if dims == 1: return nn.Conv1d(*args, **kwargs)\n",
        "    elif dims == 2: return nn.Conv2d(*args, **kwargs)\n",
        "    elif dims == 3: return nn.Conv3d(*args, **kwargs)\n",
        "\n",
        "\n",
        "def avg_pool_nd(dims, *args, **kwargs):\n",
        "    \"\"\"Create a 1D, 2D, or 3D average pooling module.\"\"\"\n",
        "    if dims == 1: return nn.AvgPool1d(*args, **kwargs)\n",
        "    elif dims == 2: return nn.AvgPool2d(*args, **kwargs)\n",
        "    elif dims == 3: return nn.AvgPool3d(*args, **kwargs)\n",
        "\n",
        "# conv_nd,\n",
        "# avg_pool_nd,\n",
        "# zero_module,\n",
        "# normalization, # nn.GroupNorm\n",
        "# timestep_embedding,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j9wUA7Vk0Y03"
      },
      "outputs": [],
      "source": [
        "# @title RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2)\n",
        "\n",
        "# rotemb = RotEmb(10)\n",
        "# seq_len=10\n",
        "# pos = torch.linspace(0,1,seq_len).to(device)#.unsqueeze(-1)\n",
        "# rot_emb = rotemb(pos)\n",
        "# print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8YwHsHFURq0",
        "outputId": "37513ab7-8758-4750-8fb9-c7a3074ed22e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out.0.weight Parameter containing:\n",
            "tensor([1.0139], requires_grad=True)\n",
            "out.0.bias Parameter containing:\n",
            "tensor([0.0390], requires_grad=True)\n",
            "out.2.weight Parameter containing:\n",
            "tensor([[[[ 0.3486,  0.3104,  0.1254],\n",
            "          [-0.0805,  0.0014,  0.3263],\n",
            "          [-0.2240, -0.0140,  0.0595]]]], requires_grad=True)\n",
            "out.2.bias Parameter containing:\n",
            "tensor([-0.1123], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# @title test init zero\n",
        "\n",
        "class Me(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # self.out = nn.Sequential(nn.SiLU(), conv_nd(2, in_ch=1, out_ch=1, stride=3, padding=1))\n",
        "        self.out = nn.Sequential(nn.GroupNorm(1,1),nn.SiLU(), conv_nd(2, 1, 1, 3, padding=1))\n",
        "        # self.in_ch = in_ch\n",
        "        # for weight in self.out.parameters(): torch.nn.init.zeros_(weight)\n",
        "    def forward(self, x):\n",
        "        return self.out(x)\n",
        "\n",
        "# model = Me()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "x = torch.rand(4,1,16,16)\n",
        "y = torch.rand(4,1,16,16)\n",
        "with torch.amp.autocast('cuda'):\n",
        "    loss = F.mse_loss(model(x), y)\n",
        "# print(loss)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "# scaler.scale(loss).backward()\n",
        "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "# scaler.step(optim)\n",
        "# scaler.update()\n",
        "for name, param in model.named_parameters(): print(name, param)\n",
        "\n",
        "# x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# # print(t)\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# # [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "# print(x.shape,t.shape,cond.shape)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "# # optim = torch.optim.AdamW(model.parameters(), lr=1e-5) # 1e-3 3e-3\n",
        "# optim = torch.optim.SGD(model.parameters(), lr=1e-5) # 1e-3 3e-3\n",
        "\n",
        "# cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "\n",
        "\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMFVOgT4sJ-y",
        "outputId": "8fb41ce8-4620-471f-84f5-9825389f2142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3300625\n",
            "torch.Size([4, 1, 16, 16]) torch.Size([4]) torch.Size([4, 10])\n",
            "torch.Size([4, 1, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title stable diffusion unet down\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x): # [N,C,...]\n",
        "        # if self.conv_dim == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        x = self.conv(x) # optional\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        # stride = 2 if conv_dim != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        self.op = nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1) # optional\n",
        "        # self.op = avg_pool_nd(conv_dim, kernel_size=stride, stride=stride) # alternative\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial]\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, temb_dim, out_ch=None, scale_shift=False, updown=False, drop=0.):\n",
        "        super().__init__()\n",
        "        self.temb_dim = temb_dim # number of timestep embedding channels\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch = in_ch, out_ch\n",
        "        self.scale_shift = scale_shift\n",
        "        if updown=='up': self.h_upd, self.x_upd = Upsample(in_ch), Upsample(in_ch)\n",
        "        elif updown=='down': self.h_upd, self.x_upd = Downsample(in_ch), Downsample(in_ch)\n",
        "        else: self.h_upd = self.x_upd = nn.Identity()\n",
        "        # self.in_layers = nn.Sequential(nn.GroupNorm(min(32,in_ch), in_ch), nn.SiLU(), self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1),)\n",
        "        self.in_layers = nn.Sequential(nn.SiLU(), self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1),)\n",
        "\n",
        "        self.emb_layers = nn.Sequential(nn.SiLU(), nn.Linear(temb_dim, 2 * out_ch if scale_shift else out_ch),)\n",
        "        self.out_layers = nn.Sequential(\n",
        "            # nn.GroupNorm(min(32,out_ch), out_ch), nn.SiLU(), nn.Dropout(drop), zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "            nn.SiLU(), nn.Dropout(drop), nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        if out_ch == in_ch: self.skip = nn.Identity() # no need to change chanels\n",
        "        else:\n",
        "            # self.skip = nn.Conv2d(in_ch, out_ch, 3, padding=1) # spatial convolution to change the channels in the skip connection\n",
        "            self.skip = nn.Conv2d(in_ch, out_ch, 1) # smaller 1x1 convolution to change the channels in the skip connection\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, temb_dim]\n",
        "        # print(\"res fwd x\", x.shape, self.in_ch, self.out_ch)\n",
        "        h = self.in_layers(x) # norm, act, h_upd, conv\n",
        "        x = self.x_upd(x)\n",
        "        emb_out = self.emb_layers(emb) # act, lin\n",
        "        # print(\"res fwd h emb_out\", h.shape, emb_out.shape)\n",
        "        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]\n",
        "        if self.scale_shift: # FiLM\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return h + self.skip(x) # [N, C, ...]\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, updown=False, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            # Downsample(in_ch, out_ch) if updown=='down' else nn.Identity(),\n",
        "            Downsample(in_ch) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch),\n",
        "            # SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            crossattnblock(out_ch, d_head, cond_dim),\n",
        "            Upsample(out_ch) if updown=='up' else nn.Identity(),\n",
        "            # Upsample(in_ch, out_ch) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, model_ch=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.model_ch = model_ch # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = model_ch // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(model_ch)\n",
        "        temb_dim = model_ch# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(model_ch, temb_dim), nn.SiLU(), nn.Linear(temb_dim, temb_dim))\n",
        "\n",
        "        self.input_blocks = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1))\n",
        "\n",
        "        ch_list = [model_ch*2**i for i in range(depth+1)] # [32, 64, 128, 256]\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = 2*ch_list[-1] # 512\n",
        "        self.middle_block = Seq(\n",
        "            Downsample(ch_list[-1]),\n",
        "            ResBlock(ch_list[-1], temb_dim, ch),\n",
        "            # SpatialTransformer(ch, ch//d_head, d_head, cond_dim=cond_dim),\n",
        "            crossattnblock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, temb_dim, ch_list[-1]),\n",
        "            Upsample(ch_list[-1]),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        # self.out = nn.Sequential(nn.GroupNorm(min(32,model_ch), model_ch), nn.SiLU(), zero_module(nn.Conv2d(model_ch, out_ch, 3, padding=1)))\n",
        "        # self.out = nn.Sequential(nn.GroupNorm(min(32,model_ch), model_ch), nn.SiLU(), nn.Conv2d(model_ch, out_ch, 3, padding=1))\n",
        "        self.out = nn.Sequential(nn.SiLU(), nn.Conv2d(model_ch, out_ch, 3, padding=1))\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        # t_emb = timestep_embedding(t, self.model_ch, repeat_only=False)\n",
        "        emb = self.time_emb(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, temb_dim)\n",
        "        blocks = []\n",
        "        x = self.input_blocks(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, model_ch=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# x=torch.rand((batch,3,16,16),device=device)\n",
        "x=torch.rand((batch,1,16,16),device=device)\n",
        "y=torch.rand((batch,1,16,16),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# print(t)\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "# [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "print(x.shape,t.shape,cond.shape)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-5) # 1e-3 3e-3\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-5) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ1hbnSC12tA",
        "outputId": "b5b6c590-00c4-4088-e832-80516bf18bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56192\n",
            "torch.Size([4, 64, 16, 16])\n",
            "56003\n",
            "torch.Size([4, 3, 60, 60])\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_list=[32, 64, 128, 256], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.Dropout2d(drop), nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), act,\n",
        "            # nn.Dropout2d(drop), nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), act,\n",
        "        )\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(in_ch, d_list[0], k_list[0], 2, k_list[0]//2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.Dropout2d(drop), nn.Conv2d(d_list[0], d_list[1], k_list[1], 2, k_list[1]//2), nn.BatchNorm2d(d_list[1]), act,\n",
        "            # nn.Dropout2d(drop), nn.Conv2d(d_list[1], d_list[2], k_list[2], 2, k_list[2]//2), nn.BatchNorm2d(d_list[2]), act,\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x) # [batch, 3,64,64] -> [batch, c,h,w]\n",
        "\n",
        "class Deconv(nn.Module):\n",
        "    # def __init__(self, d_list=[32, 64, 128, 256], act=nn.GELU()): # ReLU GELU SiLU\n",
        "    def __init__(self, out_ch=3, d_list=[32, 64, 128, 256], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            # nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), act,\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            # nn.ConvTranspose2d(d_list[2], d_list[1], k_list[2], 2, k_list[0]//2, output_padding=1), nn.BatchNorm2d(d_list[1]), act,\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], k_list[1], 2, k_list[0]//2, output_padding=1), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.ConvTranspose2d(d_list[0], out_ch, k_list[0], 2, k_list[0]//2, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): return self.deconv(x) # [batch, c,h,w] -> [batch, 3,64,64]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "enc = conv(input)\n",
        "print(enc.shape)\n",
        "\n",
        "deconv = Deconv().to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "out = deconv(enc)\n",
        "print(out.shape)\n",
        "print(out.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKm_CrSNowqA",
        "outputId": "5c4badc4-66eb-4c5a-dc3e-8bcedd3882fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand((4,3,64,64), device=device)\n",
        "enc = conv(input)\n",
        "print(enc.dtype)\n",
        "out = deconv(enc)\n",
        "print(out.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nvBRca98P-GW"
      },
      "outputs": [],
      "source": [
        "# @title latent flow model\n",
        "class LFM(nn.Module): # latent flow model\n",
        "    def __init__(self, in_ch=3, d_list=[16, 16], model_ch=16, cond_dim=16, depth=3):\n",
        "        super().__init__()\n",
        "        d_list=[16, 16] # [16,32]\n",
        "        k_list=[3,3] # [7,5]\n",
        "        self.conv = Conv(in_ch=in_ch, d_list=d_list, k_list=k_list, act=nn.ReLU()) # ReLU GELU SiLU\n",
        "        self.deconv = Deconv(out_ch=in_ch, d_list=d_list, k_list=k_list, act=nn.ReLU()) # ReLU GELU SiLU\n",
        "        self.unet = UNet(in_ch=d_list[-1], model_ch=16, cond_dim=cond_dim, depth=3)\n",
        "\n",
        "    def loss(self, img, cond):\n",
        "        x1 = self.conv(img)\n",
        "        img_ = self.deconv(x1)\n",
        "        # print(img_.cpu()[:4].shape, img.dtype, img_.dtype, x1.dtype)\n",
        "        imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        fm_loss = otfm_loss(self.unet, x1, cond)\n",
        "        return ae_loss, fm_loss\n",
        "        # loss = ae_loss + fm_loss\n",
        "        # return loss\n",
        "\n",
        "\n",
        "    # def forward(self, x, t=None, cond=None): # [N, C, ...]\n",
        "\n",
        "\n",
        "    def sample(self, cond, n_samples = 1):\n",
        "        self.eval()\n",
        "        # cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).to(torch.float)\n",
        "        with torch.no_grad():\n",
        "            x1_ = reverse_flow(self.unet, cond, n_samples, timesteps=25)\n",
        "            img_ = self.deconv(x1_)\n",
        "        return img_\n",
        "\n",
        "\n",
        "# # cond = cond_emb(torch.tensor([4], device=device))\n",
        "# cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).to(torch.float)\n",
        "\n",
        "# n_samples = 1\n",
        "\n",
        "model = LFM(d_list=[16, 16], model_ch=16, cond_dim=10, depth=3).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=3e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))])\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "batch_size = 64 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvgOmsCPaCk"
      },
      "outputs": [],
      "source": [
        "optim.param_groups[0][\"lr\"] = 1e-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2bipgghY7O",
        "outputId": "530df932-f104-4133-dfd4-85be453525ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py:56: UserWarning: <class '__main__.LogitNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title LogitNormal\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "class LogitNormal(dist.Distribution):\n",
        "    def __init__(self, mu=0, std=.5):\n",
        "        super().__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "        self._normal = dist.Normal(mu, std) # https://pytorch.org/docs/stable/distributions.html#normal\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        eps = self._normal.rsample(sample_shape)\n",
        "        return torch.sigmoid(eps) # https://en.wikipedia.org/wiki/Logit-normal_distribution\n",
        "\n",
        "logit_normal = LogitNormal()\n",
        "# samples = logit_normal.rsample((10,))\n",
        "# print(samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfsabyM8P7q3",
        "outputId": "076fe4bc-3c2d-4266-e4d6-04c13134cf7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-125-139244bc52fc>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.6224094033241272\n",
            "10 0.5929259657859802\n",
            "20 0.6004299521446228\n",
            "30 0.5874409675598145\n",
            "40 0.5604001879692078\n",
            "50 0.5329011678695679\n",
            "60 0.5882507562637329\n",
            "70 0.5708752274513245\n",
            "80 0.5338509678840637\n",
            "90 0.5581578016281128\n",
            "100 0.5736000537872314\n",
            "110 0.5490924715995789\n",
            "120 0.5783054828643799\n",
            "130 0.5895012021064758\n",
            "140 0.5752569437026978\n",
            "150 0.5479761958122253\n",
            "160 0.6009811758995056\n",
            "170 0.6006122827529907\n",
            "180 0.6243885159492493\n",
            "190 0.6196404099464417\n",
            "200 0.5943436622619629\n",
            "210 0.6072250008583069\n",
            "220 0.5927649140357971\n",
            "230 0.5782375335693359\n",
            "240 0.5660936236381531\n",
            "250 0.5824975371360779\n",
            "260 0.5805423259735107\n",
            "270 0.6148970723152161\n",
            "280 0.5176538228988647\n",
            "290 0.5623419284820557\n",
            "300 0.5711844563484192\n",
            "310 0.5683111548423767\n",
            "320 0.5673784613609314\n",
            "330 0.5877169370651245\n",
            "340 0.5384754538536072\n",
            "350 0.5553321838378906\n",
            "360 0.5681657195091248\n",
            "370 0.5764341354370117\n",
            "380 0.5612912774085999\n",
            "390 0.6073899865150452\n",
            "400 0.5625343918800354\n",
            "410 0.5752730965614319\n",
            "420 0.6011112332344055\n",
            "430 0.6222236156463623\n",
            "440 0.6022807955741882\n",
            "450 0.605353832244873\n",
            "460 0.5816993713378906\n",
            "470 0.5712974667549133\n",
            "480 0.6091875433921814\n",
            "490 0.5624975562095642\n",
            "500 0.5740593671798706\n",
            "510 0.6012946963310242\n",
            "520 0.5548645853996277\n",
            "530 0.5734461545944214\n",
            "540 0.5772688984870911\n",
            "550 0.6040660738945007\n",
            "560 0.5967638492584229\n",
            "570 0.5512776970863342\n",
            "580 0.5860303044319153\n",
            "590 0.5671300888061523\n",
            "600 0.5783510804176331\n",
            "610 0.5362353920936584\n",
            "620 0.5956037640571594\n",
            "630 0.5856655836105347\n",
            "640 0.5673244595527649\n",
            "650 0.5461161732673645\n",
            "660 0.5948452353477478\n",
            "670 0.5512356758117676\n",
            "680 0.5965660810470581\n",
            "690 0.5756776928901672\n",
            "700 0.590754508972168\n",
            "710 0.5964550375938416\n",
            "720 0.5891627669334412\n",
            "730 0.5345221161842346\n",
            "740 0.5699091553688049\n",
            "750 0.6091111302375793\n",
            "760 0.6030790209770203\n",
            "770 0.5756890773773193\n",
            "780 0.6167686581611633\n",
            "790 0.5848245620727539\n",
            "800 0.5837940573692322\n",
            "810 0.5876567959785461\n",
            "820 0.5566623210906982\n",
            "830 0.5536861419677734\n",
            "840 0.5696287155151367\n",
            "850 0.559194028377533\n",
            "860 0.592623233795166\n",
            "870 0.5930585265159607\n",
            "880 0.567115843296051\n",
            "890 0.5500354170799255\n",
            "900 0.5860024094581604\n",
            "910 0.5780065059661865\n",
            "920 0.5754759907722473\n",
            "930 0.580118715763092\n",
            "0 0.5840510129928589\n",
            "10 0.5548951029777527\n",
            "20 0.5984499454498291\n",
            "30 0.5978128910064697\n",
            "40 0.5681077837944031\n",
            "50 0.5637930035591125\n",
            "60 0.553740382194519\n",
            "70 0.5922209024429321\n",
            "80 0.5871165990829468\n",
            "90 0.5938456058502197\n",
            "100 0.5668295621871948\n",
            "110 0.6131398677825928\n",
            "120 0.5682414174079895\n",
            "130 0.5747825503349304\n",
            "140 0.5887671113014221\n",
            "150 0.5772864818572998\n",
            "160 0.5669844746589661\n",
            "170 0.5608163475990295\n",
            "180 0.564245343208313\n",
            "190 0.5589145421981812\n",
            "200 0.576656699180603\n",
            "210 0.5725315809249878\n",
            "220 0.5670358538627625\n",
            "230 0.5548630356788635\n",
            "240 0.5402218103408813\n",
            "250 0.5444844365119934\n",
            "260 0.5894655585289001\n",
            "270 0.5398906469345093\n",
            "280 0.5368464589118958\n",
            "290 0.5459128618240356\n",
            "300 0.5537322759628296\n",
            "310 0.5416100025177002\n",
            "320 0.5602415800094604\n",
            "330 0.5648522973060608\n",
            "340 0.5751168131828308\n",
            "350 0.5583832263946533\n",
            "360 0.596627414226532\n",
            "370 0.5648714900016785\n",
            "380 0.5804558992385864\n",
            "390 0.5725946426391602\n",
            "400 0.5958389043807983\n",
            "410 0.5445551872253418\n",
            "420 0.5715616345405579\n",
            "430 0.6207302808761597\n",
            "440 0.5692715048789978\n",
            "450 0.5802003741264343\n",
            "460 0.5594179630279541\n",
            "470 0.5724086165428162\n",
            "480 0.5337479710578918\n",
            "490 0.5596224665641785\n",
            "500 0.5818982124328613\n",
            "510 0.5661225318908691\n",
            "520 0.5659111142158508\n",
            "530 0.5594473481178284\n",
            "540 0.5133081078529358\n",
            "550 0.5663559436798096\n",
            "560 0.5312681198120117\n",
            "570 0.5559424757957458\n",
            "580 0.5613186359405518\n",
            "590 0.5280444622039795\n",
            "600 0.5245817303657532\n",
            "610 0.5235013365745544\n",
            "620 0.5327982902526855\n",
            "630 0.5228046178817749\n",
            "640 0.5821353793144226\n",
            "650 0.553317666053772\n",
            "660 0.5767607688903809\n",
            "670 0.5449058413505554\n",
            "680 0.5597612261772156\n",
            "690 0.5786275267601013\n",
            "700 0.545351505279541\n",
            "710 0.5590115189552307\n",
            "720 0.566028356552124\n",
            "730 0.5923706889152527\n",
            "740 0.5831032991409302\n",
            "750 0.5640352964401245\n",
            "760 0.5483382344245911\n",
            "770 0.5774233341217041\n",
            "780 0.5833974480628967\n",
            "790 0.5672743916511536\n",
            "800 0.5380541086196899\n",
            "810 0.5658090710639954\n",
            "820 0.5605682134628296\n",
            "830 0.5263718366622925\n",
            "840 0.5580700635910034\n",
            "850 0.5560777187347412\n",
            "860 0.5620765089988708\n",
            "870 0.5435500741004944\n",
            "880 0.5370298624038696\n",
            "890 0.524933934211731\n",
            "900 0.5304872393608093\n",
            "910 0.5636930465698242\n",
            "920 0.5523794293403625\n",
            "930 0.5391642451286316\n",
            "0 0.5253111124038696\n",
            "10 0.5232069492340088\n",
            "20 0.5457428693771362\n",
            "30 0.5463939309120178\n",
            "40 0.5429127216339111\n",
            "50 0.5748404860496521\n",
            "60 0.5285197496414185\n",
            "70 0.533897340297699\n",
            "80 0.5532268285751343\n",
            "90 0.5722600221633911\n",
            "100 0.5641974806785583\n",
            "110 0.5810492634773254\n",
            "120 0.572124183177948\n",
            "130 0.5665280222892761\n",
            "140 0.586027204990387\n",
            "150 0.554857075214386\n",
            "160 0.5309503674507141\n",
            "170 0.5498799085617065\n",
            "180 0.5526004433631897\n",
            "190 0.5669366121292114\n",
            "200 0.5236221551895142\n",
            "210 0.5600475072860718\n",
            "220 0.5465715527534485\n",
            "230 0.5554711818695068\n",
            "240 0.5733523368835449\n",
            "250 0.5496849417686462\n",
            "260 0.5358830094337463\n",
            "270 0.5195784568786621\n",
            "280 0.5618596076965332\n",
            "290 0.5550765991210938\n",
            "300 0.5641034841537476\n",
            "310 0.5498430728912354\n",
            "320 0.5281530618667603\n",
            "330 0.5511634349822998\n",
            "340 0.5681473016738892\n",
            "350 0.5231945514678955\n",
            "360 0.5699599385261536\n",
            "370 0.543087899684906\n",
            "380 0.5442695617675781\n",
            "390 0.5425952076911926\n",
            "400 0.5343753695487976\n",
            "410 0.5312790274620056\n",
            "420 0.5247122049331665\n",
            "430 0.5514951348304749\n",
            "440 0.5691325068473816\n",
            "450 0.5598928332328796\n",
            "460 0.5515973567962646\n",
            "470 0.5068724155426025\n",
            "480 0.5052640438079834\n",
            "490 0.5367004871368408\n",
            "500 0.5330872535705566\n",
            "510 0.5219623446464539\n",
            "520 0.544232189655304\n",
            "530 0.5495896339416504\n",
            "540 0.5374679565429688\n",
            "550 0.5326699018478394\n",
            "560 0.5510727763175964\n",
            "570 0.5300145149230957\n",
            "580 0.5527281165122986\n",
            "590 0.539863109588623\n",
            "600 0.5501406788825989\n",
            "610 0.538180410861969\n",
            "620 0.5399104356765747\n",
            "630 0.5190723538398743\n",
            "640 0.5370306968688965\n",
            "650 0.5373350381851196\n",
            "660 0.5343787670135498\n",
            "670 0.5427024960517883\n",
            "680 0.5746204257011414\n",
            "690 0.579442024230957\n",
            "700 0.5871220827102661\n",
            "710 0.5516802072525024\n",
            "720 0.5507053136825562\n",
            "730 0.533686101436615\n",
            "740 0.522568941116333\n",
            "750 0.5382484793663025\n",
            "760 0.542332112789154\n",
            "770 0.5766931772232056\n",
            "780 0.5142420530319214\n",
            "790 0.5117296576499939\n",
            "800 0.5407812595367432\n",
            "810 0.4808303713798523\n",
            "820 0.5089913606643677\n",
            "830 0.5191217064857483\n",
            "840 0.5119379162788391\n",
            "850 0.5094083547592163\n",
            "860 0.5219712853431702\n",
            "870 0.555038332939148\n",
            "880 0.5481874942779541\n",
            "890 0.5090265274047852\n",
            "900 0.5189604759216309\n",
            "910 0.5540677905082703\n",
            "920 0.5210400819778442\n",
            "930 0.5196428894996643\n",
            "0 0.5374801754951477\n",
            "10 0.540187418460846\n",
            "20 0.5232715010643005\n",
            "30 0.5457833409309387\n",
            "40 0.5722209215164185\n",
            "50 0.5677053928375244\n",
            "60 0.5515521764755249\n",
            "70 0.5558785796165466\n",
            "80 0.5668254494667053\n",
            "90 0.5545240044593811\n",
            "100 0.5812166929244995\n",
            "110 0.5233545899391174\n",
            "120 0.5352163910865784\n",
            "130 0.551647961139679\n",
            "140 0.5368645787239075\n",
            "150 0.5193533897399902\n",
            "160 0.5420087575912476\n",
            "170 0.5162956714630127\n",
            "180 0.5178849697113037\n",
            "190 0.5184389352798462\n",
            "200 0.5195333957672119\n",
            "210 0.5153459906578064\n",
            "220 0.5010448694229126\n",
            "230 0.546089231967926\n",
            "240 0.4958128035068512\n",
            "250 0.5654107928276062\n",
            "260 0.5000491142272949\n",
            "270 0.49199172854423523\n",
            "280 0.5123674869537354\n",
            "290 0.5313757658004761\n",
            "300 0.5350614786148071\n",
            "310 0.5030765533447266\n",
            "320 0.520465612411499\n",
            "330 0.5403493642807007\n",
            "340 0.5668449401855469\n",
            "350 0.5364124178886414\n",
            "360 0.5295677781105042\n",
            "370 0.5293464064598083\n",
            "380 0.5375979542732239\n",
            "390 0.5431953072547913\n",
            "400 0.5503169298171997\n",
            "410 0.5049643516540527\n",
            "420 0.5426372289657593\n",
            "430 0.5357720851898193\n",
            "440 0.57521653175354\n",
            "450 0.5526785850524902\n",
            "460 0.5630179643630981\n",
            "470 0.5203115940093994\n",
            "480 0.5528402924537659\n",
            "490 0.5583629012107849\n",
            "500 0.5353419780731201\n",
            "510 0.5447731018066406\n",
            "520 0.522718608379364\n",
            "530 0.536310613155365\n",
            "540 0.5285226702690125\n",
            "550 0.5180517435073853\n",
            "560 0.5336036086082458\n",
            "570 0.5025407075881958\n",
            "580 0.5149357914924622\n",
            "590 0.5356826186180115\n",
            "600 0.5314645767211914\n",
            "610 0.5102925896644592\n",
            "620 0.5352954864501953\n",
            "630 0.48202821612358093\n",
            "640 0.5208844542503357\n",
            "650 0.5676848292350769\n",
            "660 0.5377061367034912\n",
            "670 0.5360069274902344\n",
            "680 0.5082289576530457\n",
            "690 0.5203717947006226\n",
            "700 0.5195377469062805\n",
            "710 0.5369592308998108\n",
            "720 0.5535900592803955\n",
            "730 0.5290825366973877\n",
            "740 0.5197312831878662\n",
            "750 0.5197957158088684\n",
            "760 0.545069694519043\n",
            "770 0.5099767446517944\n",
            "780 0.5029261112213135\n",
            "790 0.5350030660629272\n",
            "800 0.5420650839805603\n",
            "810 0.6023755073547363\n",
            "820 0.5045253038406372\n",
            "830 0.5655894875526428\n",
            "840 0.5174590945243835\n",
            "850 0.5419620275497437\n",
            "860 0.5167216658592224\n",
            "870 0.5284444689750671\n",
            "880 0.5167956948280334\n",
            "890 0.501778781414032\n",
            "900 0.49645403027534485\n",
            "910 0.5034267902374268\n",
            "920 0.5225751399993896\n",
            "930 0.5267428159713745\n",
            "0 0.5139488577842712\n",
            "10 0.514967143535614\n",
            "20 0.519426703453064\n",
            "30 0.49499350786209106\n",
            "40 0.5028982758522034\n",
            "50 0.4817506670951843\n",
            "60 0.51680988073349\n",
            "70 0.5273165106773376\n",
            "80 0.5400795936584473\n",
            "90 0.5203473567962646\n",
            "100 0.4914665222167969\n",
            "110 0.5123283863067627\n",
            "120 0.5336692333221436\n",
            "130 0.5270575881004333\n",
            "140 0.5240209102630615\n",
            "150 0.5210642218589783\n",
            "160 0.5252732038497925\n",
            "170 0.5134907960891724\n",
            "180 0.515018105506897\n",
            "190 0.5206523537635803\n",
            "200 0.509692907333374\n",
            "210 0.5132272839546204\n",
            "220 0.530031681060791\n",
            "230 0.48333168029785156\n",
            "240 0.5283942818641663\n",
            "250 0.522706151008606\n",
            "260 0.4976343810558319\n",
            "270 0.5284304022789001\n",
            "280 0.540307343006134\n",
            "290 0.5370086431503296\n",
            "300 0.535688042640686\n",
            "310 0.5173775553703308\n",
            "320 0.5136584043502808\n",
            "330 0.5180227160453796\n",
            "340 0.5003217458724976\n",
            "350 0.5275351405143738\n",
            "360 0.5093056559562683\n",
            "370 0.5371423959732056\n",
            "380 0.49543195962905884\n",
            "390 0.4977608919143677\n",
            "400 0.4942702054977417\n",
            "410 0.5065721273422241\n",
            "420 0.5297134518623352\n",
            "430 0.5219542980194092\n",
            "440 0.5108202695846558\n",
            "450 0.4959961771965027\n",
            "460 0.5414442420005798\n",
            "470 0.5118706226348877\n",
            "480 0.5325116515159607\n",
            "490 0.5185695886611938\n",
            "500 0.545150637626648\n",
            "510 0.5270023345947266\n",
            "520 0.5039985179901123\n",
            "530 0.5028097629547119\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def otfm_loss(model, x1, cond, sig_min = 0.001, eps = 1e-5): # https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "    batch = x1.size(0)\n",
        "    # t = torch.rand((batch,), device=device) % (1 - eps)\n",
        "    t = logit_normal.rsample((batch,)).to(device)\n",
        "    t_ = t[...,None,None,None]\n",
        "    x0 = torch.randn_like(x1)\n",
        "    # print(\"otfm_loss\", x0.shape, t.shape)\n",
        "    psi_t = (1 - (1-sig_min)*t_)*x0 + t_*x1 # ψt(x) = (1 − (1 − σmin)t)x + tx1, (22)\n",
        "    v_psi = model(psi_t, t, cond) # vt(ψt(x0))\n",
        "    d_psi = x1 - (1 - sig_min) * x0 #\n",
        "    return F.mse_loss(v_psi, d_psi) # LCFM(θ)\n",
        "\n",
        "# batch = 4\n",
        "# model = UNetModel(cond_dim=16).to(device)\n",
        "# x1=torch.rand((batch,3,16,16),device=device)\n",
        "# t = torch.rand((batch,1), device=device) # in [0,1] [N,1]\n",
        "# # print(t)\n",
        "# cond=torch.rand((batch,16),device=device)\n",
        "# loss = otfm_loss(model, x1, cond)\n",
        "\n",
        "\n",
        "def train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "        # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.amp.autocast('cuda'): # torch.amp.GradScaler('cuda')\n",
        "        # with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "            # x1 = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # loss = ae_loss + fm_loss\n",
        "            x1 = F.interpolate(x1, size=(16,16))#.repeat(1,3,1,1)\n",
        "            loss = otfm_loss(model, x1, cond) # unet\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        if i % 10 == 0: print(i,loss.item())\n",
        "        # if i % 10 == 0: print(i,ae_loss.item(),fm_loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "        except: pass\n",
        "\n",
        "# x1 = F.interpolate(x1, size=(28,28))\n",
        "# F.avg_pool2d(input, kernel_size, stride=None, padding=0)\n",
        "\n",
        "for epoch in range(40):\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    # cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).to(torch.float)\n",
        "    # sampled_data = model.sample(cond, n_samples = 1)\n",
        "    # imshow(torchvision.utils.make_grid(sampled_data.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "7d73768b-0240-42e1-efdd-d739f03acd28"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250219_125107-6fs3nwog</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/lfm/runs/6fs3nwog' target=\"_blank\">polished-dust-15</a></strong> to <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">https://wandb.ai/bobdole/lfm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/lfm/runs/6fs3nwog' target=\"_blank\">https://wandb.ai/bobdole/lfm/runs/6fs3nwog</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"lfm\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6KfhGOPSJC4",
        "outputId": "2d137a29-87f1-449c-f3b6-c46afbbc89d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torchdiffeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "OFc76OzlFnE1",
        "outputId": "09a85dc9-392b-4441-d1b4-2db6dc6096fc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title Sampling save\n",
        "\n",
        "def reverse_flow(unet, cond, num_samples=1, timesteps=25):\n",
        "    unet.eval()\n",
        "    dt = 1.0 / timesteps\n",
        "    x = torch.randn((num_samples, 1,16,16), device=device)\n",
        "    # x = torch.randn((num_samples, 16,16,16), device=device)\n",
        "    cond = cond.repeat(num_samples,1) # [n_samples, cond_dim]\n",
        "    for i in range(timesteps, 0, -1): # [timesteps,...,1]\n",
        "        t = torch.full((num_samples,), i * dt, device=device)  # Current time # [num_samples] 1. # torch.tensor(i * dt, device=device).repeat(n_samples)\n",
        "        with torch.no_grad():\n",
        "            model = lambda y,t: -unet(y, t, cond)\n",
        "            v = model(x, t)\n",
        "            x = x - dt * v # Euler update # 25steps:1sec\n",
        "\n",
        "            # k1 = model(x, t)\n",
        "            # k2 = model(x - 0.5 * dt * k1, t - 0.5 * dt)\n",
        "            # k3 = model(x - 0.5 * dt * k2, t - 0.5 * dt)\n",
        "            # k4 = model(x - dt * k3, t - dt)\n",
        "            # x = x - (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4) # RK4 update # 25steps:4.5sec\n",
        "    return x\n",
        "\n",
        "\n",
        "# from torchdiffeq import odeint # https://github.com/rtqichen/torchdiffeq\n",
        "# def reverse_flow(unet, cond, n_samples=1, timesteps=25):\n",
        "#     unet.eval()\n",
        "#     # x_init = torch.randn(n_samples, 1,16,16, device=device)\n",
        "#     x_init = torch.randn(n_samples, 16,16,16, device=device)\n",
        "#     t_span = torch.tensor([1.0, 0.0], device=device)  # Reverse from t=1 to t=0\n",
        "#     # t_span = torch.linspace(1.0, 0.0, timesteps, device=device)  # Use timesteps to augment t_span\n",
        "#     # t_span = torch.cat([torch.rand(timesteps-1, device=device).sort()[0], torch.tensor([1.],device=device)])\n",
        "#     # print(t_span)\n",
        "#     cond = cond.repeat(n_samples,1)\n",
        "#     with torch.no_grad(): # 25steps: dopri5:2sec\n",
        "#         f = lambda t,y: -unet(y, t.repeat(n_samples), cond)\n",
        "#         x_samples = odeint(f, x_init, t_span, method='dopri5') # dopri5 euler rk4 # nn.Module, [batch, *img], [batch]\n",
        "#     return x_samples[-1] # Final sampled data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# relative rtol and absolute atol error tolerance.\n",
        "# RKAdaptiveStepsizeODESolver(func, y0, rtol, atol, min_step=0, max_step=float('inf'), first_step=None, step_t=None, jump_t=None, safety=0.9, ifactor=10.0, dfactor=0.2, max_num_steps=2 ** 31 - 1, dtype=torch.float64, **kwargs)\n",
        "# https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/rk_common.py#L159\n",
        "\n",
        "# https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/odeint.py#L36\n",
        "# odeint(func, y0, t, *, rtol=1e-7, atol=1e-9, method=None, options=None, event_fn=None)\n",
        "\n",
        "# class ODEFunc(nn.Module): # https://github.com/rtqichen/torchdiffeq/blob/master/examples/ode_demo.py\n",
        "#     def forward(self, t, y): return dydt\n",
        "# # dy/dt = f(t, y)    y(t0) = y0\n",
        "\n",
        "\n",
        "cond = F.one_hot(torch.tensor([3], device=device), num_classes=10).to(torch.float)\n",
        "n_samples = 1\n",
        "sampled_data = reverse_flow(model, cond, n_samples, timesteps=25)\n",
        "# sampled_data = model.sample(cond, n_samples = 1)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(sampled_data.detach().cpu().squeeze())\n",
        "plt.show()\n",
        "\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(torchvision.utils.make_grid(sampled_data.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "5kMpN08KtE4F",
        "outputId": "24a6c415-a5c3-4389-ef34-562e0ca8fc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "torch.Size([2, 16, 16, 16]) torch.Size([4]) torch.Size([2, 10])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b49b93d30bed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5ed41310b1a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, cond)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5ed41310b1a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, cond)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5ed41310b1a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, cond)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'emb'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fwdparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'cond'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fwdparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5ed41310b1a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_rest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# act, drop, conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0memb_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [N, C, ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "y = torch.randn(n_samples, 16,16,16, device=device)\n",
        "t = torch.tensor([1.0, 0.0], device=device)  # Reverse from t=1 to t=0\n",
        "cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).to(torch.float)\n",
        "\n",
        "cond = cond.repeat(n_samples,1)\n",
        "print()\n",
        "print(y.shape, t.repeat(n_samples).shape, cond.shape)\n",
        "out = model.unet(y, t.repeat(n_samples), cond)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "nFhmfVnrOiQd",
        "outputId": "941d0d30-ab06-4006-8ba3-596806111fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# dataiter = iter(train_data)\n",
        "x,y = next(dataiter)\n",
        "# print(x)\n",
        "print(x.shape)\n",
        "x=x.unsqueeze(0)\n",
        "# x1 = F.interpolate(x, size=(16,16))#.repeat(1,3,1,1)\n",
        "x1 = F.interpolate(x, size=(64,64))\n",
        "# plt.imshow(x1.squeeze())\n",
        "# plt.show()\n",
        "imshow(torchvision.utils.make_grid(x1.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgGBIAw9vqzK"
      },
      "outputs": [],
      "source": [
        "num_samples=1000\n",
        "timesteps=5\n",
        "dt = 1.0 / timesteps\n",
        "for i in range(timesteps, 0, -1): # [timesteps,...,1]\n",
        "    # print(i)\n",
        "    t = torch.full((num_samples, 1), i * dt, device=device)  # Current time\n",
        "    print(t)\n",
        "    break\n",
        "# [num_samples, 1] 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CstpDwVHfXg4",
        "outputId": "19f4778c-00ac-4448-b355-4b79b10d3d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time_emb.0.weight Parameter containing:\n",
            "tensor([[ 2.2224e-01, -1.1569e-01,  2.1734e-01, -7.0454e-02, -1.9370e-01,\n",
            "         -8.1925e-02, -8.0945e-02, -6.6646e-02,  1.4450e-01,  4.8725e-02,\n",
            "          1.7204e-01,  1.0088e-01, -6.0856e-02, -1.3187e-01,  2.3881e-01,\n",
            "         -1.3164e-01],\n",
            "        [-2.3228e-01,  1.7033e-01,  3.9568e-02, -1.1796e-01, -2.2946e-01,\n",
            "          8.5528e-02, -1.0027e-01,  2.1428e-01,  1.2432e-01,  1.0318e-01,\n",
            "          1.8889e-02, -1.5491e-02, -2.1642e-01, -2.0027e-01,  6.2008e-02,\n",
            "          1.5723e-01],\n",
            "        [ 2.0396e-01, -1.6001e-01,  8.7920e-02, -1.3276e-01, -1.6379e-01,\n",
            "         -1.0306e-01,  6.1081e-02,  1.8134e-01,  4.1300e-02,  2.9612e-02,\n",
            "          1.3295e-01,  1.2934e-01,  8.1500e-02,  1.7800e-01,  1.7543e-02,\n",
            "         -2.3165e-01],\n",
            "        [ 1.2318e-01, -2.0054e-01, -9.8127e-02,  3.2401e-02,  1.0639e-01,\n",
            "          3.7568e-02, -4.9770e-06,  2.4743e-01,  6.4122e-03,  5.3389e-02,\n",
            "          2.4320e-02, -1.8702e-02, -1.0420e-01,  1.1202e-02,  1.2723e-01,\n",
            "          8.8092e-02],\n",
            "        [-2.0736e-01, -1.0030e-01, -1.1918e-01,  1.4442e-01,  2.1877e-01,\n",
            "         -2.4617e-01, -1.4231e-01,  2.3804e-01,  1.5828e-01, -7.6946e-02,\n",
            "          2.0051e-01,  1.7758e-01,  1.9369e-01,  1.3061e-01, -1.6903e-01,\n",
            "         -2.1539e-01],\n",
            "        [-1.4665e-02, -3.7406e-02,  1.5004e-01,  4.1454e-02,  1.8748e-01,\n",
            "         -4.7212e-02,  7.3418e-02, -2.2490e-01,  5.4399e-02, -8.3911e-02,\n",
            "          1.2354e-02,  8.0298e-02,  1.2317e-01,  1.7601e-01,  1.1412e-01,\n",
            "          2.6959e-02],\n",
            "        [ 9.5820e-02, -2.2625e-01,  2.1784e-01,  6.7029e-02,  1.3886e-01,\n",
            "          2.1284e-02,  2.4190e-01, -3.4399e-02, -2.1840e-01, -1.5111e-01,\n",
            "         -5.8863e-02,  1.4879e-01, -1.8233e-01, -1.3751e-01,  1.8010e-01,\n",
            "         -1.6997e-02],\n",
            "        [-2.4474e-01,  1.1986e-01,  2.2611e-01,  2.4226e-02,  3.5316e-02,\n",
            "          1.0340e-01, -1.5481e-01,  2.2936e-01, -2.2306e-01, -1.1591e-01,\n",
            "         -2.2029e-01, -2.3566e-01,  6.9733e-02, -2.2802e-01,  1.7497e-01,\n",
            "          3.4862e-02],\n",
            "        [-9.4381e-02, -1.0976e-01,  1.6862e-01, -8.2610e-02,  2.4175e-01,\n",
            "          2.0714e-01, -1.3463e-02,  1.9787e-02, -1.3943e-01, -4.5901e-02,\n",
            "          2.4193e-02, -2.1767e-01, -2.5071e-02, -1.4409e-01,  4.2368e-02,\n",
            "         -2.0244e-01],\n",
            "        [-2.4011e-01, -2.9174e-02,  2.3007e-02, -1.0528e-02,  2.3551e-02,\n",
            "          1.5415e-01, -1.3218e-01, -5.5732e-02,  2.4428e-02,  4.2646e-03,\n",
            "         -1.0429e-01,  2.0990e-01, -7.1576e-02, -1.3860e-01, -2.3341e-01,\n",
            "          1.2674e-01],\n",
            "        [-2.4858e-01, -5.7793e-02,  7.8053e-02, -1.0504e-01,  1.5480e-01,\n",
            "          7.7291e-02, -4.7909e-02,  3.7482e-02, -8.5913e-02,  8.5261e-02,\n",
            "          8.8172e-02,  7.3819e-03,  1.5686e-01, -5.6717e-02, -3.4508e-03,\n",
            "          1.2187e-01],\n",
            "        [ 1.5327e-01, -2.2646e-01, -2.1605e-01,  5.7584e-02, -6.1293e-02,\n",
            "          1.8037e-01,  6.2768e-02, -5.0087e-02,  2.3033e-01,  2.3864e-01,\n",
            "          1.2808e-01, -1.7705e-01, -9.9865e-02,  1.1385e-01, -2.0451e-01,\n",
            "          2.3825e-01],\n",
            "        [ 1.2008e-01,  2.1845e-02,  3.7717e-02,  1.2886e-01, -2.2818e-01,\n",
            "         -2.4288e-01,  1.8149e-01, -9.3326e-02,  6.3538e-02, -1.6780e-01,\n",
            "          2.1636e-01, -2.0441e-01,  7.9302e-02, -6.3546e-03,  2.0117e-01,\n",
            "         -1.1128e-01],\n",
            "        [ 8.8320e-02,  1.5079e-01,  2.2559e-02, -2.3487e-01, -8.0608e-02,\n",
            "         -8.6073e-02,  1.1230e-01, -2.1917e-01, -1.1557e-01,  1.2868e-01,\n",
            "          1.2761e-01, -1.3836e-01, -1.3591e-01,  1.9684e-01, -1.7007e-01,\n",
            "          4.2897e-02],\n",
            "        [ 1.5442e-01,  3.3195e-02, -1.9176e-01,  1.2703e-01,  1.9877e-02,\n",
            "          3.2251e-03,  1.9604e-01,  1.3258e-01,  3.5461e-02,  1.8803e-01,\n",
            "          1.7213e-01,  1.9879e-01,  1.1426e-01, -1.9024e-01, -1.7382e-01,\n",
            "         -1.4739e-01],\n",
            "        [ 2.1558e-01, -5.1947e-02,  2.2027e-01,  1.2868e-01,  2.4986e-01,\n",
            "          8.6603e-02,  6.2257e-02,  9.7736e-02, -3.4953e-02, -3.1984e-03,\n",
            "          7.6173e-02, -4.3616e-02,  2.7561e-02, -2.3836e-01, -5.6689e-02,\n",
            "         -2.1389e-01]], device='cuda:0', requires_grad=True)\n",
            "time_emb.0.bias Parameter containing:\n",
            "tensor([ 0.1652, -0.2155, -0.0584, -0.2107, -0.2114, -0.0480, -0.2051, -0.0023,\n",
            "        -0.1067,  0.2302,  0.1890, -0.1613, -0.1679,  0.1703, -0.0891, -0.1893],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "time_emb.2.weight Parameter containing:\n",
            "tensor([[ 0.0489, -0.2269,  0.1493,  0.0733, -0.1743, -0.0567, -0.0848,  0.0337,\n",
            "          0.0511, -0.2139,  0.1965,  0.2390, -0.0239,  0.2310,  0.1751,  0.2173],\n",
            "        [-0.0666, -0.2136, -0.2467, -0.0442,  0.1545, -0.0429,  0.0890,  0.1181,\n",
            "          0.1183,  0.1962, -0.0939,  0.0526, -0.0677, -0.1355, -0.2173, -0.1205],\n",
            "        [-0.0138, -0.2300,  0.2035,  0.0807, -0.1785,  0.1688,  0.1196, -0.0572,\n",
            "         -0.0483,  0.0181,  0.0253,  0.0049,  0.1205, -0.2332, -0.0117, -0.0902],\n",
            "        [ 0.2042, -0.1735, -0.0460, -0.2272, -0.0517, -0.1102,  0.2406,  0.2204,\n",
            "          0.0849,  0.2319, -0.0177, -0.1828,  0.1791,  0.1209, -0.0951, -0.0305],\n",
            "        [ 0.0188, -0.1065,  0.1917,  0.1836, -0.1821,  0.0288, -0.2122, -0.1775,\n",
            "          0.1850,  0.1230, -0.1745,  0.0929,  0.0137, -0.1880,  0.1230,  0.1594],\n",
            "        [ 0.0975,  0.1185,  0.1521, -0.2050, -0.1743,  0.1020,  0.1261,  0.2491,\n",
            "         -0.2136,  0.0007, -0.1383, -0.1491, -0.1917,  0.1108,  0.0449,  0.1222],\n",
            "        [ 0.0557, -0.1771,  0.0719, -0.1919,  0.0820,  0.1820,  0.2225, -0.0577,\n",
            "          0.0961,  0.1500,  0.0812,  0.2265, -0.0802, -0.0469, -0.0973,  0.0219],\n",
            "        [ 0.2417,  0.0069,  0.2144, -0.1915, -0.0150,  0.1586,  0.1361,  0.1160,\n",
            "         -0.0887, -0.2084, -0.0250, -0.1604,  0.1991, -0.1055,  0.1531, -0.1314],\n",
            "        [-0.1191,  0.0125, -0.1365, -0.2444, -0.0121, -0.0372,  0.0422,  0.0300,\n",
            "          0.0445,  0.0170,  0.0551, -0.2388,  0.1524, -0.0244, -0.2358, -0.1652],\n",
            "        [ 0.1138, -0.0771, -0.1542, -0.1189,  0.1705,  0.0431,  0.0503,  0.1692,\n",
            "          0.0573, -0.0434, -0.0330,  0.0222, -0.0185,  0.1237,  0.1237, -0.0035],\n",
            "        [-0.1245,  0.0956,  0.1211,  0.2371, -0.1830, -0.1528,  0.1696,  0.0033,\n",
            "         -0.1985,  0.1578, -0.0074, -0.1493,  0.0011, -0.1148,  0.0087, -0.2211],\n",
            "        [-0.0164,  0.0005,  0.0057, -0.2142,  0.2208,  0.2040, -0.2133,  0.2472,\n",
            "          0.1800,  0.0118, -0.0946, -0.1259,  0.1284,  0.0922,  0.0979,  0.1770],\n",
            "        [-0.0402,  0.2368,  0.2363, -0.0606, -0.0183,  0.1073, -0.2183, -0.1744,\n",
            "         -0.0727, -0.1981, -0.1461, -0.0270, -0.2225,  0.2277,  0.2497, -0.2317],\n",
            "        [-0.2023,  0.1194,  0.1420,  0.1259,  0.0336, -0.2188,  0.0441,  0.1292,\n",
            "         -0.1828, -0.1208,  0.1701, -0.1473, -0.0848,  0.1502,  0.2071, -0.1651],\n",
            "        [-0.0749,  0.0822,  0.2377,  0.0184,  0.1063, -0.1732,  0.1728,  0.0193,\n",
            "          0.1713, -0.0563,  0.1977, -0.1348,  0.1742,  0.1598, -0.2166,  0.0559],\n",
            "        [ 0.0898, -0.1477, -0.0664,  0.0304,  0.0329,  0.1708, -0.1689, -0.1177,\n",
            "          0.2114, -0.0123, -0.1766, -0.1479, -0.1306,  0.1363, -0.0659, -0.1682]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "time_emb.2.bias Parameter containing:\n",
            "tensor([ 0.1090,  0.2342,  0.0549,  0.0801,  0.1776,  0.0959, -0.2029,  0.1209,\n",
            "         0.0988, -0.2257, -0.0656,  0.0825, -0.0960,  0.2473,  0.2455, -0.2412],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "input_blocks.0.weight Parameter containing:\n",
            "tensor([[[[ 0.1406, -0.0057,  0.1655],\n",
            "          [ 0.2486, -0.2431,  0.0073],\n",
            "          [-0.1592,  0.0381, -0.1727]]],\n",
            "\n",
            "\n",
            "        [[[-0.3207, -0.2784, -0.1004],\n",
            "          [ 0.2867, -0.1504, -0.2400],\n",
            "          [-0.2326,  0.1514,  0.1269]]],\n",
            "\n",
            "\n",
            "        [[[-0.2150, -0.0888,  0.1432],\n",
            "          [ 0.1858,  0.3311,  0.2485],\n",
            "          [-0.2383, -0.0440,  0.1807]]],\n",
            "\n",
            "\n",
            "        [[[-0.1172,  0.1524,  0.1141],\n",
            "          [ 0.1588,  0.0128, -0.2615],\n",
            "          [ 0.1349, -0.0724,  0.1360]]],\n",
            "\n",
            "\n",
            "        [[[-0.2376, -0.0138,  0.2309],\n",
            "          [ 0.3039,  0.0841,  0.2940],\n",
            "          [ 0.1424, -0.0108,  0.3307]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2613, -0.3218,  0.1364],\n",
            "          [-0.0432, -0.0979,  0.0909],\n",
            "          [ 0.0505, -0.2710,  0.0971]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0493, -0.2677, -0.0127],\n",
            "          [ 0.2520,  0.0934, -0.0444],\n",
            "          [ 0.3046, -0.1757,  0.2329]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3229, -0.0284, -0.2835],\n",
            "          [ 0.1747,  0.1617,  0.1639],\n",
            "          [-0.2353,  0.0748,  0.2337]]],\n",
            "\n",
            "\n",
            "        [[[-0.1918, -0.1164,  0.0782],\n",
            "          [ 0.1446, -0.1418, -0.2785],\n",
            "          [ 0.2027,  0.2926,  0.3144]]],\n",
            "\n",
            "\n",
            "        [[[-0.2956,  0.2553, -0.3037],\n",
            "          [-0.1538, -0.1059,  0.1080],\n",
            "          [-0.1474, -0.0729, -0.1881]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2869,  0.0464,  0.1623],\n",
            "          [-0.3236, -0.0405,  0.1803],\n",
            "          [ 0.3176,  0.2694, -0.3248]]],\n",
            "\n",
            "\n",
            "        [[[-0.0194,  0.2626,  0.1558],\n",
            "          [-0.1119, -0.0412, -0.3173],\n",
            "          [-0.1580,  0.1325, -0.2545]]],\n",
            "\n",
            "\n",
            "        [[[-0.0104,  0.1598, -0.0756],\n",
            "          [-0.0014,  0.0866,  0.2619],\n",
            "          [-0.0912,  0.3246, -0.0090]]],\n",
            "\n",
            "\n",
            "        [[[-0.1477, -0.2727,  0.0125],\n",
            "          [ 0.0196, -0.0346, -0.0448],\n",
            "          [-0.1160,  0.0115, -0.0059]]],\n",
            "\n",
            "\n",
            "        [[[-0.2174,  0.3043, -0.2430],\n",
            "          [ 0.2142, -0.0594,  0.1732],\n",
            "          [ 0.2995,  0.2176, -0.0280]]],\n",
            "\n",
            "\n",
            "        [[[-0.2907,  0.2803, -0.0840],\n",
            "          [-0.0603, -0.1458,  0.0985],\n",
            "          [ 0.0814,  0.0320,  0.1035]]]], device='cuda:0', requires_grad=True)\n",
            "input_blocks.0.bias Parameter containing:\n",
            "tensor([-0.0569,  0.3079, -0.1236, -0.1776,  0.0391, -0.2213,  0.0726, -0.3316,\n",
            "         0.1875, -0.0955,  0.2457, -0.2489,  0.2519,  0.0197, -0.0777, -0.2721],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "down_list.0.seq.0.op.weight Parameter containing:\n",
            "tensor([[[[ 0.0029,  0.0411,  0.0096],\n",
            "          [-0.0152, -0.0719, -0.0790],\n",
            "          [ 0.0461,  0.0546,  0.0282]],\n",
            "\n",
            "         [[-0.0026, -0.0733, -0.0341],\n",
            "          [ 0.0008,  0.0777,  0.0206],\n",
            "          [-0.0623, -0.0607, -0.0446]],\n",
            "\n",
            "         [[-0.0411,  0.0074, -0.0681],\n",
            "          [ 0.0225, -0.0400, -0.0709],\n",
            "          [ 0.0584,  0.0524, -0.0577]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0800, -0.0686,  0.0801],\n",
            "          [-0.0488, -0.0720, -0.0448],\n",
            "          [-0.0768, -0.0239,  0.0750]],\n",
            "\n",
            "         [[ 0.0416,  0.0691, -0.0379],\n",
            "          [ 0.0188,  0.0365,  0.0816],\n",
            "          [ 0.0635,  0.0078,  0.0084]],\n",
            "\n",
            "         [[ 0.0426,  0.0177,  0.0831],\n",
            "          [-0.0550,  0.0288,  0.0703],\n",
            "          [-0.0465,  0.0384, -0.0582]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0345, -0.0528,  0.0313],\n",
            "          [ 0.0218,  0.0830,  0.0437],\n",
            "          [ 0.0579,  0.0133,  0.0637]],\n",
            "\n",
            "         [[-0.0334, -0.0784, -0.0153],\n",
            "          [ 0.0793,  0.0405,  0.0503],\n",
            "          [-0.0298, -0.0801, -0.0147]],\n",
            "\n",
            "         [[-0.0461,  0.0208,  0.0612],\n",
            "          [ 0.0461,  0.0423,  0.0361],\n",
            "          [-0.0285, -0.0175,  0.0744]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0232, -0.0282,  0.0534],\n",
            "          [-0.0339, -0.0215,  0.0614],\n",
            "          [-0.0447, -0.0626,  0.0241]],\n",
            "\n",
            "         [[-0.0241, -0.0651,  0.0090],\n",
            "          [ 0.0489, -0.0624,  0.0303],\n",
            "          [-0.0404,  0.0646, -0.0029]],\n",
            "\n",
            "         [[-0.0488, -0.0587,  0.0825],\n",
            "          [ 0.0769,  0.0523,  0.0811],\n",
            "          [ 0.0025,  0.0470, -0.0338]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0407, -0.0102,  0.0100],\n",
            "          [ 0.0012,  0.0106,  0.0302],\n",
            "          [-0.0199,  0.0555,  0.0457]],\n",
            "\n",
            "         [[ 0.0319,  0.0148,  0.0538],\n",
            "          [ 0.0787, -0.0507, -0.0590],\n",
            "          [ 0.0004,  0.0407, -0.0128]],\n",
            "\n",
            "         [[-0.0509,  0.0687, -0.0724],\n",
            "          [ 0.0035, -0.0569,  0.0511],\n",
            "          [ 0.0328,  0.0197,  0.0504]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0772,  0.0220,  0.0675],\n",
            "          [-0.0033,  0.0248,  0.0638],\n",
            "          [ 0.0204, -0.0183, -0.0060]],\n",
            "\n",
            "         [[-0.0511, -0.0174, -0.0654],\n",
            "          [-0.0553,  0.0575,  0.0832],\n",
            "          [ 0.0502,  0.0175, -0.0733]],\n",
            "\n",
            "         [[-0.0150, -0.0613,  0.0090],\n",
            "          [ 0.0758, -0.0594,  0.0432],\n",
            "          [-0.0642, -0.0536, -0.0628]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0678,  0.0148, -0.0772],\n",
            "          [ 0.0077,  0.0517,  0.0761],\n",
            "          [ 0.0589, -0.0752, -0.0390]],\n",
            "\n",
            "         [[-0.0152, -0.0185,  0.0459],\n",
            "          [-0.0175,  0.0209, -0.0627],\n",
            "          [-0.0781,  0.0514,  0.0028]],\n",
            "\n",
            "         [[ 0.0167,  0.0751, -0.0163],\n",
            "          [-0.0450, -0.0264, -0.0134],\n",
            "          [-0.0475, -0.0547, -0.0376]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0642,  0.0278, -0.0331],\n",
            "          [-0.0796,  0.0235, -0.0660],\n",
            "          [-0.0484,  0.0788, -0.0630]],\n",
            "\n",
            "         [[-0.0722, -0.0728, -0.0203],\n",
            "          [ 0.0148, -0.0101, -0.0645],\n",
            "          [ 0.0008, -0.0238, -0.0361]],\n",
            "\n",
            "         [[ 0.0104, -0.0814, -0.0188],\n",
            "          [ 0.0228, -0.0272,  0.0227],\n",
            "          [-0.0375, -0.0197, -0.0619]]],\n",
            "\n",
            "\n",
            "        [[[-0.0080,  0.0519, -0.0440],\n",
            "          [-0.0124, -0.0397, -0.0071],\n",
            "          [-0.0354,  0.0661, -0.0749]],\n",
            "\n",
            "         [[ 0.0209, -0.0630, -0.0124],\n",
            "          [-0.0542, -0.0170,  0.0779],\n",
            "          [-0.0493,  0.0133, -0.0185]],\n",
            "\n",
            "         [[ 0.0256, -0.0721,  0.0062],\n",
            "          [ 0.0537, -0.0726,  0.0438],\n",
            "          [ 0.0721, -0.0132,  0.0589]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0165, -0.0456, -0.0717],\n",
            "          [ 0.0167, -0.0025,  0.0794],\n",
            "          [-0.0472,  0.0135, -0.0467]],\n",
            "\n",
            "         [[-0.0165, -0.0023, -0.0133],\n",
            "          [ 0.0798, -0.0557, -0.0642],\n",
            "          [-0.0503, -0.0280,  0.0300]],\n",
            "\n",
            "         [[ 0.0302, -0.0485, -0.0188],\n",
            "          [-0.0558,  0.0165,  0.0135],\n",
            "          [ 0.0300, -0.0134,  0.0792]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0783, -0.0523, -0.0733],\n",
            "          [-0.0315,  0.0130,  0.0273],\n",
            "          [ 0.0551,  0.0093,  0.0690]],\n",
            "\n",
            "         [[ 0.0749,  0.0297, -0.0592],\n",
            "          [-0.0280, -0.0075, -0.0250],\n",
            "          [ 0.0749,  0.0414,  0.0788]],\n",
            "\n",
            "         [[-0.0533, -0.0081,  0.0538],\n",
            "          [ 0.0078, -0.0151,  0.0181],\n",
            "          [ 0.0795,  0.0106, -0.0691]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0688, -0.0577, -0.0683],\n",
            "          [ 0.0338, -0.0017, -0.0516],\n",
            "          [-0.0214,  0.0697, -0.0796]],\n",
            "\n",
            "         [[ 0.0069,  0.0645,  0.0287],\n",
            "          [ 0.0688,  0.0092, -0.0776],\n",
            "          [ 0.0831,  0.0435, -0.0473]],\n",
            "\n",
            "         [[ 0.0378,  0.0281,  0.0098],\n",
            "          [-0.0579, -0.0476, -0.0044],\n",
            "          [-0.0068,  0.0756, -0.0146]]]], device='cuda:0', requires_grad=True)\n",
            "down_list.0.seq.0.op.bias Parameter containing:\n",
            "tensor([ 0.0340,  0.0719,  0.0411, -0.0018,  0.0563, -0.0058, -0.0491,  0.0604,\n",
            "        -0.0059, -0.0265,  0.0206, -0.0539, -0.0529, -0.0684,  0.0155,  0.0085,\n",
            "         0.0067,  0.0522, -0.0639, -0.0534, -0.0255,  0.0198,  0.0540,  0.0759,\n",
            "         0.0018,  0.0822,  0.0098,  0.0056,  0.0370,  0.0494,  0.0526,  0.0629],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "down_list.1.seq.0.op.weight Parameter containing:\n",
            "tensor([[[[-3.6201e-02,  7.6232e-04, -5.3788e-02],\n",
            "          [ 1.6884e-02,  1.9836e-02,  4.2690e-02],\n",
            "          [ 4.7127e-02, -1.4082e-02, -2.7522e-02]],\n",
            "\n",
            "         [[-4.4925e-02, -1.7424e-02,  6.7984e-03],\n",
            "          [-7.4786e-03,  4.4517e-02,  4.6313e-02],\n",
            "          [ 4.7549e-02, -5.8143e-03, -3.2278e-02]],\n",
            "\n",
            "         [[ 7.1785e-03, -4.1390e-02, -5.8554e-02],\n",
            "          [ 4.9776e-02, -3.7070e-02, -4.1846e-02],\n",
            "          [ 1.4061e-02,  3.8004e-02,  1.8215e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3237e-02, -1.2559e-02,  5.0485e-02],\n",
            "          [-8.4586e-03, -5.0921e-02, -1.0659e-02],\n",
            "          [-1.1326e-02,  2.5437e-02, -2.1831e-02]],\n",
            "\n",
            "         [[-4.9694e-03, -1.8654e-02, -8.2323e-03],\n",
            "          [ 3.3975e-02,  3.2814e-02,  2.3248e-02],\n",
            "          [ 4.1292e-02,  3.7751e-02,  1.9193e-03]],\n",
            "\n",
            "         [[ 1.9296e-03, -3.0788e-02, -9.2962e-03],\n",
            "          [-4.3011e-02,  5.1524e-02, -3.1793e-03],\n",
            "          [-5.4567e-03,  4.7495e-02, -3.1326e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.4248e-03, -2.4698e-02,  1.6292e-02],\n",
            "          [ 3.5576e-02, -5.5443e-02, -2.0279e-02],\n",
            "          [ 1.7407e-05,  2.4688e-02, -2.5726e-02]],\n",
            "\n",
            "         [[ 3.9708e-02, -1.1997e-02, -5.7624e-02],\n",
            "          [-4.1856e-02,  4.6134e-03, -2.6298e-02],\n",
            "          [ 5.5657e-02,  4.0292e-02, -5.1301e-03]],\n",
            "\n",
            "         [[-1.1558e-03,  4.4614e-02, -5.8465e-02],\n",
            "          [-5.5267e-02, -9.7656e-03,  4.5689e-02],\n",
            "          [-1.1132e-02,  1.3935e-02,  6.9888e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.8513e-02, -5.7333e-02,  5.7951e-03],\n",
            "          [ 1.6353e-02, -2.0649e-02,  2.6585e-02],\n",
            "          [-3.9477e-02, -2.0399e-02,  2.1498e-03]],\n",
            "\n",
            "         [[ 9.4354e-03, -4.7702e-02, -5.4501e-03],\n",
            "          [ 1.2428e-02,  5.0849e-02, -2.9934e-02],\n",
            "          [ 5.1358e-03,  7.0978e-03,  2.7477e-02]],\n",
            "\n",
            "         [[ 4.0608e-03, -6.6572e-03,  3.3694e-02],\n",
            "          [-5.7031e-02,  5.3678e-02,  9.0175e-03],\n",
            "          [ 3.4303e-02,  1.6025e-02, -3.3269e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.7967e-02,  3.1709e-02, -5.7872e-02],\n",
            "          [-4.7072e-02,  2.8273e-02,  9.3072e-03],\n",
            "          [-3.8889e-02, -4.4915e-02, -4.8143e-02]],\n",
            "\n",
            "         [[-4.4905e-02, -2.2803e-02, -1.5570e-02],\n",
            "          [-5.3494e-02,  8.5912e-03, -7.2550e-03],\n",
            "          [ 3.8274e-02,  3.0311e-02, -3.4196e-02]],\n",
            "\n",
            "         [[-5.4604e-02,  1.8486e-02, -1.7125e-02],\n",
            "          [-3.3705e-02,  2.9823e-02,  2.9661e-02],\n",
            "          [-2.6300e-02,  3.2282e-02, -2.0559e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.6847e-03,  3.6497e-02, -2.9783e-02],\n",
            "          [-2.8531e-02,  4.9198e-02,  2.4005e-02],\n",
            "          [ 3.0631e-02,  2.3755e-02, -5.8731e-02]],\n",
            "\n",
            "         [[ 3.6263e-02, -2.5367e-02, -5.5049e-02],\n",
            "          [-1.2974e-02, -2.1539e-02, -8.1878e-03],\n",
            "          [-1.5090e-02, -1.5636e-02,  1.3919e-02]],\n",
            "\n",
            "         [[-9.8685e-03,  5.2127e-02, -2.3131e-02],\n",
            "          [ 1.1501e-02, -5.1532e-02,  3.1079e-02],\n",
            "          [-9.3081e-03, -3.5879e-02,  2.1814e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3753e-02, -4.7345e-02,  5.6515e-04],\n",
            "          [ 4.1213e-02, -1.9917e-02, -4.3664e-02],\n",
            "          [-4.3712e-02, -2.1667e-02,  3.7879e-02]],\n",
            "\n",
            "         [[ 2.1196e-02,  4.0892e-02, -3.2048e-02],\n",
            "          [ 2.1778e-02,  7.1403e-03,  2.7754e-02],\n",
            "          [ 4.2141e-03, -7.3436e-03,  2.4773e-02]],\n",
            "\n",
            "         [[ 1.1854e-02, -4.6931e-02,  4.8126e-03],\n",
            "          [ 3.2880e-02, -5.2942e-02, -2.5682e-02],\n",
            "          [-2.0739e-02,  5.6881e-02, -2.3234e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7630e-02, -4.6896e-02,  2.7173e-02],\n",
            "          [ 2.7707e-02,  2.0065e-02, -1.3114e-02],\n",
            "          [ 1.0690e-02,  2.1829e-02, -5.9054e-03]],\n",
            "\n",
            "         [[ 1.1001e-04,  4.7172e-02, -9.8387e-03],\n",
            "          [ 5.8841e-02, -1.3069e-02,  3.2593e-02],\n",
            "          [ 3.2299e-03,  4.0005e-02, -1.8846e-02]],\n",
            "\n",
            "         [[ 2.4416e-03,  2.5475e-02, -3.9289e-02],\n",
            "          [ 3.9319e-02,  2.4026e-02,  2.5537e-03],\n",
            "          [-4.9416e-02, -4.7449e-02,  5.7895e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.5458e-02,  3.8637e-02, -5.2791e-02],\n",
            "          [ 3.2005e-02,  1.0009e-02,  1.8518e-02],\n",
            "          [-5.8086e-02, -1.4275e-02,  5.8044e-02]],\n",
            "\n",
            "         [[-4.5997e-02,  2.2824e-02, -2.3738e-02],\n",
            "          [-2.5560e-02,  2.6958e-02, -1.4168e-02],\n",
            "          [-6.3113e-03, -1.7654e-02,  4.7110e-02]],\n",
            "\n",
            "         [[-5.4920e-02, -3.7327e-02, -2.6981e-02],\n",
            "          [ 1.1833e-02,  2.2968e-02,  1.1562e-02],\n",
            "          [-3.2388e-02,  3.0087e-02,  5.2797e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.9507e-02,  1.6360e-02,  5.8058e-02],\n",
            "          [ 4.8254e-02, -5.2169e-03,  5.0920e-02],\n",
            "          [-1.8717e-02, -3.8733e-02,  5.0013e-02]],\n",
            "\n",
            "         [[-4.3664e-02,  3.1357e-02, -5.6953e-02],\n",
            "          [ 1.4465e-02, -5.6384e-03,  3.3059e-02],\n",
            "          [ 4.6525e-02, -3.8826e-03, -3.7407e-02]],\n",
            "\n",
            "         [[-1.3307e-02,  4.1448e-02,  5.6623e-03],\n",
            "          [ 5.3698e-02, -1.8950e-02, -3.4482e-02],\n",
            "          [-3.2183e-02,  4.1704e-02, -3.1338e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4781e-02, -4.6650e-02,  2.7736e-02],\n",
            "          [-1.6700e-02, -2.3635e-02, -4.8423e-02],\n",
            "          [ 1.3303e-02, -3.7632e-02,  3.5668e-02]],\n",
            "\n",
            "         [[ 2.2915e-02,  3.7421e-02, -3.0432e-02],\n",
            "          [ 2.9721e-02,  2.1226e-03, -1.6051e-02],\n",
            "          [ 4.5661e-02, -2.3990e-02, -2.5484e-02]],\n",
            "\n",
            "         [[ 6.1091e-03, -3.2424e-02,  7.5787e-03],\n",
            "          [ 5.4557e-02, -4.3319e-03, -3.9384e-02],\n",
            "          [-2.6190e-02,  1.8377e-02,  4.8943e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.4050e-03, -2.6017e-02,  5.6973e-02],\n",
            "          [ 1.6103e-02,  5.0643e-02,  3.6321e-02],\n",
            "          [ 1.3258e-02,  3.7004e-02,  5.7495e-02]],\n",
            "\n",
            "         [[ 2.2628e-02, -5.1867e-03,  5.6942e-03],\n",
            "          [ 2.2037e-02,  2.3468e-02,  3.8569e-02],\n",
            "          [-6.9096e-03, -6.8318e-03,  3.8509e-02]],\n",
            "\n",
            "         [[ 5.3124e-02, -3.0016e-02, -3.5610e-02],\n",
            "          [-4.1166e-02,  1.6031e-02, -7.6213e-03],\n",
            "          [-9.5307e-03, -1.0777e-02, -5.8227e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "down_list.1.seq.0.op.bias Parameter containing:\n",
            "tensor([ 0.0462,  0.0169,  0.0546, -0.0303, -0.0509, -0.0103, -0.0508, -0.0392,\n",
            "        -0.0304, -0.0289,  0.0586,  0.0181, -0.0063,  0.0375, -0.0235, -0.0341,\n",
            "        -0.0397,  0.0584, -0.0059,  0.0259,  0.0383,  0.0270, -0.0307,  0.0587,\n",
            "         0.0207,  0.0565,  0.0524,  0.0070, -0.0384,  0.0314,  0.0070, -0.0554,\n",
            "         0.0531,  0.0034, -0.0496, -0.0012, -0.0417,  0.0268, -0.0174,  0.0268,\n",
            "         0.0140, -0.0035, -0.0113,  0.0226, -0.0209, -0.0393,  0.0540,  0.0093,\n",
            "        -0.0230, -0.0324,  0.0199,  0.0522, -0.0058, -0.0195,  0.0022, -0.0408,\n",
            "        -0.0218, -0.0283,  0.0104, -0.0369,  0.0228, -0.0382, -0.0137,  0.0176],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "down_list.2.seq.0.op.weight Parameter containing:\n",
            "tensor([[[[ 1.1893e-02, -4.6264e-03,  2.7565e-02],\n",
            "          [ 3.3600e-02,  2.9722e-02,  2.2203e-02],\n",
            "          [-3.7623e-02,  2.9678e-02,  1.2627e-02]],\n",
            "\n",
            "         [[-2.1165e-02, -2.5557e-04, -2.6046e-02],\n",
            "          [ 3.7462e-02,  1.3493e-02, -3.8075e-03],\n",
            "          [-1.5401e-02, -4.0510e-02, -1.0638e-02]],\n",
            "\n",
            "         [[ 8.5384e-05, -2.2483e-02, -1.9101e-02],\n",
            "          [ 1.7826e-02, -1.5007e-02, -2.4472e-02],\n",
            "          [ 1.7347e-02,  3.5759e-02, -3.0254e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.8356e-02,  3.3157e-02,  2.8482e-02],\n",
            "          [-3.8065e-03,  3.0813e-03,  2.1771e-02],\n",
            "          [-6.8476e-03, -2.9932e-02,  1.0282e-02]],\n",
            "\n",
            "         [[ 3.2214e-02,  1.6946e-02, -1.7261e-02],\n",
            "          [ 3.2680e-02, -1.7612e-02,  5.8415e-04],\n",
            "          [-1.1012e-03,  1.1060e-02,  1.3121e-02]],\n",
            "\n",
            "         [[-2.3955e-02,  4.0667e-02,  2.8983e-02],\n",
            "          [-2.7376e-02, -3.7819e-02, -2.8237e-02],\n",
            "          [-1.4494e-02, -1.8117e-02,  3.0114e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.9625e-03,  3.9495e-02, -4.1021e-02],\n",
            "          [-2.7486e-02, -2.4493e-02, -2.0224e-02],\n",
            "          [-3.0478e-02, -3.1359e-02,  2.0883e-02]],\n",
            "\n",
            "         [[ 3.1170e-02, -6.4438e-03, -2.6641e-02],\n",
            "          [ 1.0343e-03,  3.0464e-02,  8.3056e-03],\n",
            "          [ 2.6159e-02, -1.9511e-02,  2.0523e-02]],\n",
            "\n",
            "         [[ 7.7934e-03, -3.4595e-02, -1.4424e-02],\n",
            "          [-1.8321e-03,  3.6265e-02, -1.7580e-02],\n",
            "          [ 3.5585e-03,  6.7019e-03,  1.3654e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2531e-02,  3.8233e-02, -2.3922e-02],\n",
            "          [-3.5731e-02,  1.6474e-02, -1.1151e-02],\n",
            "          [ 1.4366e-02,  2.5356e-02,  1.0400e-02]],\n",
            "\n",
            "         [[ 1.7919e-02, -1.1797e-02, -1.6232e-02],\n",
            "          [ 1.9205e-02,  1.6939e-02, -7.7271e-04],\n",
            "          [-1.5733e-02, -2.0394e-02,  1.9580e-02]],\n",
            "\n",
            "         [[ 4.1427e-02, -3.0745e-02, -4.3596e-03],\n",
            "          [ 3.7594e-02, -4.1666e-03, -1.6264e-02],\n",
            "          [-9.9026e-03,  3.6665e-02,  2.9634e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 7.4748e-03,  2.3024e-02,  2.7492e-02],\n",
            "          [ 2.0026e-02,  3.0716e-02,  1.1488e-02],\n",
            "          [ 4.1091e-02,  5.0474e-03, -1.1996e-02]],\n",
            "\n",
            "         [[ 1.4275e-03,  1.2953e-04, -3.2284e-02],\n",
            "          [-4.1399e-02, -2.2759e-02, -3.2927e-02],\n",
            "          [ 1.4714e-02, -1.6904e-02,  2.8833e-02]],\n",
            "\n",
            "         [[ 2.5888e-02, -4.0073e-03,  1.6855e-02],\n",
            "          [ 2.6585e-02,  3.9718e-02, -2.4474e-03],\n",
            "          [ 3.2363e-02,  4.1211e-02,  3.2062e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7632e-02, -2.2025e-02, -3.3992e-02],\n",
            "          [ 3.0393e-02, -1.6751e-02,  3.8048e-02],\n",
            "          [-3.4067e-02,  4.0976e-02, -2.3121e-02]],\n",
            "\n",
            "         [[-1.3918e-02, -7.8874e-03,  3.7351e-02],\n",
            "          [ 3.7444e-03,  3.9940e-02,  2.2684e-02],\n",
            "          [-3.2656e-02, -1.4963e-02,  5.0235e-03]],\n",
            "\n",
            "         [[-2.4432e-02,  1.1729e-02,  7.2339e-03],\n",
            "          [-4.2548e-03,  1.2760e-02, -9.0255e-03],\n",
            "          [-4.1361e-02, -2.4625e-02,  1.8024e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.1966e-02, -1.7326e-02,  7.7395e-03],\n",
            "          [ 1.6438e-03, -2.7983e-02,  1.7470e-02],\n",
            "          [ 8.8453e-03, -2.5530e-02, -3.8822e-02]],\n",
            "\n",
            "         [[-2.0720e-02,  2.3094e-02, -2.7083e-02],\n",
            "          [ 1.6527e-02,  9.4385e-03, -3.3637e-02],\n",
            "          [ 8.9717e-03,  1.3747e-02, -3.7026e-03]],\n",
            "\n",
            "         [[ 6.1563e-03,  3.1581e-02,  2.4194e-02],\n",
            "          [ 3.9051e-02,  2.3267e-02,  3.3342e-02],\n",
            "          [-1.9996e-02, -1.2256e-02,  4.1394e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.0139e-03, -3.5546e-02,  4.1221e-02],\n",
            "          [-4.7919e-03,  2.6578e-02, -3.0751e-02],\n",
            "          [-2.2357e-02, -4.2022e-03,  1.4037e-02]],\n",
            "\n",
            "         [[ 3.7090e-02,  3.3934e-02, -3.0337e-02],\n",
            "          [ 3.5255e-03,  1.7338e-02, -7.3951e-03],\n",
            "          [-3.1239e-02, -2.8715e-02, -3.8580e-02]],\n",
            "\n",
            "         [[-1.6500e-02, -1.1271e-02,  2.1485e-02],\n",
            "          [ 1.5523e-03, -3.2702e-02, -9.3907e-03],\n",
            "          [-4.6141e-03,  1.4500e-02,  1.8221e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.5368e-02,  3.2322e-02, -8.5642e-03],\n",
            "          [ 2.7348e-02,  4.1349e-02, -2.3749e-02],\n",
            "          [-2.6897e-02, -1.6848e-02, -3.6241e-02]],\n",
            "\n",
            "         [[ 4.0588e-02,  1.8699e-02,  2.9035e-02],\n",
            "          [-2.7404e-02, -2.6674e-04, -7.4361e-03],\n",
            "          [ 2.2870e-02, -1.6933e-02, -1.4910e-02]],\n",
            "\n",
            "         [[ 2.8941e-02,  2.6547e-02, -2.8597e-02],\n",
            "          [-3.2379e-02, -1.0560e-02,  1.4994e-02],\n",
            "          [ 9.1692e-03,  3.8592e-02,  3.6470e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.1022e-02, -4.1226e-02, -9.0186e-03],\n",
            "          [ 1.1126e-03,  3.5315e-02,  1.3608e-02],\n",
            "          [ 2.0506e-02, -2.4138e-02,  4.0186e-03]],\n",
            "\n",
            "         [[ 6.2433e-03, -3.2107e-02, -3.8721e-02],\n",
            "          [ 1.3318e-03, -1.9456e-03, -5.0473e-04],\n",
            "          [ 3.8692e-02,  1.3873e-02, -1.0921e-02]],\n",
            "\n",
            "         [[-1.8307e-02, -3.2004e-02, -1.5840e-03],\n",
            "          [ 3.8088e-03,  1.9754e-02, -3.0235e-02],\n",
            "          [-1.2063e-02,  1.1846e-03,  7.5163e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2522e-02, -1.2987e-02,  8.3566e-03],\n",
            "          [ 2.0034e-02, -4.0043e-02,  1.7759e-02],\n",
            "          [-2.7932e-02, -1.3813e-02,  4.0774e-03]],\n",
            "\n",
            "         [[-3.3873e-02,  3.4663e-02, -3.3265e-03],\n",
            "          [-2.3279e-02, -1.0102e-02,  2.5965e-02],\n",
            "          [-4.0968e-02, -4.6106e-03,  1.4650e-02]],\n",
            "\n",
            "         [[-2.2321e-02,  3.8404e-02, -1.1860e-02],\n",
            "          [ 2.4134e-02, -3.8874e-02,  4.0125e-02],\n",
            "          [-3.1767e-02,  3.4344e-02,  4.0002e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6283e-02, -2.7841e-02,  6.5153e-03],\n",
            "          [ 3.2353e-02, -1.3158e-02,  3.9919e-02],\n",
            "          [-2.4163e-02, -9.6645e-03,  3.8489e-02]],\n",
            "\n",
            "         [[-4.6889e-03,  1.9570e-02,  3.4370e-02],\n",
            "          [-1.4970e-02,  1.1083e-02,  3.2872e-02],\n",
            "          [-1.4432e-02,  2.8792e-03, -1.5465e-02]],\n",
            "\n",
            "         [[-8.2840e-03,  3.0202e-03,  4.4669e-03],\n",
            "          [ 3.5399e-02, -2.0658e-02, -1.1206e-02],\n",
            "          [-3.9702e-02,  4.0332e-02, -1.7033e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "down_list.2.seq.0.op.bias Parameter containing:\n",
            "tensor([ 0.0075, -0.0152, -0.0314,  0.0271, -0.0403, -0.0010,  0.0228, -0.0377,\n",
            "         0.0050, -0.0068,  0.0230, -0.0158, -0.0170,  0.0153, -0.0327, -0.0009,\n",
            "         0.0258,  0.0327,  0.0168,  0.0214,  0.0239,  0.0404,  0.0287, -0.0090,\n",
            "         0.0023,  0.0021, -0.0218, -0.0378, -0.0176, -0.0125, -0.0104, -0.0340,\n",
            "         0.0010, -0.0249, -0.0273, -0.0360,  0.0132, -0.0003,  0.0139,  0.0405,\n",
            "         0.0283, -0.0126, -0.0239,  0.0358,  0.0286,  0.0145,  0.0339, -0.0301,\n",
            "         0.0004,  0.0293, -0.0351,  0.0223,  0.0041, -0.0247, -0.0282,  0.0384,\n",
            "        -0.0400,  0.0304, -0.0088,  0.0177, -0.0203, -0.0066, -0.0222, -0.0172,\n",
            "        -0.0381,  0.0016, -0.0097,  0.0059, -0.0223,  0.0107,  0.0141,  0.0403,\n",
            "        -0.0171, -0.0200, -0.0020, -0.0371,  0.0191, -0.0257,  0.0385,  0.0149,\n",
            "        -0.0164, -0.0160, -0.0257,  0.0215,  0.0183, -0.0104,  0.0245, -0.0374,\n",
            "         0.0237, -0.0086,  0.0075,  0.0250, -0.0090,  0.0274,  0.0044,  0.0097,\n",
            "        -0.0008, -0.0226, -0.0218, -0.0005, -0.0173, -0.0109,  0.0108,  0.0224,\n",
            "        -0.0036, -0.0125, -0.0331, -0.0098, -0.0348,  0.0032,  0.0371,  0.0143,\n",
            "         0.0109, -0.0037, -0.0100,  0.0075,  0.0328, -0.0192,  0.0160,  0.0392,\n",
            "        -0.0122, -0.0345, -0.0172,  0.0086,  0.0181, -0.0085, -0.0052, -0.0259],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "middle_block.0.op.weight Parameter containing:\n",
            "tensor([[[[ 1.6511e-02,  2.7169e-02,  1.2173e-02],\n",
            "          [-8.9591e-04,  2.6655e-02,  1.5853e-03],\n",
            "          [ 1.5625e-02,  2.8629e-02,  7.1452e-03]],\n",
            "\n",
            "         [[-1.8613e-02, -9.1552e-03, -7.8881e-03],\n",
            "          [-1.0623e-02, -2.9066e-03,  1.9416e-02],\n",
            "          [ 2.2438e-03,  1.2550e-02,  1.8283e-02]],\n",
            "\n",
            "         [[ 1.3759e-02, -1.7329e-04,  2.0927e-02],\n",
            "          [ 2.5499e-02, -1.8378e-02, -2.8171e-02],\n",
            "          [ 5.3362e-03,  2.8946e-02,  1.3854e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1335e-02,  2.0389e-03, -2.4578e-02],\n",
            "          [ 2.2945e-02,  2.0912e-02,  1.3672e-02],\n",
            "          [ 3.0539e-03,  2.2203e-02,  2.3408e-02]],\n",
            "\n",
            "         [[-2.7223e-02,  9.9305e-03, -2.6947e-02],\n",
            "          [ 2.3384e-02,  2.8280e-02, -2.0857e-02],\n",
            "          [-1.5585e-02, -1.7323e-02,  7.6484e-03]],\n",
            "\n",
            "         [[-1.5722e-02, -1.7950e-02, -7.9909e-03],\n",
            "          [ 1.7915e-02, -1.5551e-02,  1.1243e-02],\n",
            "          [-2.5386e-02,  1.7857e-02,  1.4249e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.2897e-03,  1.9281e-04,  6.0394e-03],\n",
            "          [ 2.7751e-02, -1.9055e-02,  2.3018e-02],\n",
            "          [-9.0598e-03,  1.5311e-02,  1.8520e-02]],\n",
            "\n",
            "         [[-6.5560e-03,  1.4667e-03, -2.2297e-02],\n",
            "          [-1.9240e-02,  5.0799e-03,  2.9163e-02],\n",
            "          [ 5.8535e-03,  5.8367e-03, -3.5289e-03]],\n",
            "\n",
            "         [[-2.3438e-02, -1.1083e-02, -1.4789e-03],\n",
            "          [ 1.5409e-02, -1.6215e-03,  1.5590e-02],\n",
            "          [ 1.6771e-02, -9.0869e-04, -2.1501e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.9256e-03,  2.6816e-02,  3.6938e-03],\n",
            "          [ 6.8741e-03,  1.4664e-02,  8.9379e-03],\n",
            "          [-1.0377e-02, -1.4391e-02,  6.5989e-03]],\n",
            "\n",
            "         [[-2.5373e-02,  6.0654e-03,  2.0597e-02],\n",
            "          [ 1.7138e-02, -2.7661e-02,  2.7205e-02],\n",
            "          [-2.6837e-02, -2.6818e-02, -1.5941e-02]],\n",
            "\n",
            "         [[ 2.1343e-02, -1.7677e-02,  6.3744e-03],\n",
            "          [-1.6281e-02,  2.3314e-02,  1.1266e-02],\n",
            "          [-1.7932e-02,  2.2052e-02,  9.1843e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.1011e-02,  2.1141e-02,  5.9866e-03],\n",
            "          [-2.1998e-03,  1.8943e-02,  2.7669e-02],\n",
            "          [-5.1175e-03, -2.2400e-02,  1.5310e-02]],\n",
            "\n",
            "         [[-5.1080e-03,  1.2682e-02,  1.6896e-02],\n",
            "          [ 1.6349e-03,  2.6476e-02, -7.7761e-03],\n",
            "          [-2.1226e-02,  1.9924e-02,  6.3992e-03]],\n",
            "\n",
            "         [[ 2.5725e-02,  4.5863e-03, -1.5564e-02],\n",
            "          [ 2.5155e-03,  2.6736e-02,  1.3744e-02],\n",
            "          [-2.2917e-02, -1.8896e-02,  4.6641e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7191e-02,  4.7796e-03, -3.5436e-04],\n",
            "          [ 1.1682e-03,  1.7029e-02,  2.7329e-02],\n",
            "          [-1.7790e-02,  4.3233e-03,  2.8678e-02]],\n",
            "\n",
            "         [[ 8.5398e-03,  1.3731e-03,  1.5283e-02],\n",
            "          [-2.6402e-02, -2.3337e-02, -8.4882e-03],\n",
            "          [ 1.0670e-02, -1.7806e-02,  6.8486e-03]],\n",
            "\n",
            "         [[ 2.7681e-02,  8.5666e-03, -4.1243e-03],\n",
            "          [ 1.5278e-02,  1.4453e-02,  2.1421e-03],\n",
            "          [-2.3506e-02,  6.8305e-03, -6.8431e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-5.6842e-03,  1.0207e-02,  2.3415e-02],\n",
            "          [ 6.0677e-03, -2.0646e-02,  5.6089e-03],\n",
            "          [-1.0279e-02,  1.1576e-02,  5.2749e-03]],\n",
            "\n",
            "         [[-3.8441e-03, -1.6270e-02, -2.0247e-02],\n",
            "          [ 2.8407e-02,  2.0263e-02, -1.9656e-02],\n",
            "          [ 1.6521e-02,  1.1420e-02, -1.3793e-02]],\n",
            "\n",
            "         [[-2.0523e-02,  1.6200e-02, -2.7047e-02],\n",
            "          [ 2.2183e-02, -1.6506e-02,  2.9387e-02],\n",
            "          [ 1.2749e-02,  2.7781e-02, -2.6036e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7603e-02, -2.2466e-03, -9.8341e-03],\n",
            "          [ 1.1377e-02, -1.1246e-02, -8.4316e-03],\n",
            "          [-4.2446e-03, -2.7777e-03,  7.3719e-03]],\n",
            "\n",
            "         [[-1.9320e-02, -1.0922e-02,  5.7605e-03],\n",
            "          [-1.9062e-02,  2.4103e-02,  2.7147e-02],\n",
            "          [ 1.0407e-02,  1.0113e-03,  2.6850e-02]],\n",
            "\n",
            "         [[-9.5459e-04,  2.8326e-03, -7.5273e-03],\n",
            "          [-6.5210e-03,  2.9091e-02, -1.0584e-02],\n",
            "          [ 1.3057e-02,  1.8384e-02,  1.3286e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.5111e-04,  1.2068e-03, -1.3263e-02],\n",
            "          [ 2.4403e-02,  4.3464e-03, -1.9327e-02],\n",
            "          [ 1.1335e-02, -4.4939e-03,  9.0615e-04]],\n",
            "\n",
            "         [[ 2.5158e-02, -1.7762e-02, -2.9056e-02],\n",
            "          [-2.4148e-02, -2.9145e-02,  2.7773e-02],\n",
            "          [ 2.6428e-02, -1.5560e-02, -8.0797e-03]],\n",
            "\n",
            "         [[ 8.9640e-04,  2.3938e-02, -1.9626e-02],\n",
            "          [ 2.2561e-02,  1.5834e-02,  4.1639e-03],\n",
            "          [ 2.3284e-02,  2.0221e-02, -1.0014e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4390e-02, -4.3126e-03,  6.9368e-03],\n",
            "          [-2.1140e-03,  3.2379e-03,  2.5632e-02],\n",
            "          [-2.6360e-02,  5.2154e-03, -2.7877e-02]],\n",
            "\n",
            "         [[ 2.4041e-02, -2.9171e-02,  2.7072e-02],\n",
            "          [-2.1462e-02,  1.5526e-02, -2.4343e-05],\n",
            "          [-2.0826e-03, -2.6725e-02,  1.0530e-02]],\n",
            "\n",
            "         [[-1.2323e-02,  2.6103e-02,  9.2589e-03],\n",
            "          [ 1.4294e-02, -2.3036e-02, -2.1500e-02],\n",
            "          [ 1.4995e-02,  6.2710e-03, -5.5841e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.9650e-03, -2.1154e-02,  1.8254e-02],\n",
            "          [ 2.3710e-02, -7.0632e-03,  1.9629e-02],\n",
            "          [-7.7667e-03, -1.0649e-02,  8.0891e-03]],\n",
            "\n",
            "         [[ 1.1673e-02, -2.9407e-02, -1.8970e-02],\n",
            "          [-2.4082e-02,  2.8662e-02, -2.7162e-02],\n",
            "          [ 2.1678e-02, -2.2203e-02, -2.0231e-02]],\n",
            "\n",
            "         [[-2.2195e-02,  1.8404e-02,  2.8935e-02],\n",
            "          [-1.9124e-03, -2.5132e-02, -5.4985e-03],\n",
            "          [-2.3049e-02, -1.9410e-02, -1.6513e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7444e-03,  2.5952e-02,  3.1771e-03],\n",
            "          [-1.8209e-02,  3.4976e-03,  6.7080e-03],\n",
            "          [ 4.1599e-03, -1.6958e-03,  3.1740e-03]],\n",
            "\n",
            "         [[ 1.9901e-02,  1.8760e-02,  1.2005e-02],\n",
            "          [ 1.9577e-02, -2.8829e-02, -2.2056e-02],\n",
            "          [-8.3874e-03,  1.2635e-02,  2.7063e-03]],\n",
            "\n",
            "         [[-6.4591e-03, -1.3818e-02,  1.1736e-02],\n",
            "          [ 1.0375e-02,  1.2509e-02,  6.7317e-03],\n",
            "          [-7.8221e-03, -2.8640e-02,  2.3521e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "middle_block.0.op.bias Parameter containing:\n",
            "tensor([ 0.0182,  0.0114,  0.0200, -0.0238,  0.0197, -0.0058, -0.0024, -0.0214,\n",
            "         0.0064, -0.0028, -0.0150,  0.0023, -0.0022,  0.0096, -0.0036, -0.0187,\n",
            "        -0.0014,  0.0084,  0.0266, -0.0213,  0.0026,  0.0218,  0.0169,  0.0275,\n",
            "         0.0186,  0.0172, -0.0111, -0.0143, -0.0104,  0.0200,  0.0164, -0.0223,\n",
            "         0.0290,  0.0287, -0.0125,  0.0208,  0.0165,  0.0119, -0.0147,  0.0231,\n",
            "        -0.0240,  0.0009, -0.0279,  0.0076, -0.0255,  0.0118,  0.0183, -0.0264,\n",
            "         0.0003, -0.0148, -0.0169,  0.0260, -0.0291, -0.0254, -0.0152, -0.0087,\n",
            "        -0.0082,  0.0098,  0.0150,  0.0259,  0.0253,  0.0131,  0.0010, -0.0162,\n",
            "         0.0041, -0.0278, -0.0011, -0.0282, -0.0009,  0.0257,  0.0119, -0.0225,\n",
            "        -0.0087, -0.0154, -0.0113, -0.0257, -0.0135,  0.0068,  0.0201, -0.0096,\n",
            "        -0.0240,  0.0141,  0.0272,  0.0285, -0.0129, -0.0162,  0.0253, -0.0114,\n",
            "        -0.0195,  0.0068, -0.0230, -0.0222,  0.0001,  0.0126, -0.0037,  0.0219,\n",
            "         0.0143, -0.0140, -0.0273,  0.0139,  0.0217,  0.0030,  0.0273, -0.0290,\n",
            "        -0.0030,  0.0187, -0.0055, -0.0133,  0.0034, -0.0032, -0.0171,  0.0232,\n",
            "         0.0006,  0.0264, -0.0024,  0.0277,  0.0069, -0.0222,  0.0018, -0.0260,\n",
            "         0.0216, -0.0224, -0.0133,  0.0159, -0.0068, -0.0151,  0.0102, -0.0174],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "middle_block.1.conv.weight Parameter containing:\n",
            "tensor([[[[-2.7124e-02, -1.4129e-04, -5.5596e-03],\n",
            "          [-2.8648e-02,  9.1655e-05,  4.2822e-03],\n",
            "          [-1.1342e-02, -6.3964e-03, -1.0182e-02]],\n",
            "\n",
            "         [[ 1.2816e-04, -2.7801e-02,  8.4461e-03],\n",
            "          [ 2.6409e-02, -2.8939e-02,  3.9820e-03],\n",
            "          [ 1.6527e-02,  1.5991e-02,  1.1077e-02]],\n",
            "\n",
            "         [[ 8.4156e-03,  2.7010e-02,  8.6109e-03],\n",
            "          [ 3.3429e-03,  7.3794e-03,  1.1200e-02],\n",
            "          [-1.5330e-02,  2.8542e-02, -2.0823e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.5689e-03,  2.0079e-03, -1.7098e-03],\n",
            "          [-5.2345e-03,  5.6140e-03, -1.6366e-02],\n",
            "          [-2.5736e-03,  8.3180e-05, -2.8465e-02]],\n",
            "\n",
            "         [[ 1.7925e-02, -2.9096e-02, -2.2453e-02],\n",
            "          [ 2.9138e-02, -1.8868e-02,  1.4545e-02],\n",
            "          [ 5.0987e-03, -1.3847e-02, -9.3836e-03]],\n",
            "\n",
            "         [[ 1.9591e-02,  1.1395e-03,  1.9518e-02],\n",
            "          [-1.2783e-02, -2.3696e-03, -2.0907e-02],\n",
            "          [ 5.7853e-03,  1.6691e-02, -9.2021e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9918e-03,  2.0719e-02,  2.9425e-02],\n",
            "          [ 9.0421e-03, -1.5529e-03, -1.5867e-02],\n",
            "          [ 1.0914e-03,  4.9309e-03, -6.6003e-03]],\n",
            "\n",
            "         [[ 1.9235e-02,  1.7460e-02,  2.3387e-02],\n",
            "          [ 1.4797e-02, -8.3062e-03,  2.0575e-02],\n",
            "          [ 1.1613e-02, -1.1360e-02,  2.3480e-02]],\n",
            "\n",
            "         [[-1.7101e-02, -1.4345e-02, -2.3641e-02],\n",
            "          [ 6.6909e-03, -1.3377e-03, -2.6058e-03],\n",
            "          [ 1.8102e-02,  6.3911e-03, -1.5886e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.9914e-03, -2.2759e-02,  4.7253e-03],\n",
            "          [ 8.2879e-03,  2.1436e-02, -1.9497e-02],\n",
            "          [ 1.4909e-02, -1.6632e-02, -2.6420e-02]],\n",
            "\n",
            "         [[ 1.6299e-02,  4.0556e-03, -1.3156e-02],\n",
            "          [ 2.7127e-02,  8.3263e-03,  1.2018e-02],\n",
            "          [-2.0853e-02,  1.1650e-03, -2.7623e-02]],\n",
            "\n",
            "         [[ 2.0981e-02,  2.0116e-02, -2.6853e-03],\n",
            "          [ 1.3142e-02, -1.4789e-02, -9.8632e-03],\n",
            "          [ 2.8955e-04, -7.2612e-03,  1.5322e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4639e-02,  1.8715e-02,  2.7082e-02],\n",
            "          [-4.3736e-03, -2.5166e-02,  2.3944e-02],\n",
            "          [ 1.9807e-02,  2.4599e-02,  2.6752e-02]],\n",
            "\n",
            "         [[ 2.3506e-02, -1.9890e-02,  6.1461e-03],\n",
            "          [ 2.8653e-02,  2.7993e-02, -1.3762e-02],\n",
            "          [ 1.8183e-02,  1.9533e-02, -2.5192e-02]],\n",
            "\n",
            "         [[ 2.4422e-02,  8.7733e-03,  1.7901e-03],\n",
            "          [ 9.0343e-04, -2.3994e-02,  1.8389e-02],\n",
            "          [ 2.6110e-02, -2.3319e-02, -1.0450e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0618e-02,  2.1487e-02,  5.4922e-03],\n",
            "          [ 2.7534e-03,  2.7351e-02,  9.4554e-03],\n",
            "          [ 2.1106e-02,  1.6057e-02, -2.2928e-02]],\n",
            "\n",
            "         [[ 7.0810e-03, -2.6794e-02, -1.5995e-02],\n",
            "          [ 4.4964e-03, -7.3513e-03,  2.1656e-02],\n",
            "          [-1.7719e-02,  4.6913e-03, -1.9998e-02]],\n",
            "\n",
            "         [[-2.3414e-02,  1.4244e-02, -1.4365e-02],\n",
            "          [-2.4816e-02,  3.9711e-04, -2.7614e-02],\n",
            "          [-2.4327e-02,  1.7839e-02, -3.2335e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.8865e-02,  1.8822e-02, -2.8658e-02],\n",
            "          [ 8.9882e-03,  9.7525e-03,  8.0182e-03],\n",
            "          [ 2.4076e-02, -1.7111e-02, -9.9372e-03]],\n",
            "\n",
            "         [[ 1.6817e-02, -1.8008e-02,  1.3578e-02],\n",
            "          [ 2.4754e-02,  2.1705e-02,  2.7733e-02],\n",
            "          [-2.7316e-03,  1.7354e-02, -7.0209e-03]],\n",
            "\n",
            "         [[ 1.4594e-02,  2.7589e-02,  1.1227e-02],\n",
            "          [ 2.3133e-02, -5.8473e-03,  4.2925e-03],\n",
            "          [ 2.5546e-03,  1.9152e-02, -1.4141e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3469e-02, -2.2923e-02,  1.7323e-02],\n",
            "          [ 3.1156e-03, -1.1853e-02,  1.4013e-02],\n",
            "          [ 1.8019e-03,  2.6055e-02,  9.1306e-04]],\n",
            "\n",
            "         [[-1.9795e-02,  2.8124e-02, -2.0987e-02],\n",
            "          [ 4.6385e-03, -2.1914e-02,  1.1804e-03],\n",
            "          [ 2.7110e-02, -1.9148e-02, -4.2218e-03]],\n",
            "\n",
            "         [[-9.4827e-03, -6.0202e-03, -9.8494e-03],\n",
            "          [ 2.1143e-02,  2.0277e-02,  3.4032e-03],\n",
            "          [ 1.3103e-02, -2.2522e-02,  7.5488e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0653e-03, -1.5500e-02, -2.0064e-02],\n",
            "          [-4.6264e-03,  8.1968e-03, -1.6239e-02],\n",
            "          [ 2.5175e-02, -1.1150e-02,  3.9432e-03]],\n",
            "\n",
            "         [[ 1.3187e-02,  2.6184e-02,  1.4554e-02],\n",
            "          [ 2.3317e-03,  3.4242e-03,  2.0143e-03],\n",
            "          [-2.3507e-02, -1.0579e-03, -2.8341e-02]],\n",
            "\n",
            "         [[-9.6817e-03,  2.4111e-02, -2.6424e-02],\n",
            "          [ 9.5669e-03, -9.3087e-03,  7.9218e-03],\n",
            "          [ 2.7359e-02,  1.3136e-02, -1.2178e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.2982e-03,  2.5259e-02, -2.3317e-02],\n",
            "          [-1.0194e-02, -2.2402e-02,  1.5014e-02],\n",
            "          [ 1.7573e-03,  1.0470e-02,  7.4842e-03]],\n",
            "\n",
            "         [[ 1.7333e-02, -2.2975e-02, -2.3168e-02],\n",
            "          [-6.7967e-03,  1.0064e-02,  1.7280e-02],\n",
            "          [-2.3368e-02, -9.1237e-03,  2.1110e-02]],\n",
            "\n",
            "         [[ 6.2271e-03, -2.6425e-02, -1.9229e-02],\n",
            "          [ 2.6299e-02,  2.6812e-02, -1.1265e-02],\n",
            "          [ 4.1571e-04,  1.1013e-03,  2.3275e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.8036e-02, -1.7038e-02,  2.0841e-02],\n",
            "          [ 1.8218e-02, -1.0475e-02, -2.1606e-03],\n",
            "          [ 7.3855e-03,  2.1376e-02, -1.3288e-02]],\n",
            "\n",
            "         [[-1.4110e-03, -1.4637e-02, -1.9026e-02],\n",
            "          [-2.8117e-03,  5.5363e-03, -6.8674e-03],\n",
            "          [-2.7630e-02,  2.7989e-02, -1.1139e-02]],\n",
            "\n",
            "         [[-2.4907e-02,  2.5976e-02, -2.1847e-02],\n",
            "          [ 7.7697e-03, -1.6553e-02, -8.1509e-03],\n",
            "          [ 1.5014e-02, -1.7716e-02, -2.1988e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7449e-02,  1.0440e-02, -1.7251e-02],\n",
            "          [-1.5821e-03, -1.3674e-02, -2.4873e-02],\n",
            "          [ 6.4570e-03, -2.7881e-02,  9.3303e-03]],\n",
            "\n",
            "         [[-2.5408e-03, -1.0775e-02,  2.1507e-02],\n",
            "          [ 2.5875e-02,  2.7415e-02, -1.4571e-02],\n",
            "          [ 2.9117e-02, -2.9288e-02,  2.5921e-02]],\n",
            "\n",
            "         [[-9.6872e-03,  2.6715e-02,  1.4247e-03],\n",
            "          [-1.5591e-02,  1.7395e-02, -2.1699e-02],\n",
            "          [ 2.4978e-02,  2.9446e-02,  1.2344e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "middle_block.1.conv.bias Parameter containing:\n",
            "tensor([ 0.0288, -0.0249, -0.0068, -0.0180, -0.0200, -0.0169,  0.0277, -0.0201,\n",
            "         0.0284, -0.0091,  0.0084,  0.0129,  0.0234,  0.0162, -0.0210, -0.0035,\n",
            "        -0.0270, -0.0074,  0.0193, -0.0080, -0.0209,  0.0154,  0.0214, -0.0126,\n",
            "        -0.0269, -0.0036,  0.0024,  0.0155,  0.0278,  0.0214, -0.0277,  0.0258,\n",
            "        -0.0254,  0.0224,  0.0112, -0.0204,  0.0133, -0.0231, -0.0120, -0.0171,\n",
            "         0.0030, -0.0234,  0.0219,  0.0262,  0.0078, -0.0015, -0.0015,  0.0259,\n",
            "        -0.0113,  0.0028, -0.0042,  0.0223, -0.0061,  0.0193, -0.0014,  0.0133,\n",
            "         0.0163,  0.0099, -0.0102, -0.0255, -0.0290, -0.0084, -0.0111, -0.0289,\n",
            "         0.0184, -0.0114, -0.0196,  0.0193,  0.0164, -0.0017,  0.0207,  0.0114,\n",
            "        -0.0075, -0.0229, -0.0053,  0.0154, -0.0075, -0.0124,  0.0074, -0.0178,\n",
            "        -0.0249, -0.0114,  0.0024,  0.0189, -0.0007,  0.0104,  0.0221,  0.0265,\n",
            "         0.0090, -0.0237,  0.0249,  0.0226, -0.0114,  0.0153,  0.0055,  0.0129,\n",
            "        -0.0260, -0.0022, -0.0251, -0.0280, -0.0214, -0.0201, -0.0293, -0.0208,\n",
            "         0.0075, -0.0142, -0.0269,  0.0285, -0.0085,  0.0278, -0.0023,  0.0117,\n",
            "         0.0040, -0.0051, -0.0033,  0.0070, -0.0142, -0.0183, -0.0250,  0.0178,\n",
            "         0.0153,  0.0278,  0.0206,  0.0016, -0.0255,  0.0033,  0.0228, -0.0053],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "up_list.0.seq.1.conv.weight Parameter containing:\n",
            "tensor([[[[ 1.2178e-02, -1.4625e-02,  1.3634e-02],\n",
            "          [-6.7237e-03, -1.7854e-02,  1.6838e-03],\n",
            "          [ 2.0348e-02,  1.3006e-02, -2.4911e-03]],\n",
            "\n",
            "         [[-6.5263e-03,  6.4347e-03,  1.9998e-02],\n",
            "          [ 8.0019e-03, -1.6153e-02, -1.8881e-02],\n",
            "          [-1.1309e-02,  1.2784e-02, -1.2217e-02]],\n",
            "\n",
            "         [[-1.6233e-02, -9.6172e-03, -1.4462e-02],\n",
            "          [ 7.2302e-04,  1.5300e-02, -3.3127e-03],\n",
            "          [-9.9950e-03, -5.9617e-03,  1.7664e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2252e-02, -2.0312e-02,  1.3435e-02],\n",
            "          [-1.0444e-02, -1.8721e-02, -7.4050e-03],\n",
            "          [ 1.9384e-02, -1.5446e-03, -3.8271e-04]],\n",
            "\n",
            "         [[ 9.7935e-03,  1.9179e-02, -1.2304e-02],\n",
            "          [ 2.8969e-03, -1.3438e-02, -1.4884e-02],\n",
            "          [ 1.4818e-02, -1.9055e-02, -1.3905e-02]],\n",
            "\n",
            "         [[-7.5570e-03, -5.7465e-04, -1.5217e-02],\n",
            "          [-1.8188e-03, -1.1901e-02, -9.8235e-03],\n",
            "          [ 5.9755e-03, -2.7424e-03, -1.5260e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6131e-03,  8.1023e-03,  1.9565e-02],\n",
            "          [ 5.1280e-03, -2.2788e-03,  1.9174e-02],\n",
            "          [ 1.5825e-02, -4.3656e-03, -1.1125e-03]],\n",
            "\n",
            "         [[-1.7790e-02,  8.0361e-03, -6.9925e-03],\n",
            "          [ 3.5513e-03, -1.7311e-02,  1.4796e-02],\n",
            "          [ 1.4716e-02,  1.7980e-02,  1.9147e-02]],\n",
            "\n",
            "         [[-1.3496e-02,  5.4223e-03,  2.0137e-02],\n",
            "          [ 1.8017e-02, -1.5579e-02, -1.1729e-02],\n",
            "          [-5.9686e-03, -1.6980e-02,  7.6176e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.9278e-03,  4.4053e-03,  1.9151e-03],\n",
            "          [-2.0796e-02,  4.3504e-04, -1.5811e-02],\n",
            "          [-9.1001e-03,  7.5680e-03,  1.7414e-02]],\n",
            "\n",
            "         [[ 1.0345e-02, -1.2826e-02, -2.3786e-03],\n",
            "          [ 1.8005e-02,  4.4644e-03, -3.2477e-03],\n",
            "          [ 2.7544e-03, -7.1768e-03, -1.4188e-02]],\n",
            "\n",
            "         [[ 1.9062e-02, -1.4281e-02,  6.2779e-03],\n",
            "          [-7.2453e-03, -1.6156e-02, -2.3720e-03],\n",
            "          [ 1.6042e-02, -9.0640e-03, -1.4322e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7287e-02, -2.0053e-02, -7.1318e-03],\n",
            "          [-1.3167e-02,  1.5236e-02,  1.4479e-02],\n",
            "          [-1.9420e-02,  7.4235e-04, -1.6118e-02]],\n",
            "\n",
            "         [[-1.4958e-02,  2.2718e-03,  2.8589e-04],\n",
            "          [-2.1284e-06,  2.4839e-03,  1.2156e-02],\n",
            "          [ 4.5887e-03, -1.1546e-02, -2.0292e-02]],\n",
            "\n",
            "         [[ 3.2628e-03,  4.2633e-03, -1.0684e-02],\n",
            "          [ 3.4678e-03, -1.2600e-03,  1.6282e-02],\n",
            "          [-1.7936e-02, -1.1701e-02,  1.4023e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5395e-02, -1.9888e-02,  1.0300e-02],\n",
            "          [ 9.5273e-03, -2.8750e-03, -3.7884e-03],\n",
            "          [-1.4786e-02,  1.0133e-02,  1.9705e-02]],\n",
            "\n",
            "         [[-5.6480e-03,  1.1851e-02, -1.6208e-02],\n",
            "          [-1.2840e-02, -2.1953e-03, -1.4669e-02],\n",
            "          [-2.5302e-03, -5.5368e-03, -1.6901e-02]],\n",
            "\n",
            "         [[ 2.5807e-03, -1.2506e-02, -1.3217e-02],\n",
            "          [ 1.1574e-02, -2.0777e-02, -2.0545e-02],\n",
            "          [ 1.0649e-02,  1.3441e-02,  1.5741e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.2544e-02, -1.3096e-02, -1.8765e-02],\n",
            "          [ 5.4426e-03,  1.0221e-02,  1.4345e-02],\n",
            "          [-3.1466e-03, -3.6452e-03,  3.4227e-03]],\n",
            "\n",
            "         [[-1.7318e-02,  6.2058e-03,  2.1423e-03],\n",
            "          [-4.1924e-03,  1.2316e-02, -6.9470e-03],\n",
            "          [-1.6570e-02, -4.0030e-03,  1.9492e-02]],\n",
            "\n",
            "         [[ 4.6714e-03, -1.2182e-02,  5.3087e-03],\n",
            "          [-1.1928e-03, -1.0704e-02, -1.5978e-02],\n",
            "          [ 1.2120e-03,  6.5756e-03,  8.9940e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.0359e-03,  1.8673e-02, -2.0304e-03],\n",
            "          [ 2.0133e-02,  6.4376e-03,  1.8764e-03],\n",
            "          [-1.0269e-02,  1.1000e-02,  3.3787e-03]],\n",
            "\n",
            "         [[-1.2705e-02, -1.7754e-02,  3.5790e-03],\n",
            "          [ 7.4347e-03,  1.8143e-02, -3.6800e-03],\n",
            "          [ 1.3096e-02,  4.8140e-03, -2.4841e-03]],\n",
            "\n",
            "         [[ 9.2413e-03,  1.5120e-02, -7.0762e-03],\n",
            "          [ 1.0200e-02, -1.8856e-02,  2.0308e-02],\n",
            "          [-1.2987e-02, -6.4287e-03, -1.7552e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.1232e-02,  3.9954e-04, -1.2075e-02],\n",
            "          [-6.2019e-03,  5.1143e-03,  1.1078e-02],\n",
            "          [ 1.9404e-02, -2.9095e-03,  4.2707e-03]],\n",
            "\n",
            "         [[ 1.0394e-02,  1.0153e-02, -6.1324e-03],\n",
            "          [-1.6753e-02,  1.5865e-02,  5.2048e-03],\n",
            "          [-4.2831e-05,  4.2127e-03, -1.5628e-02]],\n",
            "\n",
            "         [[ 1.1236e-02, -1.5203e-02,  5.8068e-03],\n",
            "          [ 1.1739e-02, -2.0368e-02,  1.5012e-02],\n",
            "          [-8.1283e-04, -1.0611e-02, -1.2304e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7184e-02,  8.3464e-03, -1.0214e-02],\n",
            "          [-2.0573e-02, -6.5966e-03,  9.8026e-03],\n",
            "          [-1.5955e-02, -2.0028e-02,  9.6563e-03]],\n",
            "\n",
            "         [[-1.6583e-02,  1.9202e-02,  4.5119e-03],\n",
            "          [-1.2130e-02,  1.1013e-02,  1.2027e-02],\n",
            "          [-8.4299e-03,  1.5955e-02, -1.7051e-03]],\n",
            "\n",
            "         [[-1.0607e-02, -4.8037e-03, -7.5257e-03],\n",
            "          [ 1.0035e-02,  1.2616e-02, -7.2938e-03],\n",
            "          [ 1.7995e-02,  1.1582e-02, -3.4889e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.8663e-02, -1.1187e-02, -3.4194e-03],\n",
            "          [-1.9767e-02,  9.8192e-03, -1.3129e-02],\n",
            "          [ 6.6414e-04, -6.1334e-03,  1.3066e-02]],\n",
            "\n",
            "         [[ 1.7349e-02,  1.7531e-03,  9.5390e-03],\n",
            "          [-1.3382e-02,  2.0150e-02,  9.3625e-03],\n",
            "          [-1.6263e-02,  7.9678e-03, -1.5458e-03]],\n",
            "\n",
            "         [[-1.8342e-02, -1.4577e-02,  1.2381e-02],\n",
            "          [-7.1361e-03,  1.6212e-02, -2.0208e-02],\n",
            "          [ 9.0757e-03,  1.8477e-02, -4.9127e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9857e-02, -1.8657e-02,  3.6783e-03],\n",
            "          [-9.7428e-03,  9.8331e-03,  2.7521e-03],\n",
            "          [-2.0689e-03,  1.7715e-02, -2.0065e-02]],\n",
            "\n",
            "         [[ 2.8941e-03,  1.5688e-02, -1.4477e-02],\n",
            "          [-6.4608e-03,  1.5145e-03, -1.2858e-02],\n",
            "          [ 3.8418e-03, -6.8440e-03, -1.1480e-02]],\n",
            "\n",
            "         [[ 4.8921e-03, -1.1939e-02, -4.8364e-03],\n",
            "          [-2.0531e-02, -1.2628e-02,  3.8417e-03],\n",
            "          [ 1.5761e-02,  7.6010e-03, -3.0595e-03]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "up_list.0.seq.1.conv.bias Parameter containing:\n",
            "tensor([ 6.6159e-03, -1.7619e-02, -1.1533e-02, -3.8730e-03, -2.0523e-02,\n",
            "        -1.8949e-02,  1.9065e-02,  3.9563e-03, -1.4665e-03, -1.2790e-02,\n",
            "         1.9890e-02, -1.8340e-02,  3.6726e-03,  1.2918e-02, -1.1727e-02,\n",
            "        -1.5606e-02,  8.6611e-03,  1.9487e-02, -4.0691e-03,  2.7961e-03,\n",
            "         8.8759e-03, -1.2992e-02,  7.4907e-03,  1.7602e-02,  8.9878e-04,\n",
            "         1.6015e-02,  1.1209e-02, -1.3986e-02, -1.9863e-02,  2.0312e-02,\n",
            "        -1.5496e-02, -5.3411e-03, -4.7623e-03, -2.3684e-03, -7.7267e-03,\n",
            "        -2.6882e-03,  1.8074e-02,  9.3590e-03,  3.5106e-03,  8.5254e-03,\n",
            "        -1.7014e-02, -1.1731e-02, -1.9443e-04, -2.0552e-02,  8.0536e-04,\n",
            "        -1.4698e-02,  1.3732e-02,  8.3273e-05,  1.1684e-02,  5.0980e-03,\n",
            "         1.8465e-02, -1.2891e-02, -9.7231e-03, -4.5447e-03, -1.6648e-02,\n",
            "        -1.0133e-02, -4.1326e-03,  5.1201e-03,  2.4477e-03, -7.9485e-03,\n",
            "         6.0952e-03, -1.3984e-02,  1.5643e-02,  7.5556e-03], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "up_list.1.seq.1.conv.weight Parameter containing:\n",
            "tensor([[[[-9.4414e-03, -1.1499e-02, -1.9475e-03],\n",
            "          [-1.8894e-02,  1.7156e-02, -1.9799e-02],\n",
            "          [ 6.2130e-03,  6.7344e-03,  4.6342e-03]],\n",
            "\n",
            "         [[ 1.5937e-02, -1.5396e-02,  1.2214e-04],\n",
            "          [ 1.3682e-02,  3.2749e-03, -2.4468e-03],\n",
            "          [-1.7109e-03,  6.8893e-03, -2.6769e-02]],\n",
            "\n",
            "         [[-1.5989e-02, -2.8295e-02, -6.2622e-03],\n",
            "          [-1.0239e-02, -6.0463e-03, -2.8693e-02],\n",
            "          [ 3.3098e-03, -1.7544e-02, -2.4190e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.2527e-03, -1.8194e-02,  1.4135e-02],\n",
            "          [ 1.4587e-02,  7.9812e-03, -2.5187e-03],\n",
            "          [-1.6773e-02,  2.6819e-02, -7.4701e-03]],\n",
            "\n",
            "         [[-1.8165e-02,  2.8214e-02,  2.4324e-02],\n",
            "          [ 1.1917e-02, -8.8203e-03, -1.5206e-02],\n",
            "          [-1.2493e-02, -2.8548e-02, -1.9690e-02]],\n",
            "\n",
            "         [[ 2.6125e-04,  2.8133e-02,  1.3402e-02],\n",
            "          [ 2.5953e-02, -1.8710e-02, -2.4154e-02],\n",
            "          [-1.0090e-02, -1.0087e-02,  4.5357e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.6422e-02,  1.7137e-02, -5.0044e-03],\n",
            "          [ 2.4837e-02, -2.8079e-02, -4.9523e-06],\n",
            "          [ 2.6867e-02, -7.0660e-03, -1.9307e-02]],\n",
            "\n",
            "         [[-2.0916e-02, -1.0584e-02,  2.5157e-02],\n",
            "          [-2.9330e-02, -2.1067e-02,  1.6937e-02],\n",
            "          [-4.3769e-03,  6.8218e-03,  4.5962e-03]],\n",
            "\n",
            "         [[ 2.2493e-02,  1.5589e-02, -9.9921e-03],\n",
            "          [-5.9203e-03, -1.4234e-02, -4.4527e-03],\n",
            "          [-1.1381e-03,  2.5711e-03, -6.8092e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.6671e-03,  8.7496e-03, -3.2111e-03],\n",
            "          [ 7.8674e-03,  9.1145e-03,  7.3993e-03],\n",
            "          [-3.6220e-03, -2.4029e-02,  1.7067e-02]],\n",
            "\n",
            "         [[ 1.4204e-03, -2.7290e-02,  1.4347e-03],\n",
            "          [ 2.5129e-02,  1.7595e-03, -2.6162e-02],\n",
            "          [-1.1487e-02, -2.0249e-03,  1.2181e-02]],\n",
            "\n",
            "         [[ 1.9255e-02, -1.0755e-02, -1.5639e-02],\n",
            "          [-2.2987e-02,  1.1437e-04, -2.2610e-02],\n",
            "          [-2.0146e-02,  1.0814e-02, -1.6142e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3191e-02,  2.6173e-02, -2.6187e-02],\n",
            "          [ 2.0815e-02,  2.7968e-02, -1.1357e-02],\n",
            "          [ 1.6860e-02, -2.0942e-03,  1.6466e-02]],\n",
            "\n",
            "         [[-2.8675e-03,  4.7305e-03,  2.3063e-02],\n",
            "          [ 2.8902e-02,  9.8081e-03,  5.4081e-03],\n",
            "          [-1.2186e-02, -1.2640e-02, -1.7827e-02]],\n",
            "\n",
            "         [[-2.4723e-02, -1.0331e-02, -1.8832e-02],\n",
            "          [ 2.1374e-02, -2.5464e-02, -6.9691e-03],\n",
            "          [ 2.8498e-02, -1.1285e-02,  1.3205e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1154e-02, -2.7369e-02, -2.8676e-02],\n",
            "          [ 1.0974e-02, -4.0197e-03, -7.7203e-03],\n",
            "          [-2.7906e-02,  1.0808e-02,  2.7856e-02]],\n",
            "\n",
            "         [[-2.6119e-02, -7.4243e-03, -3.8128e-03],\n",
            "          [ 5.3864e-03, -1.7512e-02,  2.1972e-02],\n",
            "          [-5.2207e-04, -5.7985e-03, -2.3984e-02]],\n",
            "\n",
            "         [[ 1.8483e-02,  9.6689e-03,  8.9953e-03],\n",
            "          [-2.0853e-02, -1.6792e-02,  2.2953e-02],\n",
            "          [ 1.7505e-02,  1.9294e-02,  1.7798e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.7527e-02, -2.8806e-02, -7.0182e-03],\n",
            "          [ 2.6070e-02,  7.6792e-03,  1.0972e-03],\n",
            "          [-1.8311e-02, -2.1800e-02,  1.3200e-02]],\n",
            "\n",
            "         [[-2.7421e-02,  2.8122e-02, -2.9049e-02],\n",
            "          [ 1.1544e-02,  2.1746e-03, -1.1577e-02],\n",
            "          [-2.2710e-02, -1.5060e-02,  7.0348e-03]],\n",
            "\n",
            "         [[ 2.1626e-02, -2.8228e-02, -2.3368e-02],\n",
            "          [ 2.2113e-02, -3.6350e-03,  1.9908e-02],\n",
            "          [-1.5469e-02,  2.7073e-02,  2.0555e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0144e-02,  5.8759e-03, -1.8174e-03],\n",
            "          [-1.1128e-02,  1.3649e-02, -2.1578e-02],\n",
            "          [-7.8314e-03,  8.9255e-03,  1.0558e-02]],\n",
            "\n",
            "         [[-2.4856e-02,  2.1859e-02,  6.7304e-03],\n",
            "          [-9.4852e-03, -2.7815e-03,  2.1799e-02],\n",
            "          [-2.2040e-02,  1.0139e-02, -6.3979e-03]],\n",
            "\n",
            "         [[ 1.1488e-03,  1.0084e-02,  2.2542e-02],\n",
            "          [ 2.6484e-02, -1.5473e-02, -2.7204e-02],\n",
            "          [-1.1691e-02,  2.3600e-02,  1.6157e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.9820e-03, -9.1018e-03,  2.8680e-02],\n",
            "          [-2.5566e-02, -2.7268e-02, -8.6469e-03],\n",
            "          [ 7.6136e-03, -9.1186e-03,  1.7368e-02]],\n",
            "\n",
            "         [[-2.0113e-02,  1.6829e-02, -1.6113e-02],\n",
            "          [ 1.2256e-02, -6.4913e-04, -1.1327e-02],\n",
            "          [ 1.4610e-02, -2.4637e-02,  2.2964e-02]],\n",
            "\n",
            "         [[ 6.6662e-04,  2.7905e-03, -4.0038e-03],\n",
            "          [-1.0400e-02,  1.1932e-02, -2.1401e-02],\n",
            "          [-2.4087e-03,  7.8080e-03,  1.8136e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6240e-02, -2.0053e-02,  5.8367e-03],\n",
            "          [ 2.5885e-02, -1.1337e-03,  3.1996e-03],\n",
            "          [-1.7317e-02,  1.1637e-02, -2.4410e-02]],\n",
            "\n",
            "         [[-1.6648e-02, -1.1724e-02,  2.3288e-02],\n",
            "          [ 8.1422e-03,  8.5990e-03, -1.3401e-02],\n",
            "          [-2.4539e-02,  1.0776e-02, -1.3922e-02]],\n",
            "\n",
            "         [[ 2.8937e-02, -2.0658e-02,  2.4238e-02],\n",
            "          [ 2.9271e-02,  1.2865e-02, -1.4224e-02],\n",
            "          [ 4.7668e-03,  2.0508e-02,  2.7306e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2579e-03, -8.2128e-03,  1.5785e-02],\n",
            "          [-1.6732e-05, -2.5867e-02,  1.2486e-03],\n",
            "          [-1.4825e-02, -1.3620e-02, -2.0601e-02]],\n",
            "\n",
            "         [[-1.3372e-02,  2.3866e-02, -1.1089e-02],\n",
            "          [-1.9484e-02,  1.1546e-02,  5.5824e-03],\n",
            "          [-5.4077e-03, -7.0484e-03,  8.0404e-03]],\n",
            "\n",
            "         [[ 1.6530e-02, -2.7884e-02,  5.5697e-03],\n",
            "          [-1.9950e-02, -2.5445e-02, -2.4119e-02],\n",
            "          [-2.0719e-03,  2.5471e-02, -6.7444e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.4791e-03,  1.2986e-03,  4.3684e-03],\n",
            "          [ 8.3679e-04, -1.8443e-02,  1.5717e-02],\n",
            "          [ 2.2860e-02,  2.7104e-02, -1.3813e-02]],\n",
            "\n",
            "         [[ 3.6870e-03,  1.4401e-02, -2.7325e-02],\n",
            "          [-1.2469e-02,  2.2307e-02, -7.3273e-03],\n",
            "          [ 1.6502e-02,  5.7979e-03,  1.8238e-02]],\n",
            "\n",
            "         [[ 6.4352e-04,  2.1574e-02,  3.6523e-03],\n",
            "          [-9.3234e-03,  1.3922e-02, -3.8805e-03],\n",
            "          [-2.7431e-02,  1.6424e-02, -2.8236e-03]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "up_list.1.seq.1.conv.bias Parameter containing:\n",
            "tensor([-0.0146,  0.0155, -0.0002,  0.0274, -0.0001,  0.0213, -0.0013,  0.0242,\n",
            "         0.0109, -0.0236,  0.0018, -0.0038,  0.0162,  0.0138,  0.0118,  0.0008,\n",
            "        -0.0036,  0.0074,  0.0221, -0.0248, -0.0179,  0.0138, -0.0063,  0.0132,\n",
            "        -0.0269, -0.0236,  0.0194,  0.0018,  0.0038,  0.0095,  0.0058,  0.0209],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "up_list.2.seq.1.conv.weight Parameter containing:\n",
            "tensor([[[[ 2.8734e-02, -1.0500e-02,  2.4912e-02],\n",
            "          [ 2.6130e-02,  2.3825e-02, -2.7578e-02],\n",
            "          [ 2.1958e-02, -2.6949e-03,  2.6049e-03]],\n",
            "\n",
            "         [[-3.8639e-02, -2.5091e-02,  6.0188e-03],\n",
            "          [ 2.0324e-02, -1.2920e-02,  4.8246e-03],\n",
            "          [-1.7588e-02,  3.6412e-02, -2.3842e-02]],\n",
            "\n",
            "         [[-2.9751e-02,  2.8198e-02, -3.6256e-02],\n",
            "          [-2.0885e-02, -1.5764e-02,  6.8139e-04],\n",
            "          [-1.3401e-02, -3.6874e-02, -9.5679e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4154e-02,  1.0420e-02,  1.3969e-02],\n",
            "          [ 1.0891e-02, -2.2813e-02, -2.1565e-02],\n",
            "          [-7.9462e-03,  7.0408e-03,  2.2012e-02]],\n",
            "\n",
            "         [[-8.6030e-03,  3.6672e-03,  8.0224e-03],\n",
            "          [ 3.6488e-02, -9.1181e-04, -1.6096e-02],\n",
            "          [ 2.9076e-02, -2.0952e-02, -1.0277e-02]],\n",
            "\n",
            "         [[ 3.0841e-02, -2.4822e-02, -1.1908e-02],\n",
            "          [-1.1405e-02, -6.0182e-03, -3.5095e-02],\n",
            "          [ 1.1067e-02,  6.2312e-03, -2.0763e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9929e-02,  2.9525e-02, -8.0381e-03],\n",
            "          [-4.1610e-02,  1.1581e-02,  1.6197e-02],\n",
            "          [-1.3149e-02,  9.2094e-04, -2.5403e-02]],\n",
            "\n",
            "         [[-1.4476e-02,  3.8800e-03,  4.1595e-02],\n",
            "          [ 1.8366e-03,  2.7637e-02, -1.1670e-02],\n",
            "          [-1.7812e-02, -2.8902e-02,  2.2231e-03]],\n",
            "\n",
            "         [[-4.1063e-03,  8.0395e-03,  2.4206e-02],\n",
            "          [ 2.7683e-02, -4.0444e-02, -2.1734e-02],\n",
            "          [-1.3198e-02,  2.0194e-02,  4.1521e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3602e-02,  5.4860e-03,  2.1716e-02],\n",
            "          [-4.0529e-02,  3.6193e-02,  6.4727e-03],\n",
            "          [-4.1014e-02, -2.4611e-02,  1.1565e-02]],\n",
            "\n",
            "         [[-1.7626e-03, -2.9446e-02,  8.9475e-03],\n",
            "          [ 7.0341e-04,  1.7885e-02,  3.4627e-02],\n",
            "          [-1.1631e-02, -2.2259e-02,  2.4549e-02]],\n",
            "\n",
            "         [[-2.6193e-02,  1.9654e-02,  1.6293e-02],\n",
            "          [ 1.2349e-02, -1.6266e-02, -3.1049e-02],\n",
            "          [-2.6694e-02, -1.4618e-02, -3.5923e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.6362e-03, -4.0715e-02,  1.5639e-02],\n",
            "          [ 1.4511e-02, -2.0645e-02,  1.1015e-02],\n",
            "          [-2.3756e-02,  1.6826e-02,  1.7174e-02]],\n",
            "\n",
            "         [[-1.5441e-02, -2.8488e-02, -1.7785e-02],\n",
            "          [ 3.5117e-02,  2.6216e-02, -2.7889e-02],\n",
            "          [ 2.0092e-03,  3.9429e-02,  2.0424e-02]],\n",
            "\n",
            "         [[ 3.1316e-02,  6.3148e-03,  6.9825e-03],\n",
            "          [ 2.3348e-02,  1.0913e-02,  2.7948e-02],\n",
            "          [-1.5766e-02, -1.9524e-02,  1.4587e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.6417e-02, -3.6763e-02, -2.1813e-02],\n",
            "          [ 8.4893e-03, -1.2866e-02,  3.8829e-02],\n",
            "          [-2.9269e-02, -2.2340e-02, -1.3012e-02]],\n",
            "\n",
            "         [[ 2.2864e-02, -3.9492e-02, -1.2548e-02],\n",
            "          [-1.2305e-02, -3.5634e-02,  1.1610e-02],\n",
            "          [-3.3187e-02, -3.0643e-02,  1.3924e-02]],\n",
            "\n",
            "         [[ 2.6454e-02,  1.7635e-02,  2.6126e-02],\n",
            "          [ 3.9769e-02,  3.4887e-02, -1.4425e-02],\n",
            "          [-1.0912e-02,  2.0022e-02, -9.6131e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.3740e-02, -1.8320e-02,  2.5093e-02],\n",
            "          [ 2.2288e-02,  2.5203e-02, -1.7357e-02],\n",
            "          [ 1.5867e-02,  3.2415e-02, -2.8448e-02]],\n",
            "\n",
            "         [[-3.3332e-02, -3.2185e-03, -2.9989e-02],\n",
            "          [-3.3537e-02, -4.3245e-03, -1.2731e-02],\n",
            "          [ 2.5326e-02, -3.1166e-02,  4.1685e-03]],\n",
            "\n",
            "         [[-2.8087e-03,  3.4673e-03,  1.2156e-02],\n",
            "          [-3.1638e-02,  3.0487e-02, -4.0479e-02],\n",
            "          [ 2.4396e-02,  4.4263e-03,  2.1431e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5420e-02,  2.4955e-03, -1.7000e-02],\n",
            "          [-4.7972e-03,  1.4438e-02,  2.5218e-02],\n",
            "          [-2.1017e-03, -1.5336e-02,  2.8691e-02]],\n",
            "\n",
            "         [[ 2.1938e-03, -2.9631e-02, -2.7150e-02],\n",
            "          [-4.5451e-03,  4.0469e-02, -4.0132e-02],\n",
            "          [ 2.5616e-02,  1.4521e-02, -1.9288e-02]],\n",
            "\n",
            "         [[ 2.3490e-02,  1.9547e-02,  2.3745e-02],\n",
            "          [ 3.4546e-02,  1.3716e-02, -3.1835e-02],\n",
            "          [ 9.3108e-03, -1.7844e-03, -2.0868e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.1348e-02, -8.2940e-03,  1.2551e-02],\n",
            "          [-1.1548e-02, -1.3209e-02,  7.8863e-03],\n",
            "          [-2.2122e-02,  1.5888e-02, -2.8161e-02]],\n",
            "\n",
            "         [[ 8.7078e-03,  4.1413e-02,  4.1620e-02],\n",
            "          [ 2.1960e-02, -3.1427e-02, -2.4277e-02],\n",
            "          [-1.2711e-02, -2.0393e-02, -2.6604e-02]],\n",
            "\n",
            "         [[-3.5560e-02,  1.9321e-02,  5.9222e-05],\n",
            "          [ 1.7553e-02,  1.5180e-02, -2.6456e-02],\n",
            "          [-2.6561e-02, -3.6639e-02, -2.8513e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5312e-02,  4.1137e-02, -8.1388e-03],\n",
            "          [ 3.9706e-02, -1.7225e-02, -1.3597e-02],\n",
            "          [-3.2694e-02, -3.3363e-02,  8.9985e-03]],\n",
            "\n",
            "         [[-3.3523e-03, -1.5545e-02,  1.5528e-02],\n",
            "          [-2.8743e-03,  7.6359e-03,  8.8948e-03],\n",
            "          [ 3.5370e-02,  4.0367e-02, -1.3727e-02]],\n",
            "\n",
            "         [[-2.1994e-05,  3.2918e-02,  1.5613e-02],\n",
            "          [-2.0728e-02, -2.9003e-03, -1.9007e-02],\n",
            "          [ 2.2091e-02, -5.0711e-03, -1.6689e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.2444e-03,  4.1686e-03, -1.2170e-02],\n",
            "          [ 1.4510e-02,  3.3524e-02,  1.3631e-02],\n",
            "          [ 3.4116e-02, -9.5753e-03,  1.2928e-02]],\n",
            "\n",
            "         [[ 3.2399e-02,  3.6272e-02,  1.0689e-03],\n",
            "          [-2.6058e-04,  3.8064e-02,  1.9981e-02],\n",
            "          [ 2.8393e-02, -1.6595e-02, -3.4317e-02]],\n",
            "\n",
            "         [[-3.4431e-02, -1.4327e-02,  3.7205e-02],\n",
            "          [-1.3246e-02, -2.9640e-02,  2.1700e-02],\n",
            "          [ 9.3539e-03,  3.3620e-02,  3.3456e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2927e-02, -3.2587e-02, -3.3255e-03],\n",
            "          [-3.3887e-02, -3.0438e-02,  3.3217e-02],\n",
            "          [-3.4679e-02,  1.1953e-02,  3.7119e-02]],\n",
            "\n",
            "         [[ 3.3411e-03, -6.6051e-03, -2.8487e-02],\n",
            "          [ 1.2969e-02,  3.3230e-02,  1.5512e-03],\n",
            "          [-7.0870e-03, -3.4672e-02,  3.2724e-03]],\n",
            "\n",
            "         [[-4.0515e-02,  1.6145e-03,  3.2206e-02],\n",
            "          [-3.6136e-02, -1.6701e-02, -7.9377e-03],\n",
            "          [ 1.8577e-02,  8.0933e-03, -1.0367e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "up_list.2.seq.1.conv.bias Parameter containing:\n",
            "tensor([-0.0275,  0.0046, -0.0226,  0.0250,  0.0413, -0.0226,  0.0139, -0.0052,\n",
            "         0.0162,  0.0208,  0.0011,  0.0230,  0.0174,  0.0147, -0.0338, -0.0069],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "out.0.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "out.0.bias Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "out.2.weight Parameter containing:\n",
            "tensor([[[[ 0.0084,  0.0126,  0.0360],\n",
            "          [-0.0214,  0.0678,  0.0774],\n",
            "          [-0.0323,  0.0479, -0.0192]],\n",
            "\n",
            "         [[-0.0311,  0.0558, -0.0644],\n",
            "          [-0.0194, -0.0309,  0.0624],\n",
            "          [-0.0010, -0.0408, -0.0235]],\n",
            "\n",
            "         [[-0.0215, -0.0319,  0.0679],\n",
            "          [ 0.0145, -0.0399, -0.0833],\n",
            "          [-0.0670,  0.0209,  0.0630]],\n",
            "\n",
            "         [[ 0.0546, -0.0206, -0.0732],\n",
            "          [ 0.0082,  0.0184,  0.0462],\n",
            "          [-0.0473, -0.0085,  0.0398]],\n",
            "\n",
            "         [[-0.0325, -0.0157, -0.0555],\n",
            "          [ 0.0121,  0.0211,  0.0496],\n",
            "          [-0.0296, -0.0320,  0.0538]],\n",
            "\n",
            "         [[-0.0287,  0.0517,  0.0598],\n",
            "          [ 0.0288, -0.0007,  0.0154],\n",
            "          [ 0.0128, -0.0038,  0.0828]],\n",
            "\n",
            "         [[ 0.0766, -0.0572, -0.0593],\n",
            "          [ 0.0480,  0.0502,  0.0721],\n",
            "          [-0.0365,  0.0337,  0.0141]],\n",
            "\n",
            "         [[-0.0453, -0.0513,  0.0013],\n",
            "          [-0.0654, -0.0163, -0.0699],\n",
            "          [ 0.0785, -0.0358,  0.0575]],\n",
            "\n",
            "         [[-0.0398,  0.0095, -0.0374],\n",
            "          [-0.0715,  0.0520,  0.0179],\n",
            "          [-0.0308, -0.0165,  0.0375]],\n",
            "\n",
            "         [[ 0.0202, -0.0160,  0.0530],\n",
            "          [-0.0013,  0.0634, -0.0293],\n",
            "          [-0.0445, -0.0277,  0.0470]],\n",
            "\n",
            "         [[ 0.0267,  0.0196, -0.0807],\n",
            "          [-0.0224,  0.0528, -0.0348],\n",
            "          [-0.0644, -0.0051,  0.0758]],\n",
            "\n",
            "         [[ 0.0337, -0.0040,  0.0412],\n",
            "          [-0.0150,  0.0126,  0.0710],\n",
            "          [-0.0740, -0.0787, -0.0513]],\n",
            "\n",
            "         [[-0.0290,  0.0210, -0.0545],\n",
            "          [ 0.0085, -0.0774, -0.0008],\n",
            "          [-0.0614, -0.0492, -0.0442]],\n",
            "\n",
            "         [[ 0.0030, -0.0174,  0.0415],\n",
            "          [-0.0708,  0.0824, -0.0375],\n",
            "          [-0.0031,  0.0522,  0.0308]],\n",
            "\n",
            "         [[ 0.0178, -0.0356, -0.0705],\n",
            "          [-0.0111, -0.0287, -0.0020],\n",
            "          [-0.0820,  0.0327,  0.0472]],\n",
            "\n",
            "         [[ 0.0470,  0.0821,  0.0749],\n",
            "          [ 0.0590, -0.0764, -0.0368],\n",
            "          [ 0.0310,  0.0407, -0.0330]]]], device='cuda:0', requires_grad=True)\n",
            "out.2.bias Parameter containing:\n",
            "tensor([0.0459], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for name, param in model.named_parameters(): print(name, param)\n",
        "\n",
        "# [[ 0.0178, -0.0356, -0.0705],\n",
        "#           [-0.0111, -0.0287, -0.0020],\n",
        "#           [-0.0820,  0.0327,  0.0472]],\n",
        "\n",
        "#          [[ 0.0470,  0.0821,  0.0749],\n",
        "#           [ 0.0590, -0.0764, -0.0368],\n",
        "#           [ 0.0310,  0.0407, -0.0330]]]]\n",
        "\n",
        "         [[ 0.0178, -0.0356, -0.0705],\n",
        "          [-0.0111, -0.0287, -0.0020],\n",
        "          [-0.0820,  0.0327,  0.0472]],\n",
        "\n",
        "         [[ 0.0470,  0.0821,  0.0749],\n",
        "          [ 0.0590, -0.0764, -0.0368],\n",
        "          [ 0.0310,  0.0407, -0.0330]]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGN0e0Mxe5UI",
        "outputId": "c5f2fb99-ced2-45e3-ee6c-70e93ce15700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-23f3dbda76f1>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load(folder+'lfm.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "modelsd, optimsd = torch.load(folder+'lfm.pkl', map_location=device).values()\n",
        "model.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrFxTtw4eFSq"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'lfm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CiEv-YOg8PFq"
      },
      "outputs": [],
      "source": [
        "# @title Sampling lebellig\n",
        "# https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "n_samples = 10_000\n",
        "with torch.no_grad():\n",
        "    x_0 = torch.randn(n_samples, 2, device=device)\n",
        "    x_1_hat = v_t.decode(x_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x_1_hat = x_1_hat.cpu().numpy()\n",
        "plt.hist2d(x_1_hat[:, 0], x_1_hat[:, 1], bins=164)\n",
        "plt.show()\n",
        "\n",
        "# https://github.com/probabilists/zuko/blob/master/zuko/utils.py\n",
        "def unpack(x: Tensor, shapes: Sequence[Size]) -> Sequence[Tensor]:\n",
        "    r\"\"\"Unpacks a packed tensor.\n",
        "\n",
        "    Arguments:\n",
        "        x: A packed tensor, with shape :math:`(*, D)`.\n",
        "        shapes: A sequence of shapes :math:`S_i`, corresponding to the total number of\n",
        "            elements :math:`D`.\n",
        "\n",
        "    Returns:\n",
        "        The unpacked tensors, with shapes :math:`(*, S_i)`.\n",
        "\n",
        "    Example:\n",
        "        >>> x = torch.randn(26)\n",
        "        >>> y, z = unpack(x, ((1, 2, 3), (4, 5)))\n",
        "        >>> y.shape\n",
        "        torch.Size([1, 2, 3])\n",
        "        >>> z.shape\n",
        "        torch.Size([4, 5])\n",
        "    \"\"\"\n",
        "\n",
        "    sizes = [math.prod(s) for s in shapes]\n",
        "\n",
        "    x = x.split(sizes, -1)\n",
        "    x = (y.unflatten(-1, (*s, 1)) for y, s in zip(x, shapes))\n",
        "    x = (y.squeeze(-1) for y in x)\n",
        "\n",
        "    return tuple(x)\n",
        "\n",
        "def odeint(\n",
        "    f: Callable[[Tensor, Tensor], Tensor],\n",
        "    x: Union[Tensor, Sequence[Tensor]],\n",
        "    t0: Union[float, Tensor],\n",
        "    t1: Union[float, Tensor],\n",
        "    phi: Iterable[Tensor] = (),\n",
        "    atol: float = 1e-6,\n",
        "    rtol: float = 1e-5,\n",
        ") -> Union[Tensor, Sequence[Tensor]]:\n",
        "    r\"\"\"Integrates a system of first-order ordinary differential equations (ODEs)\n",
        "    .. math:: \\frac{dx}{dt} = f_\\phi(t, x) ,\n",
        "    from :math:`t_0` to :math:`t_1` using the adaptive Dormand-Prince method. The\n",
        "    output is the final state\n",
        "    .. math:: x(t_1) = x_0 + \\int_{t_0}^{t_1} f_\\phi(t, x(t)) ~ dt .\n",
        "    Gradients are propagated through :math:`x_0`, :math:`t_0`, :math:`t_1` and\n",
        "    :math:`\\phi` via the adaptive checkpoint adjoint (ACA) method.\n",
        "    References:\n",
        "        | Neural Ordinary Differential Equations (Chen el al., 2018) | https://arxiv.org/abs/1806.07366\n",
        "        | Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE (Zhuang et al., 2020) | https://arxiv.org/abs/2006.02493\n",
        "\n",
        "    Arguments:\n",
        "        f: A system of first-order ODEs :math:`f_\\phi`.\n",
        "        x: The initial state :math:`x_0`.\n",
        "        t0: The initial integration time :math:`t_0`.\n",
        "        t1: The final integration time :math:`t_1`.\n",
        "        phi: The parameters :math:`\\phi` of :math:`f_\\phi`.\n",
        "        atol: The absolute tolerance.\n",
        "        rtol: The relative tolerance.\n",
        "\n",
        "    Returns: The final state :math:`x(t_1)`.\n",
        "\n",
        "    Example:\n",
        "        >>> A = torch.randn(3, 3)\n",
        "        >>> f = lambda t, x: x @ A\n",
        "        >>> x0 = torch.randn(3)\n",
        "        >>> x1 = odeint(f, x0, 0.0, 1.0)\n",
        "        >>> x1\n",
        "        tensor([-1.4596,  0.5008,  1.5828])\n",
        "    \"\"\"\n",
        "\n",
        "    settings = (atol, rtol, torch.is_grad_enabled())\n",
        "    if torch.is_tensor(x):\n",
        "        x0 = x\n",
        "        g = f\n",
        "    else: shapes = [y.shape for y in x]\n",
        "\n",
        "        def pack(x: Iterable[Tensor]) -> Tensor:\n",
        "            return torch.cat([y.flatten() for y in x])\n",
        "\n",
        "        x0 = pack(x)\n",
        "        g = lambda t, x: pack(f(t, *unpack(x, shapes)))\n",
        "\n",
        "    t0 = torch.as_tensor(t0, dtype=x0.dtype, device=x0.device)\n",
        "    t1 = torch.as_tensor(t1, dtype=x0.dtype, device=x0.device)\n",
        "\n",
        "    assert not t0.shape and not t1.shape, \"'t0' and 't1' must be scalars\"\n",
        "\n",
        "    x1 = AdaptiveCheckpointAdjoint.apply(settings, g, x0, t0, t1, *phi)\n",
        "\n",
        "    if torch.is_tensor(x):\n",
        "        return x1\n",
        "    else:\n",
        "        return unpack(x1, shapes)\n",
        "\n",
        "\n",
        "\n",
        "def wrapper(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "    t = t * torch.ones(len(x), device=x.device)\n",
        "    return self(t, x)\n",
        "\n",
        "from zuko.utils import odeint\n",
        "def decode_t0_t1(self, x_0, t0, t1):\n",
        "    return odeint(self.wrapper, x_0, t0, t1, self.parameters())\n",
        "\n",
        "\n",
        "N_SAMPLES = 10_000\n",
        "N_STEPS = 100\n",
        "t_steps = torch.linspace(0, 1, N_STEPS, device=device)\n",
        "with torch.no_grad():\n",
        "    x_t = [torch.randn(n_samples, 2, device=device)]\n",
        "    for t in range(len(t_steps)-1):\n",
        "      x_t += [v_t.decode_t0_t1(x_t[-1], t_steps[t], t_steps[t+1])]\n",
        "\n",
        "# pad predictions\n",
        "x_t = [x_t[0]]*10 + x_t + [x_t[-1]] * 10\n",
        "\n",
        "x_t_numpy = np.array([x.detach().cpu().numpy() for x in x_t])\n",
        "filename = f\"{DATASET}_{MODEL}_{N_SAMPLES}_{N_STEPS}.npy\"\n",
        "np.save(filename, x_t_numpy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPcm3g4mpD2h"
      },
      "source": [
        "## save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xGntuhWs6lL_"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen\n",
        "# https://github.com/lucidrains/imagen-pytorch/blob/main/imagen_pytorch/imagen_pytorch.py\n",
        "# https://arxiv.org/pdf/2205.11487\n",
        "\n",
        "import math\n",
        "from random import random\n",
        "from beartype.typing import List, Union, Optional\n",
        "from beartype import beartype\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial, wraps\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch import nn, einsum\n",
        "from torch.amp import autocast\n",
        "from torch.special import expm1\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import kornia.augmentation as K\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n",
        "\n",
        "from imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def identity(t, *args, **kwargs):\n",
        "    return t\n",
        "\n",
        "def divisible_by(numer, denom):\n",
        "    return (numer % denom) == 0\n",
        "\n",
        "def first(arr, d = None):\n",
        "    if len(arr) == 0:\n",
        "        return d\n",
        "    return arr[0]\n",
        "\n",
        "def maybe(fn):\n",
        "    @wraps(fn)\n",
        "    def inner(x):\n",
        "        if not exists(x):\n",
        "            return x\n",
        "        return fn(x)\n",
        "    return inner\n",
        "\n",
        "def once(fn):\n",
        "    called = False\n",
        "    @wraps(fn)\n",
        "    def inner(x):\n",
        "        nonlocal called\n",
        "        if called:\n",
        "            return\n",
        "        called = True\n",
        "        return fn(x)\n",
        "    return inner\n",
        "\n",
        "print_once = once(print)\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "def cast_tuple(val, length = None):\n",
        "    if isinstance(val, list):\n",
        "        val = tuple(val)\n",
        "\n",
        "    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n",
        "\n",
        "    if exists(length):\n",
        "        assert len(output) == length\n",
        "\n",
        "    return output\n",
        "\n",
        "def compact(input_dict):\n",
        "    return {key: value for key, value in input_dict.items() if exists(value)}\n",
        "\n",
        "def maybe_transform_dict_key(input_dict, key, fn):\n",
        "    if key not in input_dict:\n",
        "        return input_dict\n",
        "\n",
        "    copied_dict = input_dict.copy()\n",
        "    copied_dict[key] = fn(copied_dict[key])\n",
        "    return copied_dict\n",
        "\n",
        "def cast_uint8_images_to_float(images):\n",
        "    if not images.dtype == torch.uint8:\n",
        "        return images\n",
        "    return images / 255\n",
        "\n",
        "def module_device(module):\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "def zero_init_(m):\n",
        "    nn.init.zeros_(m.weight)\n",
        "    if exists(m.bias):\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "def eval_decorator(fn):\n",
        "    def inner(model, *args, **kwargs):\n",
        "        was_training = model.training\n",
        "        model.eval()\n",
        "        out = fn(model, *args, **kwargs)\n",
        "        model.train(was_training)\n",
        "        return out\n",
        "    return inner\n",
        "\n",
        "def pad_tuple_to_length(t, length, fillvalue = None):\n",
        "    remain_length = length - len(t)\n",
        "    if remain_length <= 0:\n",
        "        return t\n",
        "    return (*t, *((fillvalue,) * remain_length))\n",
        "\n",
        "# helper classes\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "# tensor helpers\n",
        "\n",
        "def log(t, eps: float = 1e-12):\n",
        "    return torch.log(t.clamp(min = eps))\n",
        "\n",
        "def l2norm(t):\n",
        "    return F.normalize(t, dim = -1)\n",
        "\n",
        "def right_pad_dims_to(x, t):\n",
        "    padding_dims = x.ndim - t.ndim\n",
        "    if padding_dims <= 0:\n",
        "        return t\n",
        "    return t.view(*t.shape, *((1,) * padding_dims))\n",
        "\n",
        "def masked_mean(t, *, dim, mask = None):\n",
        "    if not exists(mask):\n",
        "        return t.mean(dim = dim)\n",
        "\n",
        "    denom = mask.sum(dim = dim, keepdim = True)\n",
        "    mask = rearrange(mask, 'b n -> b n 1')\n",
        "    masked_t = t.masked_fill(~mask, 0.)\n",
        "\n",
        "    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n",
        "\n",
        "def resize_image_to(\n",
        "    image,\n",
        "    target_image_size,\n",
        "    clamp_range = None,\n",
        "    mode = 'nearest'\n",
        "):\n",
        "    orig_image_size = image.shape[-1]\n",
        "\n",
        "    if orig_image_size == target_image_size:\n",
        "        return image\n",
        "\n",
        "    out = F.interpolate(image, target_image_size, mode = mode)\n",
        "\n",
        "    if exists(clamp_range):\n",
        "        out = out.clamp(*clamp_range)\n",
        "\n",
        "    return out\n",
        "\n",
        "def calc_all_frame_dims(\n",
        "    downsample_factors: List[int],\n",
        "    frames\n",
        "):\n",
        "    if not exists(frames):\n",
        "        return (tuple(),) * len(downsample_factors)\n",
        "\n",
        "    all_frame_dims = []\n",
        "\n",
        "    for divisor in downsample_factors:\n",
        "        assert divisible_by(frames, divisor)\n",
        "        all_frame_dims.append((frames // divisor,))\n",
        "\n",
        "    return all_frame_dims\n",
        "\n",
        "def safe_get_tuple_index(tup, index, default = None):\n",
        "    if len(tup) <= index:\n",
        "        return default\n",
        "    return tup[index]\n",
        "\n",
        "def pack_one_with_inverse(x, pattern):\n",
        "    packed, packed_shape = pack([x], pattern)\n",
        "\n",
        "    def inverse(x, inverse_pattern = None):\n",
        "        inverse_pattern = default(inverse_pattern, pattern)\n",
        "        return unpack(x, packed_shape, inverse_pattern)[0]\n",
        "\n",
        "    return packed, inverse\n",
        "\n",
        "# image normalization functions\n",
        "# ddpms expect images to be in the range of -1 to 1\n",
        "\n",
        "def normalize_neg_one_to_one(img):\n",
        "    return img * 2 - 1\n",
        "\n",
        "def unnormalize_zero_to_one(normed_img):\n",
        "    return (normed_img + 1) * 0.5\n",
        "\n",
        "# classifier free guidance functions\n",
        "\n",
        "def prob_mask_like(shape, prob, device):\n",
        "    if prob == 1:\n",
        "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
        "    elif prob == 0:\n",
        "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
        "    else:\n",
        "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n",
        "\n",
        "# for improved cfg, getting parallel and orthogonal components of cfg update\n",
        "\n",
        "def project(x, y):\n",
        "    x, inverse = pack_one_with_inverse(x, 'b *')\n",
        "    y, _ = pack_one_with_inverse(y, 'b *')\n",
        "\n",
        "    dtype = x.dtype\n",
        "    x, y = x.double(), y.double()\n",
        "    unit = F.normalize(y, dim = -1)\n",
        "\n",
        "    parallel = (x * unit).sum(dim = -1, keepdim = True) * unit\n",
        "    orthogonal = x - parallel\n",
        "\n",
        "    return inverse(parallel).to(dtype), inverse(orthogonal).to(dtype)\n",
        "\n",
        "# gaussian diffusion with continuous time helper functions and classes\n",
        "# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n",
        "\n",
        "@torch.jit.script\n",
        "def beta_linear_log_snr(t):\n",
        "    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n",
        "\n",
        "@torch.jit.script\n",
        "def alpha_cosine_log_snr(t, s: float = 0.008):\n",
        "    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n",
        "\n",
        "def log_snr_to_alpha_sigma(log_snr):\n",
        "    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n",
        "\n",
        "class GaussianDiffusionContinuousTimes(nn.Module):\n",
        "    def __init__(self, *, noise_schedule, timesteps = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        if noise_schedule == \"linear\":\n",
        "            self.log_snr = beta_linear_log_snr\n",
        "        elif noise_schedule == \"cosine\":\n",
        "            self.log_snr = alpha_cosine_log_snr\n",
        "        else:\n",
        "            raise ValueError(f'invalid noise schedule {noise_schedule}')\n",
        "\n",
        "        self.num_timesteps = timesteps\n",
        "\n",
        "    def get_times(self, batch_size, noise_level, *, device):\n",
        "        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n",
        "\n",
        "    def sample_random_times(self, batch_size, *, device):\n",
        "        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n",
        "\n",
        "    def get_condition(self, times):\n",
        "        return maybe(self.log_snr)(times)\n",
        "\n",
        "    def get_sampling_timesteps(self, batch, *, device):\n",
        "        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n",
        "        times = repeat(times, 't -> b t', b = batch)\n",
        "        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
        "        times = times.unbind(dim = -1)\n",
        "        return times\n",
        "\n",
        "    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n",
        "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n",
        "\n",
        "        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n",
        "        log_snr = self.log_snr(t)\n",
        "        log_snr_next = self.log_snr(t_next)\n",
        "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n",
        "\n",
        "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
        "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n",
        "\n",
        "        # c - as defined near eq 33\n",
        "        c = -expm1(log_snr - log_snr_next)\n",
        "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
        "\n",
        "        # following (eq. 33)\n",
        "        posterior_variance = (sigma_next ** 2) * c\n",
        "        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    def q_sample(self, x_start, t, noise = None):\n",
        "        dtype = x_start.dtype\n",
        "\n",
        "        if isinstance(t, float):\n",
        "            batch = x_start.shape[0]\n",
        "            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        log_snr = self.log_snr(t).type(dtype)\n",
        "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n",
        "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
        "\n",
        "        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n",
        "\n",
        "    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n",
        "        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n",
        "        batch = shape[0]\n",
        "\n",
        "        if isinstance(from_t, float):\n",
        "            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n",
        "\n",
        "        if isinstance(to_t, float):\n",
        "            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_from))\n",
        "\n",
        "        log_snr = self.log_snr(from_t)\n",
        "        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n",
        "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
        "\n",
        "        log_snr_to = self.log_snr(to_t)\n",
        "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n",
        "        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n",
        "\n",
        "        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n",
        "\n",
        "    def predict_start_from_v(self, x_t, t, v):\n",
        "        log_snr = self.log_snr(t)\n",
        "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
        "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
        "        return alpha * x_t - sigma * v\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        log_snr = self.log_snr(t)\n",
        "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
        "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
        "        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n",
        "\n",
        "# norms and residuals\n",
        "\n",
        "class ChanRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.gamma = nn.Parameter(torch.ones(dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = 1) * self.scale * self.gamma\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feats, stable = False, dim = -1):\n",
        "        super().__init__()\n",
        "        self.stable = stable\n",
        "        self.dim = dim\n",
        "\n",
        "        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        dtype, dim = x.dtype, self.dim\n",
        "\n",
        "        if self.stable:\n",
        "            x = x / x.amax(dim = dim, keepdim = True).detach()\n",
        "\n",
        "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
        "        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n",
        "        mean = torch.mean(x, dim = dim, keepdim = True)\n",
        "\n",
        "        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n",
        "\n",
        "ChanLayerNorm = partial(LayerNorm, dim = -3)\n",
        "\n",
        "class Always():\n",
        "    def __init__(self, val):\n",
        "        self.val = val\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.val\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class Parallel(nn.Module):\n",
        "    def __init__(self, *fns):\n",
        "        super().__init__()\n",
        "        self.fns = nn.ModuleList(fns)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [fn(x) for fn in self.fns]\n",
        "        return sum(outputs)\n",
        "\n",
        "# attention pooling\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        scale = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim, bias = False),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, latents, mask = None):\n",
        "        x = self.norm(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        b, h = x.shape[0], self.heads\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "\n",
        "        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n",
        "        kv_input = torch.cat((x, latents), dim = -2)\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "\n",
        "        # qk rmsnorm\n",
        "\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "\n",
        "        # similarities and masking\n",
        "\n",
        "        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, max_neg_value)\n",
        "\n",
        "        # attention\n",
        "\n",
        "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
        "        attn = attn.to(sim.dtype)\n",
        "\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        num_latents = 64,\n",
        "        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n",
        "        max_seq_len = 512,\n",
        "        ff_mult = 4\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.to_latents_from_mean_pooled_seq = None\n",
        "        if num_latents_mean_pooled > 0:\n",
        "            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n",
        "                LayerNorm(dim),\n",
        "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
        "                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        n, device = x.shape[1], x.device\n",
        "        pos_emb = self.pos_emb(torch.arange(n, device = device))\n",
        "\n",
        "        x_with_pos = x + pos_emb\n",
        "\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n",
        "\n",
        "        if exists(self.to_latents_from_mean_pooled_seq):\n",
        "            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n",
        "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
        "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x_with_pos, latents, mask = mask) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "# attention\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, *, dim_head = 64, heads = 8, context_dim = None, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim, bias = False), LayerNorm(dim),)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None, attn_bias = None):\n",
        "        b, n, device = *x.shape[:2], x.device\n",
        "        x = self.norm(x)\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n",
        "        # add null key / value for classifier free guidance in prior net\n",
        "\n",
        "        nk, nv = map(lambda t: repeat(t, 'd -> b 1 d', b = b), self.null_kv.unbind(dim = -2))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "\n",
        "        # add text conditioning, if present\n",
        "        if exists(context):\n",
        "            assert exists(self.to_context)\n",
        "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
        "            k = torch.cat((ck, k), dim = -2)\n",
        "            v = torch.cat((cv, v), dim = -2)\n",
        "\n",
        "        # qk rmsnorm\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "        # calculate query / key similarities\n",
        "        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n",
        "        # relative positional encoding (T5 style)\n",
        "        if exists(attn_bias): sim = sim + attn_bias\n",
        "\n",
        "        # masking\n",
        "        max_neg_value = -torch.finfo(sim.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = F.pad(mask, (1, 0), value = True)\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, max_neg_value)\n",
        "\n",
        "        # attention\n",
        "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
        "        attn = attn.to(sim.dtype)\n",
        "\n",
        "        # aggregate values\n",
        "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "# decoder\n",
        "\n",
        "def Upsample(dim, dim_out = None):\n",
        "    dim_out = default(dim_out, dim)\n",
        "\n",
        "    return nn.Sequential(nn.Upsample(scale_factor = 2, mode = 'nearest'), nn.Conv2d(dim, dim_out, 3, padding = 1))\n",
        "\n",
        "class PixelShuffleUpsample(nn.Module):\n",
        "    \"\"\"\n",
        "    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n",
        "    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, dim_out = None):\n",
        "        super().__init__()\n",
        "        dim_out = default(dim_out, dim)\n",
        "        conv = nn.Conv2d(dim, dim_out * 4, 1)\n",
        "        self.net = nn.Sequential(conv, nn.SiLU(), nn.PixelShuffle(2))\n",
        "        self.init_conv_(conv)\n",
        "\n",
        "    def init_conv_(self, conv):\n",
        "        o, i, h, w = conv.weight.shape\n",
        "        conv_weight = torch.empty(o // 4, i, h, w)\n",
        "        nn.init.kaiming_uniform_(conv_weight)\n",
        "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
        "\n",
        "        conv.weight.data.copy_(conv_weight)\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def Downsample(dim, dim_out = None):\n",
        "    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n",
        "    # named SP-conv in the paper, but basically a pixel unshuffle\n",
        "    dim_out = default(dim_out, dim)\n",
        "    return nn.Sequential(\n",
        "        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n",
        "        nn.Conv2d(dim * 4, dim_out, 1)\n",
        "    )\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n",
        "        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n",
        "        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n",
        "\n",
        "class LearnedSinusoidalPosEmb(nn.Module):\n",
        "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        assert (dim % 2) == 0\n",
        "        half_dim = dim // 2\n",
        "        self.weights = nn.Parameter(torch.randn(half_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, 'b -> b 1')\n",
        "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
        "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
        "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
        "        return fouriered\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_out,\n",
        "        norm = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm = ChanRMSNorm(dim) if norm else Identity()\n",
        "        self.activation = nn.SiLU()\n",
        "        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.activation(x)\n",
        "        return self.project(x)\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_out,\n",
        "        *,\n",
        "        cond_dim = None,\n",
        "        time_cond_dim = None,\n",
        "        linear_attn = False,\n",
        "        use_gca = False,\n",
        "        squeeze_excite = False,\n",
        "        **attn_kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.time_mlp = None\n",
        "        if exists(time_cond_dim):\n",
        "            self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_cond_dim, dim_out * 2),)\n",
        "        self.cross_attn = None\n",
        "        if exists(cond_dim):\n",
        "            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n",
        "            self.cross_attn = attn_klass(dim = dim_out, context_dim = cond_dim, **attn_kwargs)\n",
        "        self.block1 = Block(dim, dim_out)\n",
        "        self.block2 = Block(dim_out, dim_out)\n",
        "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None, cond = None):\n",
        "\n",
        "        scale_shift = None\n",
        "        if exists(self.time_mlp) and exists(time_emb):\n",
        "            time_emb = self.time_mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
        "            scale_shift = time_emb.chunk(2, dim = 1)\n",
        "\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.cross_attn):\n",
        "            assert exists(cond)\n",
        "            h = rearrange(h, 'b c h w -> b h w c')\n",
        "            h, ps = pack([h], 'b * c')\n",
        "            h = self.cross_attn(h, context = cond) + h\n",
        "            h, = unpack(h, ps, 'b * c')\n",
        "            h = rearrange(h, 'b h w c -> b c h w')\n",
        "\n",
        "        h = self.block2(h, scale_shift = scale_shift)\n",
        "\n",
        "        h = h * self.gca(h)\n",
        "\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        context_dim = None,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        norm_context = False,\n",
        "        scale = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        context_dim = default(context_dim, dim)\n",
        "\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n",
        "\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim, bias = False), LayerNorm(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context, mask = None):\n",
        "        b, n, device = *x.shape[:2], x.device\n",
        "        x = self.norm(x)\n",
        "        context = self.norm_context(context)\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n",
        "        # add null key / value for classifier free guidance in prior net\n",
        "        nk, nv = map(lambda t: repeat(t, 'd -> b h 1 d', h = self.heads,  b = b), self.null_kv.unbind(dim = -2))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "\n",
        "        # cosine sim attention\n",
        "\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "\n",
        "        # similarities\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        # masking\n",
        "\n",
        "        max_neg_value = -torch.finfo(sim.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = F.pad(mask, (1, 0), value = True)\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, max_neg_value)\n",
        "\n",
        "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
        "        attn = attn.to(sim.dtype)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearCrossAttention(CrossAttention):\n",
        "    def forward(self, x, context, mask = None):\n",
        "        b, n, device = *x.shape[:2], x.device\n",
        "        x = self.norm(x)\n",
        "        context = self.norm_context(context)\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = self.heads), (q, k, v))\n",
        "\n",
        "        # add null key / value for classifier free guidance in prior net\n",
        "\n",
        "        nk, nv = map(lambda t: repeat(t, 'd -> (b h) 1 d', h = self.heads,  b = b), self.null_kv.unbind(dim = -2))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "\n",
        "        # masking\n",
        "\n",
        "        max_neg_value = -torch.finfo(x.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = F.pad(mask, (1, 0), value = True)\n",
        "            mask = rearrange(mask, 'b n -> b n 1')\n",
        "            k = k.masked_fill(~mask, max_neg_value)\n",
        "            v = v.masked_fill(~mask, 0.)\n",
        "\n",
        "        # linear attention\n",
        "\n",
        "        q = q.softmax(dim = -1)\n",
        "        k = k.softmax(dim = -2)\n",
        "        q = q * self.scale\n",
        "        context = einsum('b n d, b n e -> b d e', k, v)\n",
        "        out = einsum('b n d, b d e -> b n e', q, context)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 32,\n",
        "        heads = 8,\n",
        "        dropout = 0.05,\n",
        "        context_dim = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "        self.norm = ChanLayerNorm(dim)\n",
        "\n",
        "        self.nonlin = nn.SiLU()\n",
        "\n",
        "        self.to_q = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
        "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
        "        )\n",
        "\n",
        "        self.to_k = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
        "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
        "        )\n",
        "\n",
        "        self.to_v = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
        "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
        "        )\n",
        "\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Conv2d(inner_dim, dim, 1, bias = False),\n",
        "            ChanLayerNorm(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, fmap, context = None):\n",
        "        h, x, y = self.heads, *fmap.shape[-2:]\n",
        "\n",
        "        fmap = self.norm(fmap)\n",
        "        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (q, k, v))\n",
        "\n",
        "        if exists(context):\n",
        "            assert exists(self.to_context)\n",
        "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
        "            ck, cv = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (ck, cv))\n",
        "            k = torch.cat((k, ck), dim = -2)\n",
        "            v = torch.cat((v, cv), dim = -2)\n",
        "\n",
        "        q = q.softmax(dim = -1)\n",
        "        k = k.softmax(dim = -2)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = einsum('b n d, b n e -> b d e', k, v)\n",
        "        out = einsum('b n d, b d e -> b n e', q, context)\n",
        "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
        "\n",
        "        out = self.nonlin(out)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GlobalContext(nn.Module):\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim_in,\n",
        "        dim_out\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        context = self.to_k(x)\n",
        "        x, context = map(lambda t: rearrange(t, 'b n ... -> b n (...)'), (x, context))\n",
        "        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n",
        "        out = rearrange(out, '... -> ... 1')\n",
        "        return self.net(out)\n",
        "\n",
        "def FeedForward(dim, mult = 2):\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        LayerNorm(dim),\n",
        "        nn.Linear(dim, hidden_dim, bias = False),\n",
        "        nn.GELU(),\n",
        "        LayerNorm(hidden_dim),\n",
        "        nn.Linear(hidden_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "def ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        ChanLayerNorm(dim),\n",
        "        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n",
        "        nn.GELU(),\n",
        "        ChanLayerNorm(hidden_dim),\n",
        "        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n",
        "    )\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth = 1,\n",
        "        heads = 8,\n",
        "        dim_head = 32,\n",
        "        ff_mult = 2,\n",
        "        context_dim = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, context = None):\n",
        "        x = rearrange(x, 'b c h w -> b h w c')\n",
        "        x, ps = pack([x], 'b * c')\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, context = context) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        x, = unpack(x, ps, 'b * c')\n",
        "        x = rearrange(x, 'b h w c -> b c h w')\n",
        "        return x\n",
        "\n",
        "class LinearAttentionTransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth = 1,\n",
        "        heads = 8,\n",
        "        dim_head = 32,\n",
        "        ff_mult = 2,\n",
        "        context_dim = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n",
        "                ChanFeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, context = None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, context = context) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_in,\n",
        "        kernel_sizes,\n",
        "        dim_out = None,\n",
        "        stride = 2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
        "        dim_out = default(dim_out, dim_in)\n",
        "\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "        num_scales = len(kernel_sizes)\n",
        "\n",
        "        # calculate the dimension at each scale\n",
        "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n",
        "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n",
        "\n",
        "        self.convs = nn.ModuleList([])\n",
        "        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n",
        "            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n",
        "        return torch.cat(fmaps, dim = 1)\n",
        "\n",
        "class UpsampleCombiner(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        enabled = False,\n",
        "        dim_ins = tuple(),\n",
        "        dim_outs = tuple()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n",
        "        assert len(dim_ins) == len(dim_outs)\n",
        "\n",
        "        self.enabled = enabled\n",
        "\n",
        "        if not self.enabled:\n",
        "            self.dim_out = dim\n",
        "            return\n",
        "\n",
        "        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n",
        "        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n",
        "\n",
        "    def forward(self, x, fmaps = None):\n",
        "        target_size = x.shape[-1]\n",
        "\n",
        "        fmaps = default(fmaps, tuple())\n",
        "\n",
        "        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n",
        "            return x\n",
        "\n",
        "        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n",
        "        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n",
        "        return torch.cat((x, *outs), dim = 1)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n",
        "        num_resnet_blocks = 1,\n",
        "        cond_dim = None,\n",
        "        num_image_tokens = 4,\n",
        "        num_time_tokens = 2,\n",
        "        learned_sinu_pos_emb_dim = 16,\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        cond_images_channels = 0,\n",
        "        channels = 3,\n",
        "        channels_out = None,\n",
        "        attn_dim_head = 64,\n",
        "        attn_heads = 8,\n",
        "        ff_mult = 2.,\n",
        "        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "        layer_attns = True,\n",
        "        layer_attns_depth = 1,\n",
        "        layer_mid_attns_depth = 1,\n",
        "        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "        layer_cross_attns = True,\n",
        "        use_linear_attn = False,\n",
        "        use_linear_cross_attn = False,\n",
        "        cond_on_text = True,\n",
        "        max_text_len = 256,\n",
        "        init_dim = None,\n",
        "        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n",
        "        init_cross_embed = True,\n",
        "        init_cross_embed_kernel_sizes = (3, 7, 15),\n",
        "        cross_embed_downsample = False,\n",
        "        cross_embed_downsample_kernel_sizes = (2, 4),\n",
        "        attn_pool_text = True,\n",
        "        attn_pool_num_latents = 32,\n",
        "        dropout = 0.,\n",
        "        memory_efficient = False,\n",
        "        init_conv_to_final_conv_residual = False,\n",
        "        use_global_context_attn = True,\n",
        "        scale_skip_connection = True,\n",
        "        final_resnet_block = True,\n",
        "        final_conv_kernel_size = 3,\n",
        "        self_cond = False,\n",
        "        resize_mode = 'nearest',\n",
        "        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # guide researchers\n",
        "\n",
        "        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n",
        "\n",
        "        if dim < 128:\n",
        "            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n",
        "\n",
        "        # save locals to take care of some hyperparameters for cascading DDPM\n",
        "\n",
        "        self._locals = locals()\n",
        "        self._locals.pop('self', None)\n",
        "        self._locals.pop('__class__', None)\n",
        "\n",
        "        # determine dimensions\n",
        "\n",
        "        self.channels = channels\n",
        "        self.channels_out = default(channels_out, channels)\n",
        "\n",
        "        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n",
        "        # (2) in self conditioning, one appends the predict x0 (x_start)\n",
        "        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n",
        "        init_dim = default(init_dim, dim)\n",
        "\n",
        "        self.self_cond = self_cond\n",
        "\n",
        "        # optional image conditioning\n",
        "\n",
        "        self.has_cond_image = cond_images_channels > 0\n",
        "        self.cond_images_channels = cond_images_channels\n",
        "\n",
        "        init_channels += cond_images_channels\n",
        "\n",
        "        # initial convolution\n",
        "\n",
        "        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        # time conditioning\n",
        "\n",
        "        cond_dim = default(cond_dim, dim)\n",
        "        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n",
        "\n",
        "        # embedding time for log(snr) noise from continuous version\n",
        "\n",
        "        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n",
        "        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n",
        "\n",
        "        self.to_time_hiddens = nn.Sequential(\n",
        "            sinu_pos_emb,\n",
        "            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        self.to_time_cond = nn.Sequential(\n",
        "            nn.Linear(time_cond_dim, time_cond_dim)\n",
        "        )\n",
        "\n",
        "        # project to time tokens as well as time hiddens\n",
        "\n",
        "        self.to_time_tokens = nn.Sequential(\n",
        "            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
        "            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
        "        )\n",
        "\n",
        "        # low res aug noise conditioning\n",
        "\n",
        "        self.lowres_cond = lowres_cond\n",
        "\n",
        "        if lowres_cond:\n",
        "            self.to_lowres_time_hiddens = nn.Sequential(\n",
        "                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n",
        "                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n",
        "                nn.SiLU()\n",
        "            )\n",
        "\n",
        "            self.to_lowres_time_cond = nn.Sequential(\n",
        "                nn.Linear(time_cond_dim, time_cond_dim)\n",
        "            )\n",
        "\n",
        "            self.to_lowres_time_tokens = nn.Sequential(\n",
        "                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
        "                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
        "            )\n",
        "\n",
        "        # normalizations\n",
        "\n",
        "        self.norm_cond = nn.LayerNorm(cond_dim)\n",
        "\n",
        "        # text encoding conditioning (optional)\n",
        "\n",
        "        self.text_to_cond = None\n",
        "\n",
        "        if cond_on_text:\n",
        "            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n",
        "            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n",
        "\n",
        "        # finer control over whether to condition on text encodings\n",
        "\n",
        "        self.cond_on_text = cond_on_text\n",
        "\n",
        "        # attention pooling\n",
        "\n",
        "        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n",
        "\n",
        "        # for classifier free guidance\n",
        "\n",
        "        self.max_text_len = max_text_len\n",
        "\n",
        "        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n",
        "        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n",
        "\n",
        "        # for non-attention based text conditioning at all points in the network where time is also conditioned\n",
        "\n",
        "        self.to_text_non_attn_cond = None\n",
        "\n",
        "        if cond_on_text:\n",
        "            self.to_text_non_attn_cond = nn.Sequential(\n",
        "                nn.LayerNorm(cond_dim),\n",
        "                nn.Linear(cond_dim, time_cond_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(time_cond_dim, time_cond_dim)\n",
        "            )\n",
        "\n",
        "        # attention related params\n",
        "\n",
        "        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n",
        "\n",
        "        num_layers = len(in_out)\n",
        "\n",
        "        # resnet block klass\n",
        "\n",
        "        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n",
        "\n",
        "        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n",
        "\n",
        "        layer_attns = cast_tuple(layer_attns, num_layers)\n",
        "        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n",
        "        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n",
        "\n",
        "        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n",
        "        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n",
        "\n",
        "        assert all([layers == num_layers for layers in list(map(len, (layer_attns, layer_cross_attns)))])\n",
        "\n",
        "        # downsample klass\n",
        "\n",
        "        downsample_klass = Downsample\n",
        "\n",
        "        if cross_embed_downsample:\n",
        "            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n",
        "\n",
        "        # initial resnet block (for memory efficient unet)\n",
        "\n",
        "        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, use_gca = use_global_context_attn) if memory_efficient else None\n",
        "\n",
        "        # scale for resnet skip connections\n",
        "\n",
        "        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n",
        "\n",
        "        # layers\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        layer_params = [num_resnet_blocks, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n",
        "        reversed_layer_params = list(map(reversed, layer_params))\n",
        "\n",
        "        # downsampling layers\n",
        "\n",
        "        skip_connect_dims = [] # keep track of skip connection dimensions\n",
        "\n",
        "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
        "\n",
        "            if layer_attn:\n",
        "                transformer_block_klass = TransformerBlock\n",
        "            elif layer_use_linear_attn:\n",
        "                transformer_block_klass = LinearAttentionTransformerBlock\n",
        "            else:\n",
        "                transformer_block_klass = Identity\n",
        "\n",
        "            current_dim = dim_in\n",
        "\n",
        "            # whether to pre-downsample, from memory efficient unet\n",
        "\n",
        "            pre_downsample = None\n",
        "\n",
        "            if memory_efficient:\n",
        "                pre_downsample = downsample_klass(dim_in, dim_out)\n",
        "                current_dim = dim_out\n",
        "\n",
        "            skip_connect_dims.append(current_dim)\n",
        "\n",
        "            # whether to do post-downsample, for non-memory efficient unet\n",
        "\n",
        "            post_downsample = None\n",
        "            if not memory_efficient:\n",
        "                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n",
        "\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                pre_downsample,\n",
        "                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim),\n",
        "                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
        "                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n",
        "                post_downsample\n",
        "            ]))\n",
        "\n",
        "        # middle layers\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "\n",
        "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim)\n",
        "        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n",
        "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim)\n",
        "\n",
        "        # upsample klass\n",
        "\n",
        "        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n",
        "\n",
        "        # upsampling layers\n",
        "\n",
        "        upsample_fmap_dims = []\n",
        "\n",
        "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n",
        "            is_last = ind == (len(in_out) - 1)\n",
        "\n",
        "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
        "\n",
        "            if layer_attn:\n",
        "                transformer_block_klass = TransformerBlock\n",
        "            elif layer_use_linear_attn:\n",
        "                transformer_block_klass = LinearAttentionTransformerBlock\n",
        "            else:\n",
        "                transformer_block_klass = Identity\n",
        "\n",
        "            skip_connect_dim = skip_connect_dims.pop()\n",
        "\n",
        "            upsample_fmap_dims.append(dim_out)\n",
        "\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim),\n",
        "                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
        "                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n",
        "                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n",
        "            ]))\n",
        "\n",
        "        # whether to combine feature maps from all upsample blocks before final resnet block out\n",
        "\n",
        "        self.upsample_combiner = UpsampleCombiner(\n",
        "            dim = dim,\n",
        "            enabled = combine_upsample_fmaps,\n",
        "            dim_ins = upsample_fmap_dims,\n",
        "            dim_outs = dim\n",
        "        )\n",
        "\n",
        "        # whether to do a final residual from initial conv to the final resnet block out\n",
        "\n",
        "        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n",
        "        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n",
        "\n",
        "        # final optional resnet block and convolution out\n",
        "\n",
        "        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, use_gca = True) if final_resnet_block else None\n",
        "\n",
        "        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n",
        "        final_conv_dim_in += (channels if lowres_cond else 0)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n",
        "\n",
        "        zero_init_(self.final_conv)\n",
        "\n",
        "        # resize mode\n",
        "\n",
        "        self.resize_mode = resize_mode\n",
        "\n",
        "\n",
        "    # forward with classifier free guidance\n",
        "\n",
        "    def forward_with_cond_scale(\n",
        "        self,\n",
        "        *args,\n",
        "        cond_scale = 1.,\n",
        "        remove_parallel_component = True,\n",
        "        keep_parallel_frac = 0.,\n",
        "        **kwargs\n",
        "    ):\n",
        "        logits = self.forward(*args, **kwargs)\n",
        "\n",
        "        if cond_scale == 1:\n",
        "            return logits\n",
        "\n",
        "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n",
        "\n",
        "        update = (logits - null_logits)\n",
        "\n",
        "        if remove_parallel_component:\n",
        "            parallel, orthogonal = project(update, logits)\n",
        "            update = orthogonal + parallel * keep_parallel_frac\n",
        "\n",
        "        return logits + update * (cond_scale - 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        time,\n",
        "        *,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_noise_times = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        self_cond = None,\n",
        "        cond_drop_prob = 0.\n",
        "    ):\n",
        "        batch_size, device = x.shape[0], x.device\n",
        "\n",
        "        # condition on self\n",
        "\n",
        "        if self.self_cond:\n",
        "            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n",
        "            x = torch.cat((x, self_cond), dim = 1)\n",
        "\n",
        "        # add low resolution conditioning, if present\n",
        "\n",
        "        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n",
        "        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n",
        "\n",
        "        if exists(lowres_cond_img):\n",
        "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
        "\n",
        "        # condition on input image\n",
        "\n",
        "        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n",
        "\n",
        "        if exists(cond_images):\n",
        "            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n",
        "            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n",
        "            x = torch.cat((cond_images, x), dim = 1)\n",
        "\n",
        "        # initial convolution\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # init conv residual\n",
        "\n",
        "        if self.init_conv_to_final_conv_residual:\n",
        "            init_conv_residual = x.clone()\n",
        "\n",
        "        # time conditioning\n",
        "\n",
        "        time_hiddens = self.to_time_hiddens(time)\n",
        "\n",
        "        # derive time tokens\n",
        "\n",
        "        time_tokens = self.to_time_tokens(time_hiddens)\n",
        "        t = self.to_time_cond(time_hiddens)\n",
        "\n",
        "        # add lowres time conditioning to time hiddens\n",
        "        # and add lowres time tokens along sequence dimension for attention\n",
        "\n",
        "        if self.lowres_cond:\n",
        "            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n",
        "            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n",
        "            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n",
        "\n",
        "            t = t + lowres_t\n",
        "            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n",
        "\n",
        "        # text conditioning\n",
        "\n",
        "        text_tokens = None\n",
        "\n",
        "        if exists(text_embeds) and self.cond_on_text:\n",
        "\n",
        "            # conditional dropout\n",
        "\n",
        "            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n",
        "\n",
        "            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n",
        "            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n",
        "\n",
        "            # calculate text embeds\n",
        "\n",
        "            text_tokens = self.text_to_cond(text_embeds)\n",
        "\n",
        "            text_tokens = text_tokens[:, :self.max_text_len]\n",
        "\n",
        "            if exists(text_mask):\n",
        "                text_mask = text_mask[:, :self.max_text_len]\n",
        "\n",
        "            text_tokens_len = text_tokens.shape[1]\n",
        "            remainder = self.max_text_len - text_tokens_len\n",
        "\n",
        "            if remainder > 0:\n",
        "                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n",
        "\n",
        "            if exists(text_mask):\n",
        "                if remainder > 0:\n",
        "                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n",
        "\n",
        "                text_mask = rearrange(text_mask, 'b n -> b n 1')\n",
        "                text_keep_mask_embed = text_mask & text_keep_mask_embed\n",
        "\n",
        "            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n",
        "\n",
        "            text_tokens = torch.where(\n",
        "                text_keep_mask_embed,\n",
        "                text_tokens,\n",
        "                null_text_embed\n",
        "            )\n",
        "\n",
        "            if exists(self.attn_pool):\n",
        "                text_tokens = self.attn_pool(text_tokens)\n",
        "\n",
        "            # extra non-attention conditioning by projecting and then summing text embeddings to time\n",
        "            # termed as text hiddens\n",
        "\n",
        "            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n",
        "\n",
        "            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n",
        "\n",
        "            null_text_hidden = self.null_text_hidden.to(t.dtype)\n",
        "\n",
        "            text_hiddens = torch.where(\n",
        "                text_keep_mask_hidden,\n",
        "                text_hiddens,\n",
        "                null_text_hidden\n",
        "            )\n",
        "\n",
        "            t = t + text_hiddens\n",
        "\n",
        "        # main conditioning tokens (c)\n",
        "\n",
        "        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n",
        "\n",
        "        # normalize conditioning tokens\n",
        "\n",
        "        c = self.norm_cond(c)\n",
        "\n",
        "        # initial resnet block (for memory efficient unet)\n",
        "\n",
        "        if exists(self.init_resnet_block):\n",
        "            x = self.init_resnet_block(x, t)\n",
        "\n",
        "        # go through the layers of the unet, down and up\n",
        "\n",
        "        hiddens = []\n",
        "\n",
        "        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "            if exists(pre_downsample):\n",
        "                x = pre_downsample(x)\n",
        "\n",
        "            x = init_block(x, t, c)\n",
        "\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = resnet_block(x, t)\n",
        "                hiddens.append(x)\n",
        "\n",
        "            x = attn_block(x, c)\n",
        "            hiddens.append(x)\n",
        "\n",
        "            if exists(post_downsample):\n",
        "                x = post_downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t, c)\n",
        "\n",
        "        if exists(self.mid_attn):\n",
        "            x = self.mid_attn(x)\n",
        "\n",
        "        x = self.mid_block2(x, t, c)\n",
        "\n",
        "        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n",
        "\n",
        "        up_hiddens = []\n",
        "\n",
        "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
        "            x = add_skip_connection(x)\n",
        "            x = init_block(x, t, c)\n",
        "\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = add_skip_connection(x)\n",
        "                x = resnet_block(x, t)\n",
        "\n",
        "            x = attn_block(x, c)\n",
        "            up_hiddens.append(x.contiguous())\n",
        "            x = upsample(x)\n",
        "\n",
        "        # whether to combine all feature maps from upsample blocks\n",
        "\n",
        "        x = self.upsample_combiner(x, up_hiddens)\n",
        "\n",
        "        # final top-most residual if needed\n",
        "\n",
        "        if self.init_conv_to_final_conv_residual:\n",
        "            x = torch.cat((x, init_conv_residual), dim = 1)\n",
        "\n",
        "        if exists(self.final_res_block):\n",
        "            x = self.final_res_block(x, t)\n",
        "\n",
        "        if exists(lowres_cond_img):\n",
        "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "# null unet\n",
        "\n",
        "class NullUnet(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.lowres_cond = False\n",
        "        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def cast_model_parameters(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "# predefined unets, with configs lining up with hyperparameters in appendix of paper\n",
        "\n",
        "class BaseUnet64(Unet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        default_kwargs = dict(\n",
        "            dim = 512,\n",
        "            dim_mults = (1, 2, 3, 4),\n",
        "            num_resnet_blocks = 3,\n",
        "            layer_attns = (False, True, True, True),\n",
        "            layer_cross_attns = (False, True, True, True),\n",
        "            attn_heads = 8,\n",
        "            ff_mult = 2.,\n",
        "            memory_efficient = False\n",
        "        )\n",
        "        super().__init__(*args, **{**default_kwargs, **kwargs})\n",
        "\n",
        "class SRUnet256(Unet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        default_kwargs = dict(\n",
        "            dim = 128,\n",
        "            dim_mults = (1, 2, 4, 8),\n",
        "            num_resnet_blocks = (2, 4, 8, 8),\n",
        "            layer_attns = (False, False, False, True),\n",
        "            layer_cross_attns = (False, False, False, True),\n",
        "            attn_heads = 8,\n",
        "            ff_mult = 2.,\n",
        "            memory_efficient = True\n",
        "        )\n",
        "        super().__init__(*args, **{**default_kwargs, **kwargs})\n",
        "\n",
        "class SRUnet1024(Unet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        default_kwargs = dict(\n",
        "            dim = 128,\n",
        "            dim_mults = (1, 2, 4, 8),\n",
        "            num_resnet_blocks = (2, 4, 8, 8),\n",
        "            layer_attns = False,\n",
        "            layer_cross_attns = (False, False, False, True),\n",
        "            attn_heads = 8,\n",
        "            ff_mult = 2.,\n",
        "            memory_efficient = True\n",
        "        )\n",
        "        super().__init__(*args, **{**default_kwargs, **kwargs})\n",
        "\n",
        "# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n",
        "\n",
        "class Imagen(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unets,\n",
        "        *,\n",
        "        image_sizes,                                # for cascading ddpm, image size at each stage\n",
        "        text_encoder_name = DEFAULT_T5_NAME,\n",
        "        text_embed_dim = None,\n",
        "        channels = 3,\n",
        "        timesteps = 1000,\n",
        "        cond_drop_prob = 0.1,\n",
        "        loss_type = 'l2',\n",
        "        noise_schedules = 'cosine',\n",
        "        pred_objectives = 'noise',\n",
        "        random_crop_sizes = None,\n",
        "        lowres_noise_schedule = 'linear',\n",
        "        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n",
        "        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n",
        "        condition_on_text = True,\n",
        "        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n",
        "        dynamic_thresholding = True,\n",
        "        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n",
        "        only_train_unet_number = None,\n",
        "        temporal_downsample_factor = 1,\n",
        "        resize_cond_video_frames = True,\n",
        "        resize_mode = 'nearest',\n",
        "        min_snr_loss_weight = True,                 # https://arxiv.org/abs/2303.09556\n",
        "        min_snr_gamma = 5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # loss\n",
        "\n",
        "        if loss_type == 'l1':\n",
        "            loss_fn = F.l1_loss\n",
        "        elif loss_type == 'l2':\n",
        "            loss_fn = F.mse_loss\n",
        "        elif loss_type == 'huber':\n",
        "            loss_fn = F.smooth_l1_loss\n",
        "\n",
        "        self.loss_type = loss_type\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        # conditioning hparams\n",
        "\n",
        "        self.condition_on_text = condition_on_text\n",
        "        self.unconditional = not condition_on_text\n",
        "\n",
        "        # channels\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "        # automatically take care of ensuring that first unet is unconditional\n",
        "        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n",
        "\n",
        "        unets = cast_tuple(unets)\n",
        "        num_unets = len(unets)\n",
        "\n",
        "        # determine noise schedules per unet\n",
        "\n",
        "        timesteps = cast_tuple(timesteps, num_unets)\n",
        "\n",
        "        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n",
        "\n",
        "        noise_schedules = cast_tuple(noise_schedules)\n",
        "        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n",
        "        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n",
        "\n",
        "        # construct noise schedulers\n",
        "\n",
        "        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n",
        "        self.noise_schedulers = nn.ModuleList([])\n",
        "\n",
        "        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n",
        "            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n",
        "            self.noise_schedulers.append(noise_scheduler)\n",
        "\n",
        "        # randomly cropping for upsampler training\n",
        "\n",
        "        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n",
        "        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n",
        "\n",
        "        # lowres augmentation noise schedule\n",
        "\n",
        "        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n",
        "\n",
        "        # ddpm objectives - predicting noise by default\n",
        "\n",
        "        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n",
        "\n",
        "        # get text encoder\n",
        "\n",
        "        self.text_encoder_name = text_encoder_name\n",
        "        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n",
        "\n",
        "        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n",
        "\n",
        "        # construct unets\n",
        "\n",
        "        self.unets = nn.ModuleList([])\n",
        "\n",
        "        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n",
        "        self.only_train_unet_number = only_train_unet_number\n",
        "\n",
        "        for ind, one_unet in enumerate(unets):\n",
        "            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n",
        "            is_first = ind == 0\n",
        "\n",
        "            one_unet = one_unet.cast_model_parameters(\n",
        "                lowres_cond = not is_first,\n",
        "                cond_on_text = self.condition_on_text,\n",
        "                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n",
        "                channels = self.channels,\n",
        "                channels_out = self.channels\n",
        "            )\n",
        "\n",
        "            self.unets.append(one_unet)\n",
        "\n",
        "        # unet image sizes\n",
        "\n",
        "        image_sizes = cast_tuple(image_sizes)\n",
        "        self.image_sizes = image_sizes\n",
        "\n",
        "        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n",
        "\n",
        "        self.sample_channels = cast_tuple(self.channels, num_unets)\n",
        "\n",
        "        # determine whether we are training on images or video\n",
        "\n",
        "        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n",
        "        self.is_video = is_video\n",
        "\n",
        "        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n",
        "\n",
        "        self.resize_to = resize_video_to if is_video else resize_image_to\n",
        "        self.resize_to = partial(self.resize_to, mode = resize_mode)\n",
        "\n",
        "        # temporal interpolation\n",
        "\n",
        "        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n",
        "        self.temporal_downsample_factor = temporal_downsample_factor\n",
        "\n",
        "        self.resize_cond_video_frames = resize_cond_video_frames\n",
        "        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n",
        "\n",
        "        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n",
        "        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n",
        "\n",
        "        # cascading ddpm related stuff\n",
        "\n",
        "        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n",
        "        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n",
        "\n",
        "        self.lowres_sample_noise_level = lowres_sample_noise_level\n",
        "        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n",
        "\n",
        "        # classifier free guidance\n",
        "\n",
        "        self.cond_drop_prob = cond_drop_prob\n",
        "        self.can_classifier_guidance = cond_drop_prob > 0.\n",
        "\n",
        "        # normalize and unnormalize image functions\n",
        "\n",
        "        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n",
        "        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n",
        "        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n",
        "\n",
        "        # dynamic thresholding\n",
        "\n",
        "        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n",
        "        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n",
        "\n",
        "        # min snr loss weight\n",
        "\n",
        "        min_snr_loss_weight = cast_tuple(min_snr_loss_weight, num_unets)\n",
        "        min_snr_gamma = cast_tuple(min_snr_gamma, num_unets)\n",
        "\n",
        "        assert len(min_snr_loss_weight) == len(min_snr_gamma) == num_unets\n",
        "        self.min_snr_gamma = tuple((gamma if use_min_snr else None) for use_min_snr, gamma in zip(min_snr_loss_weight, min_snr_gamma))\n",
        "\n",
        "        # one temp parameter for keeping track of device\n",
        "\n",
        "        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n",
        "\n",
        "        # default to device of unets passed in\n",
        "\n",
        "        self.to(next(self.unets.parameters()).device)\n",
        "\n",
        "    def force_unconditional_(self):\n",
        "        self.condition_on_text = False\n",
        "        self.unconditional = True\n",
        "\n",
        "        for unet in self.unets:\n",
        "            unet.cond_on_text = False\n",
        "\n",
        "\n",
        "    # gaussian diffusion methods\n",
        "\n",
        "    def p_mean_variance(\n",
        "        self,\n",
        "        unet,\n",
        "        x,\n",
        "        t,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        lowres_cond_img = None,\n",
        "        self_cond = None,\n",
        "        lowres_noise_times = None,\n",
        "        cond_scale = 1.,\n",
        "        cfg_remove_parallel_component = True,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        model_output = None,\n",
        "        t_next = None,\n",
        "        pred_objective = 'noise',\n",
        "        dynamic_threshold = True\n",
        "    ):\n",
        "        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n",
        "\n",
        "        video_kwargs = dict()\n",
        "        if self.is_video:\n",
        "            video_kwargs = dict(\n",
        "                cond_video_frames = cond_video_frames,\n",
        "                post_cond_video_frames = post_cond_video_frames,\n",
        "            )\n",
        "\n",
        "        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n",
        "            x,\n",
        "            noise_scheduler.get_condition(t),\n",
        "            text_embeds = text_embeds,\n",
        "            text_mask = text_mask,\n",
        "            cond_images = cond_images,\n",
        "            cond_scale = cond_scale,\n",
        "            remove_parallel_component = cfg_remove_parallel_component,\n",
        "            keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "            lowres_cond_img = lowres_cond_img,\n",
        "            self_cond = self_cond,\n",
        "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),\n",
        "            **video_kwargs\n",
        "        ))\n",
        "\n",
        "        if pred_objective == 'noise':\n",
        "            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n",
        "        elif pred_objective == 'x_start':\n",
        "            x_start = pred\n",
        "        elif pred_objective == 'v':\n",
        "            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n",
        "        else:\n",
        "            raise ValueError(f'unknown objective {pred_objective}')\n",
        "\n",
        "        if dynamic_threshold:\n",
        "            # following pseudocode in appendix\n",
        "            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n",
        "            s = torch.quantile(\n",
        "                rearrange(x_start, 'b ... -> b (...)').abs(),\n",
        "                self.dynamic_thresholding_percentile,\n",
        "                dim = -1\n",
        "            )\n",
        "\n",
        "            s.clamp_(min = 1.)\n",
        "            s = right_pad_dims_to(x_start, s)\n",
        "            x_start = x_start.clamp(-s, s) / s\n",
        "        else:\n",
        "            x_start.clamp_(-1., 1.)\n",
        "\n",
        "        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n",
        "        return mean_and_variance, x_start\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(\n",
        "        self,\n",
        "        unet,\n",
        "        x,\n",
        "        t,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        t_next = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        cond_scale = 1.,\n",
        "        cfg_remove_parallel_component = True,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        self_cond = None,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_noise_times = None,\n",
        "        pred_objective = 'noise',\n",
        "        dynamic_threshold = True\n",
        "    ):\n",
        "        b, *_, device = *x.shape, x.device\n",
        "\n",
        "        video_kwargs = dict()\n",
        "        if self.is_video:\n",
        "            video_kwargs = dict(\n",
        "                cond_video_frames = cond_video_frames,\n",
        "                post_cond_video_frames = post_cond_video_frames,\n",
        "            )\n",
        "\n",
        "        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n",
        "            unet,\n",
        "            x = x,\n",
        "            t = t,\n",
        "            t_next = t_next,\n",
        "            noise_scheduler = noise_scheduler,\n",
        "            text_embeds = text_embeds,\n",
        "            text_mask = text_mask,\n",
        "            cond_images = cond_images,\n",
        "            cond_scale = cond_scale,\n",
        "            cfg_remove_parallel_component = cfg_remove_parallel_component,\n",
        "            cfg_keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "            lowres_cond_img = lowres_cond_img,\n",
        "            self_cond = self_cond,\n",
        "            lowres_noise_times = lowres_noise_times,\n",
        "            pred_objective = pred_objective,\n",
        "            dynamic_threshold = dynamic_threshold,\n",
        "            **video_kwargs\n",
        "        )\n",
        "\n",
        "        noise = torch.randn_like(x)\n",
        "        # no noise when t == 0\n",
        "        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n",
        "        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
        "        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
        "        return pred, x_start\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(\n",
        "        self,\n",
        "        unet,\n",
        "        shape,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_noise_times = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        inpaint_images = None,\n",
        "        inpaint_videos = None,\n",
        "        inpaint_masks = None,\n",
        "        inpaint_resample_times = 5,\n",
        "        init_images = None,\n",
        "        skip_steps = None,\n",
        "        cond_scale = 1,\n",
        "        cfg_remove_parallel_component = False,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        pred_objective = 'noise',\n",
        "        dynamic_threshold = True,\n",
        "        use_tqdm = True\n",
        "    ):\n",
        "        device = self.device\n",
        "\n",
        "        batch = shape[0]\n",
        "        img = torch.randn(shape, device = device)\n",
        "\n",
        "        # video\n",
        "\n",
        "        is_video = len(shape) == 5\n",
        "        frames = shape[-3] if is_video else None\n",
        "        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n",
        "\n",
        "        # for initialization with an image or video\n",
        "\n",
        "        if exists(init_images):\n",
        "            img += init_images\n",
        "\n",
        "        # keep track of x0, for self conditioning\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        # prepare inpainting\n",
        "\n",
        "        inpaint_images = default(inpaint_videos, inpaint_images)\n",
        "\n",
        "        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n",
        "        resample_times = inpaint_resample_times if has_inpainting else 1\n",
        "\n",
        "        if has_inpainting:\n",
        "            inpaint_images = self.normalize_img(inpaint_images)\n",
        "            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n",
        "            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1], **resize_kwargs).bool()\n",
        "\n",
        "        # time\n",
        "\n",
        "        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n",
        "\n",
        "        # whether to skip any steps\n",
        "\n",
        "        skip_steps = default(skip_steps, 0)\n",
        "        timesteps = timesteps[skip_steps:]\n",
        "\n",
        "        # video conditioning kwargs\n",
        "\n",
        "        video_kwargs = dict()\n",
        "        if self.is_video:\n",
        "            video_kwargs = dict(\n",
        "                cond_video_frames = cond_video_frames,\n",
        "                post_cond_video_frames = post_cond_video_frames,\n",
        "            )\n",
        "\n",
        "        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n",
        "            is_last_timestep = times_next == 0\n",
        "\n",
        "            for r in reversed(range(resample_times)):\n",
        "                is_last_resample_step = r == 0\n",
        "\n",
        "                if has_inpainting:\n",
        "                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n",
        "                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n",
        "\n",
        "                self_cond = x_start if unet.self_cond else None\n",
        "\n",
        "                img, x_start = self.p_sample(\n",
        "                    unet,\n",
        "                    img,\n",
        "                    times,\n",
        "                    t_next = times_next,\n",
        "                    text_embeds = text_embeds,\n",
        "                    text_mask = text_mask,\n",
        "                    cond_images = cond_images,\n",
        "                    cond_scale = cond_scale,\n",
        "                    cfg_remove_parallel_component = cfg_remove_parallel_component,\n",
        "                    cfg_keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "                    self_cond = self_cond,\n",
        "                    lowres_cond_img = lowres_cond_img,\n",
        "                    lowres_noise_times = lowres_noise_times,\n",
        "                    noise_scheduler = noise_scheduler,\n",
        "                    pred_objective = pred_objective,\n",
        "                    dynamic_threshold = dynamic_threshold,\n",
        "                    **video_kwargs\n",
        "                )\n",
        "\n",
        "                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n",
        "                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n",
        "\n",
        "                    img = torch.where(\n",
        "                        self.right_pad_dims_to_datatype(is_last_timestep),\n",
        "                        img,\n",
        "                        renoised_img\n",
        "                    )\n",
        "\n",
        "        img.clamp_(-1., 1.)\n",
        "\n",
        "        # final inpainting\n",
        "\n",
        "        if has_inpainting:\n",
        "            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n",
        "\n",
        "        unnormalize_img = self.unnormalize_img(img)\n",
        "        return unnormalize_img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    @eval_decorator\n",
        "    @beartype\n",
        "    def sample(\n",
        "        self,\n",
        "        texts: Optional[List[str]] = None,\n",
        "        text_masks = None,\n",
        "        text_embeds = None,\n",
        "        video_frames = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        inpaint_videos = None,\n",
        "        inpaint_images = None,\n",
        "        inpaint_masks = None,\n",
        "        inpaint_resample_times = 5,\n",
        "        init_images = None,\n",
        "        skip_steps = None,\n",
        "        batch_size = 1,\n",
        "        cond_scale = 1.,\n",
        "        cfg_remove_parallel_component = True,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        lowres_sample_noise_level = None,\n",
        "        start_at_unet_number = 1,\n",
        "        start_image_or_video = None,\n",
        "        stop_at_unet_number = None,\n",
        "        return_all_unet_outputs = False,\n",
        "        return_pil_images = False,\n",
        "        device = None,\n",
        "        use_tqdm = True,\n",
        "        use_one_unet_in_gpu = True\n",
        "    ):\n",
        "        device = default(device, self.device)\n",
        "        self.reset_unets_all_one_device(device = device)\n",
        "\n",
        "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n",
        "\n",
        "        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n",
        "            assert all([*map(len, texts)]), 'text cannot be empty'\n",
        "\n",
        "            with autocast('cuda', enabled = False):\n",
        "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n",
        "\n",
        "            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n",
        "\n",
        "        if not self.unconditional:\n",
        "            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n",
        "\n",
        "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n",
        "            batch_size = text_embeds.shape[0]\n",
        "\n",
        "        # inpainting\n",
        "\n",
        "        inpaint_images = default(inpaint_videos, inpaint_images)\n",
        "\n",
        "        if exists(inpaint_images):\n",
        "            if self.unconditional:\n",
        "                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n",
        "                    batch_size = inpaint_images.shape[0]\n",
        "\n",
        "            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n",
        "            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n",
        "\n",
        "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n",
        "        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n",
        "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n",
        "\n",
        "        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        is_cuda = next(self.parameters()).is_cuda\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n",
        "\n",
        "        num_unets = len(self.unets)\n",
        "\n",
        "        # condition scaling\n",
        "\n",
        "        cond_scale = cast_tuple(cond_scale, num_unets)\n",
        "\n",
        "        # add frame dimension for video\n",
        "\n",
        "        if self.is_video and exists(inpaint_images):\n",
        "            video_frames = inpaint_images.shape[2]\n",
        "\n",
        "            if inpaint_masks.ndim == 3:\n",
        "                inpaint_masks = repeat(inpaint_masks, 'b h w -> b f h w', f = video_frames)\n",
        "\n",
        "            assert inpaint_masks.shape[1] == video_frames\n",
        "\n",
        "        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n",
        "\n",
        "        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n",
        "\n",
        "        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n",
        "\n",
        "        # for initial image and skipping steps\n",
        "\n",
        "        init_images = cast_tuple(init_images, num_unets)\n",
        "        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n",
        "\n",
        "        skip_steps = cast_tuple(skip_steps, num_unets)\n",
        "\n",
        "        # handle starting at a unet greater than 1, for training only-upscaler training\n",
        "\n",
        "        if start_at_unet_number > 1:\n",
        "            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n",
        "            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n",
        "            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n",
        "\n",
        "            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n",
        "            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n",
        "            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n",
        "\n",
        "\n",
        "        # go through each unet in cascade\n",
        "\n",
        "        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):\n",
        "\n",
        "            if unet_number < start_at_unet_number:\n",
        "                continue\n",
        "\n",
        "            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n",
        "\n",
        "            context = self.one_unet_in_gpu(unet = unet) if is_cuda and use_one_unet_in_gpu else nullcontext()\n",
        "\n",
        "            with context:\n",
        "                # video kwargs\n",
        "\n",
        "                video_kwargs = dict()\n",
        "                if self.is_video:\n",
        "                    video_kwargs = dict(\n",
        "                        cond_video_frames = cond_video_frames,\n",
        "                        post_cond_video_frames = post_cond_video_frames,\n",
        "                    )\n",
        "\n",
        "                    video_kwargs = compact(video_kwargs)\n",
        "\n",
        "                if self.is_video and self.resize_cond_video_frames:\n",
        "                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n",
        "                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n",
        "\n",
        "                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n",
        "                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n",
        "\n",
        "                # low resolution conditioning\n",
        "\n",
        "                lowres_cond_img = lowres_noise_times = None\n",
        "                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n",
        "\n",
        "                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n",
        "\n",
        "                if unet.lowres_cond:\n",
        "                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n",
        "\n",
        "                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n",
        "\n",
        "                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n",
        "                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n",
        "\n",
        "                # init images or video\n",
        "\n",
        "                if exists(unet_init_images):\n",
        "                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n",
        "\n",
        "                # shape of stage\n",
        "\n",
        "                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n",
        "\n",
        "                img = self.p_sample_loop(\n",
        "                    unet,\n",
        "                    shape,\n",
        "                    text_embeds = text_embeds,\n",
        "                    text_mask = text_masks,\n",
        "                    cond_images = cond_images,\n",
        "                    inpaint_images = inpaint_images,\n",
        "                    inpaint_masks = inpaint_masks,\n",
        "                    inpaint_resample_times = inpaint_resample_times,\n",
        "                    init_images = unet_init_images,\n",
        "                    skip_steps = unet_skip_steps,\n",
        "                    cond_scale = unet_cond_scale,\n",
        "                    cfg_remove_parallel_component = cfg_remove_parallel_component,\n",
        "                    cfg_keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "                    lowres_cond_img = lowres_cond_img,\n",
        "                    lowres_noise_times = lowres_noise_times,\n",
        "                    noise_scheduler = noise_scheduler,\n",
        "                    pred_objective = pred_objective,\n",
        "                    dynamic_threshold = dynamic_threshold,\n",
        "                    use_tqdm = use_tqdm,\n",
        "                    **video_kwargs\n",
        "                )\n",
        "\n",
        "                outputs.append(img)\n",
        "\n",
        "            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n",
        "                break\n",
        "\n",
        "        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n",
        "\n",
        "        if not return_pil_images:\n",
        "            return outputs[output_index]\n",
        "\n",
        "        if not return_all_unet_outputs:\n",
        "            outputs = outputs[-1:]\n",
        "\n",
        "        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n",
        "\n",
        "        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n",
        "\n",
        "        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n",
        "\n",
        "    @beartype\n",
        "    def p_losses(\n",
        "        self,\n",
        "        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n",
        "        x_start,\n",
        "        times,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_aug_times = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        noise = None,\n",
        "        times_next = None,\n",
        "        pred_objective = 'noise',\n",
        "        min_snr_gamma = None,\n",
        "        random_crop_size = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        is_video = x_start.ndim == 5\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "        # normalize to [-1, 1]\n",
        "\n",
        "        x_start = self.normalize_img(x_start)\n",
        "        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n",
        "\n",
        "        # random cropping during training\n",
        "        # for upsamplers\n",
        "\n",
        "        if exists(random_crop_size):\n",
        "            if is_video:\n",
        "                frames = x_start.shape[2]\n",
        "                x_start, lowres_cond_img, noise = map(lambda t: rearrange(t, 'b c f h w -> (b f) c h w'), (x_start, lowres_cond_img, noise))\n",
        "\n",
        "            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n",
        "\n",
        "            # make sure low res conditioner and image both get augmented the same way\n",
        "            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n",
        "            x_start = aug(x_start)\n",
        "            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n",
        "            noise = aug(noise, params = aug._params)\n",
        "\n",
        "            if is_video:\n",
        "                x_start, lowres_cond_img, noise = map(lambda t: rearrange(t, '(b f) c h w -> b c f h w', f = frames), (x_start, lowres_cond_img, noise))\n",
        "\n",
        "        # get x_t\n",
        "\n",
        "        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n",
        "\n",
        "        # also noise the lowres conditioning image\n",
        "        # at sample time, they then fix the noise level of 0.1 - 0.3\n",
        "\n",
        "        lowres_cond_img_noisy = None\n",
        "        if exists(lowres_cond_img):\n",
        "            lowres_aug_times = default(lowres_aug_times, times)\n",
        "            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n",
        "\n",
        "        # time condition\n",
        "\n",
        "        noise_cond = noise_scheduler.get_condition(times)\n",
        "\n",
        "        # unet kwargs\n",
        "\n",
        "        unet_kwargs = dict(\n",
        "            text_embeds = text_embeds,\n",
        "            text_mask = text_mask,\n",
        "            cond_images = cond_images,\n",
        "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n",
        "            lowres_cond_img = lowres_cond_img_noisy,\n",
        "            cond_drop_prob = self.cond_drop_prob,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # self condition if needed\n",
        "\n",
        "        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n",
        "        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n",
        "        # access the member 'module' of the wrapped unet instance.\n",
        "        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n",
        "\n",
        "        if self_cond and random() < 0.5:\n",
        "            with torch.no_grad():\n",
        "                pred = unet.forward(\n",
        "                    x_noisy,\n",
        "                    noise_cond,\n",
        "                    **unet_kwargs\n",
        "                ).detach()\n",
        "\n",
        "                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n",
        "\n",
        "                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n",
        "\n",
        "        # get prediction\n",
        "\n",
        "        pred = unet.forward(\n",
        "            x_noisy,\n",
        "            noise_cond,\n",
        "            **unet_kwargs\n",
        "        )\n",
        "\n",
        "        # prediction objective\n",
        "\n",
        "        if pred_objective == 'noise':\n",
        "            target = noise\n",
        "        elif pred_objective == 'x_start':\n",
        "            target = x_start\n",
        "        elif pred_objective == 'v':\n",
        "            # derivation detailed in Appendix D of Progressive Distillation paper\n",
        "            # https://arxiv.org/abs/2202.00512\n",
        "            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n",
        "            target = alpha * noise - sigma * x_start\n",
        "        else:\n",
        "            raise ValueError(f'unknown objective {pred_objective}')\n",
        "\n",
        "        # losses\n",
        "\n",
        "        losses = self.loss_fn(pred, target, reduction = 'none')\n",
        "        losses = reduce(losses, 'b ... -> b', 'mean')\n",
        "\n",
        "        # min snr loss reweighting\n",
        "\n",
        "        snr = log_snr.exp()\n",
        "        maybe_clipped_snr = snr.clone()\n",
        "\n",
        "        if exists(min_snr_gamma):\n",
        "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
        "\n",
        "        if pred_objective == 'noise':\n",
        "            loss_weight = maybe_clipped_snr / snr\n",
        "        elif pred_objective == 'x_start':\n",
        "            loss_weight = maybe_clipped_snr\n",
        "        elif pred_objective == 'v':\n",
        "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
        "\n",
        "        losses = losses * loss_weight\n",
        "        return losses.mean()\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self,\n",
        "        images, # rename to images or video\n",
        "        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n",
        "        texts: Optional[List[str]] = None,\n",
        "        text_embeds = None,\n",
        "        text_masks = None,\n",
        "        unet_number = None,\n",
        "        cond_images = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        if self.is_video and images.ndim == 4:\n",
        "            images = rearrange(images, 'b c h w -> b c 1 h w')\n",
        "            kwargs.update(ignore_time = True)\n",
        "\n",
        "        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n",
        "        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n",
        "        unet_number = default(unet_number, 1)\n",
        "        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n",
        "\n",
        "        images = cast_uint8_images_to_float(images)\n",
        "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n",
        "\n",
        "        assert images.dtype == torch.float or images.dtype == torch.half, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n",
        "\n",
        "        unet_index = unet_number - 1\n",
        "\n",
        "        unet = default(unet, lambda: self.get_unet(unet_number))\n",
        "\n",
        "        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n",
        "\n",
        "        noise_scheduler      = self.noise_schedulers[unet_index]\n",
        "        min_snr_gamma        = self.min_snr_gamma[unet_index]\n",
        "        pred_objective       = self.pred_objectives[unet_index]\n",
        "        target_image_size    = self.image_sizes[unet_index]\n",
        "        random_crop_size     = self.random_crop_sizes[unet_index]\n",
        "        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n",
        "\n",
        "        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n",
        "\n",
        "        assert images.shape[1] == self.channels\n",
        "        assert h >= target_image_size and w >= target_image_size\n",
        "\n",
        "        frames              = images.shape[2] if is_video else None\n",
        "        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n",
        "        ignore_time         = kwargs.get('ignore_time', False)\n",
        "\n",
        "        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n",
        "        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n",
        "        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n",
        "\n",
        "        times = noise_scheduler.sample_random_times(b, device = device)\n",
        "\n",
        "        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n",
        "            assert all([*map(len, texts)]), 'text cannot be empty'\n",
        "            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n",
        "\n",
        "            with autocast('cuda', enabled = False):\n",
        "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n",
        "\n",
        "            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n",
        "\n",
        "        if not self.unconditional:\n",
        "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n",
        "\n",
        "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n",
        "        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n",
        "\n",
        "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n",
        "\n",
        "        # handle video frame conditioning\n",
        "\n",
        "        if self.is_video and self.resize_cond_video_frames:\n",
        "            downsample_scale = self.temporal_downsample_factor[unet_index]\n",
        "            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n",
        "            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n",
        "            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n",
        "\n",
        "        # handle low resolution conditioning\n",
        "\n",
        "        lowres_cond_img = lowres_aug_times = None\n",
        "        if exists(prev_image_size):\n",
        "            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n",
        "            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n",
        "\n",
        "            if self.per_sample_random_aug_noise_level:\n",
        "                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(b, device = device)\n",
        "            else:\n",
        "                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n",
        "                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n",
        "\n",
        "        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n",
        "\n",
        "        return self.p_losses(unet, images, times, text_embeds = text_embeds, text_mask = text_masks, cond_images = cond_images, noise_scheduler = noise_scheduler, lowres_cond_img = lowres_cond_img, lowres_aug_times = lowres_aug_times, pred_objective = pred_objective, min_snr_gamma = min_snr_gamma, random_crop_size = random_crop_size, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdRMD_VpK2Yp",
        "outputId": "599b1990-7614-48a7-e655-4c8ef83a6ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "down fwd torch.Size([4, 128, 64, 64])\n",
            "down fwd torch.Size([4, 256, 32, 32])\n",
            "down fwd torch.Size([4, 512, 16, 16])\n",
            "down fwd torch.Size([4, 1024, 8, 8])\n",
            "torch.Size([4, 5, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# @title unet me\n",
        "# https://github.com/milesial/Pytorch-UNet/blob/master/unet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, mid_ch=None, bias=False):\n",
        "        super().__init__()\n",
        "        if mid_ch==None: mid_ch = out_ch\n",
        "        act = nn.ReLU(inplace=True) # ReLU GELU SiLU ELU\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(mid_ch), act,\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(out_ch), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_ch, out_ch),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "                nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=1),)\n",
        "        else: self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)#, in_ch // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1) # [c,h,w]\n",
        "        diffX, diffY = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        return self.conv(torch.cat([x2, x1], dim=1))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super().__init__()\n",
        "        # self.in_ch, self.out_ch, self.bilinear = in_ch, out_ch, bilinear\n",
        "        # self.inc = DoubleConv(in_ch, 64)\n",
        "        # self.down1 = Down(64, 128)\n",
        "        # self.down2 = Down(128, 256)\n",
        "        # self.down3 = Down(256, 512)\n",
        "        # factor = 2 if bilinear else 1\n",
        "        # self.down4 = Down(512, 1024 // factor)\n",
        "        # self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        # self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        # self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        # self.up4 = Up(128, 64, bilinear)\n",
        "        # self.outc = nn.Conv2d(64, out_ch, kernel_size=1)\n",
        "\n",
        "        # https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py\n",
        "        wf=6 # width factor, 2^6 = 64 -> 1024\n",
        "        depth=4\n",
        "        self.inc = DoubleConv(in_ch, 2 ** wf)\n",
        "        self.down_list = nn.ModuleList([Down(2 ** (wf + i), 2 ** (wf + i+1)) for i in range(depth)])\n",
        "        self.up_list = nn.ModuleList([Up(2 ** (wf + i+1), 2 ** (wf + i)) for i in reversed(range(depth))])\n",
        "        self.outc = nn.Conv2d(2 ** wf, out_ch, kernel_size=1)\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x1 = self.inc(x)\n",
        "    #     x2 = self.down1(x1)\n",
        "    #     x3 = self.down2(x2)\n",
        "    #     x4 = self.down3(x3)\n",
        "    #     x5 = self.down4(x4)\n",
        "    #     print(x5.shape, x4.shape)\n",
        "    #     x = self.up1(x5, x4)\n",
        "    #     x = self.up2(x, x3)\n",
        "    #     x = self.up3(x, x2)\n",
        "    #     x = self.up4(x, x1)\n",
        "    #     logits = self.outc(x)\n",
        "    #     return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        x = self.inc(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            blocks.append(x)\n",
        "            x = down(x)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "        return self.outc(x)\n",
        "\n",
        "\n",
        "unet = UNet(3, 5)\n",
        "x=torch.rand(4,3,64,64)\n",
        "# x=torch.rand(4,3,128,128)\n",
        "# x=torch.rand(4,3,28,28)\n",
        "out = unet(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aQJWp_oTxWRz"
      },
      "outputs": [],
      "source": [
        "# @title facebook unet\n",
        "# https://github.com/facebookresearch/RCDM/blob/main/guided_diffusion_rcdm/unet.py\n",
        "from abc import abstractmethod\n",
        "import math\n",
        "import threading\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch._C import has_mkldnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from .fp16_util import convert_module_to_f16, convert_module_to_f32\n",
        "from .nn import (\n",
        "    checkpoint,\n",
        "    conv_nd,\n",
        "    linear,\n",
        "    avg_pool_nd,\n",
        "    zero_module,\n",
        "    normalization,\n",
        "    timestep_embedding,\n",
        ")\n",
        "from .nn import normalization as normalization_gg\n",
        "from .layers import SNConv2d, ccbn, bn, SNLinear, identity\n",
        "import functools\n",
        "\n",
        "class AttentionPool2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spacial_dim,\n",
        "        embed_dim,\n",
        "        num_heads_channels,\n",
        "        output_dim = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Parameter(\n",
        "            torch.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5\n",
        "        )\n",
        "        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
        "        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
        "        self.num_heads = embed_dim // num_heads_channels\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *_spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)  # NC(HW)\n",
        "        x = torch.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n",
        "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n",
        "        x = self.qkv_proj(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"Apply the module to `x` given `emb` timestep embeddings.\"\"\"\n",
        "\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an extra input.\"\"\"\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 upsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, dims=2, out_ch=None):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_ch = out_ch or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3:\n",
        "            x = F.interpolate(\n",
        "                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n",
        "            )\n",
        "        else:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 downsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, dims=2, out_ch=None):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_ch = out_ch or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2)\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(\n",
        "                dims, self.channels, self.out_ch, 3, stride=stride, padding=1\n",
        "            )\n",
        "        else:\n",
        "            assert self.channels == self.out_ch\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    \"\"\"\n",
        "    A residual block that can optionally change the number of channels.\n",
        "\n",
        "    :param channels: the number of input channels.\n",
        "    :param emb_channels: the number of timestep embedding channels.\n",
        "    :param dropout: the rate of dropout.\n",
        "    :param out_channels: if specified, the number of out channels.\n",
        "    :param use_conv: if True and out_channels is specified, use a spatial\n",
        "        convolution instead of a smaller 1x1 convolution to change the\n",
        "        channels in the skip connection.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param use_checkpoint: if True, use gradient checkpointing on this module.\n",
        "    :param up: if True, use this block for upsampling.\n",
        "    :param down: if True, use this block for downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        normalization,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        dims=2,\n",
        "        use_checkpoint=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "        instance_cond=False,\n",
        "        pretrained=False,\n",
        "        use_fp16=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.use_fp16= use_fp16\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization_gg(channels),\n",
        "            nn.SiLU(),\n",
        "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization_gg(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(\n",
        "                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = conv_nd(\n",
        "                dims, channels, self.out_channels, 3, padding=1\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the block to a Tensor, conditioned on a timestep embedding.\n",
        "\n",
        "        :param x: an [N x C x ...] Tensor of features.\n",
        "        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        return checkpoint(\n",
        "            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n",
        "        )\n",
        "\n",
        "    def _forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply conditional batch normalization if feat is not None\n",
        "        \"\"\"\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other.\n",
        "\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "        use_new_attention_order=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert (\n",
        "                channels % num_head_channels == 0\n",
        "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = normalization_gg(channels)\n",
        "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
        "        if use_new_attention_order:\n",
        "            # split qkv before split heads\n",
        "            self.attention = QKVAttention(self.num_heads)\n",
        "        else:\n",
        "            # split heads before split qkv\n",
        "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "\n",
        "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return checkpoint(self._forward, (x,), self.parameters(), False)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    \"\"\"\n",
        "    A counter for the `thop` package to count the operations in an\n",
        "    attention operation.\n",
        "    Meant to be used like:\n",
        "        macs, params = thop.profile(\n",
        "            model,\n",
        "            inputs=(inputs, timestamps),\n",
        "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
        "        )\n",
        "    \"\"\"\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    # We perform two matmuls with the same number of ops.\n",
        "    # The first computes the weight matrix, the second computes\n",
        "    # the combination of the value vectors.\n",
        "    matmul_ops = 2 * b * (num_spatial ** 2) * c\n",
        "    model.total_ops += torch.DoubleTensor([matmul_ops])\n",
        "\n",
        "\n",
        "class QKVAttentionLegacy(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"Apply QKV attention.\n",
        "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"Apply QKV attention.\n",
        "\n",
        "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"The full UNet model with attention and timestep embedding.\n",
        "    :param in_channels: channels in the input Tensor.\n",
        "    :param model_channels: base channel count for the model.\n",
        "    :param out_channels: channels in the output Tensor.\n",
        "    :param num_res_blocks: number of residual blocks per downsample.\n",
        "    :param attention_resolutions: a collection of downsample rates at which attention will take place. May be a set, list, or tuple.\n",
        "        For example, if this contains 4, then at 4x downsampling, attention will be used.\n",
        "    :param dropout: the dropout probability.\n",
        "    :param channel_mult: channel multiplier for each level of the UNet.\n",
        "    :param conv_resample: if True, use learned convolutions for upsampling and downsampling.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param num_classes: if specified (as an int), then this model will be class-conditional with `num_classes` classes.\n",
        "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
        "    :param num_heads: the number of attention heads in each attention layer.\n",
        "    :param num_heads_channels: if specified, ignore num_heads and instead use a fixed channel width per attention head.\n",
        "    :param num_heads_upsample: works with num_heads to set a different number of heads for upsampling. Deprecated.\n",
        "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
        "    :param resblock_updown: use residual blocks for up/downsampling.\n",
        "    :param use_new_attention_order: use a different attention pattern for potentially increased efficiency.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_classes=None,\n",
        "        use_checkpoint=False,\n",
        "        use_fp16=False,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_new_attention_order=False,\n",
        "        cross_replica=False,\n",
        "        mybn=False,\n",
        "        BN_eps=1e-5,\n",
        "        SN_eps=1e-12,\n",
        "        instance_cond=False,\n",
        "        G_param=\"SN\",\n",
        "        shared_dim=128,\n",
        "        dim_z=128,\n",
        "        shared_dim_feat=512,\n",
        "        G_shared=True,\n",
        "        norm_style=\"bn\",\n",
        "        ssl_dim=2048,\n",
        "        pretrained=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        self.ssl_dim = ssl_dim\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_classes = num_classes\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.dtype = torch.float16 if use_fp16 else torch.float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.which_bn = normalization_gg\n",
        "        self.shared = nn.Identity()\n",
        "        self.ssl_emb = nn.Identity()\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        if instance_cond:\n",
        "            self.ssl_emb = nn.Linear(self.ssl_dim, time_embed_dim)\n",
        "            if use_fp16:\n",
        "                self.ssl_emb = self.ssl_emb.half()\n",
        "\n",
        "        ch = input_ch = int(channel_mult[0] * model_channels)\n",
        "        self.input_blocks = nn.ModuleList(\n",
        "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
        "        )\n",
        "        self._feature_size = ch\n",
        "        input_block_chans = [ch]\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        normalization=self.which_bn,\n",
        "                        out_channels=int(mult * model_channels),\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                        instance_cond=instance_cond,\n",
        "                        pretrained=pretrained,\n",
        "                        use_fp16=use_fp16,\n",
        "                    )\n",
        "                ]\n",
        "                ch = int(mult * model_channels)\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads,\n",
        "                            num_head_channels=num_head_channels,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        )\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            normalization=self.which_bn,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            down=True,\n",
        "                            instance_cond=instance_cond,\n",
        "                            pretrained=pretrained,\n",
        "                            use_fp16=use_fp16,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Downsample(\n",
        "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                normalization=self.which_bn,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "                instance_cond=instance_cond,\n",
        "                pretrained=pretrained,\n",
        "                use_fp16=use_fp16,\n",
        "            ),\n",
        "            AttentionBlock(\n",
        "                ch,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                num_heads=num_heads,\n",
        "                num_head_channels=num_head_channels,\n",
        "                use_new_attention_order=use_new_attention_order,\n",
        "            ),\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                normalization=self.which_bn,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "                instance_cond=instance_cond,\n",
        "                pretrained=pretrained,\n",
        "                use_fp16=use_fp16,\n",
        "            ),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch + ich,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        normalization=self.which_bn,\n",
        "                        out_channels=int(model_channels * mult),\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                        instance_cond=instance_cond,\n",
        "                        pretrained=pretrained,\n",
        "                        use_fp16=use_fp16,\n",
        "                    )\n",
        "                ]\n",
        "                ch = int(model_channels * mult)\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads_upsample,\n",
        "                            num_head_channels=num_head_channels,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        )\n",
        "                    )\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            normalization=self.which_bn,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            up=True,\n",
        "                            instance_cond=instance_cond,\n",
        "                            pretrained=pretrained,\n",
        "                            use_fp16=use_fp16,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            normalization_gg(ch),\n",
        "            nn.SiLU(),\n",
        "            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "    def convert_to_fp16(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float16.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f16)\n",
        "        self.middle_block.apply(convert_module_to_f16)\n",
        "        self.output_blocks.apply(convert_module_to_f16)\n",
        "\n",
        "    def convert_to_fp32(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float32.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f32)\n",
        "        self.middle_block.apply(convert_module_to_f32)\n",
        "        self.output_blocks.apply(convert_module_to_f32)\n",
        "\n",
        "    def forward(self, x, timesteps, y=None, feat=None):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "        :param x: an [N x C x ...] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :param y: an [N] Tensor of labels, if class-conditional.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        assert (y is not None) == (\n",
        "            self.num_classes is not None\n",
        "        ), \"must specify y if and only if the model is class-conditional\"\n",
        "\n",
        "        hs = []\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            assert y.shape == (x.shape[0],)\n",
        "            emb = emb + self.label_emb(y)\n",
        "\n",
        "        if feat is not None:\n",
        "            emb = emb + self.ssl_emb(feat)\n",
        "\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb)\n",
        "        for module in self.output_blocks:\n",
        "            h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = module(h, emb)\n",
        "        h = h.type(x.dtype)\n",
        "        return self.out(h)\n",
        "\n",
        "\n",
        "class SuperResModel(UNetModel):\n",
        "    \"\"\"\n",
        "    A UNetModel that performs super-resolution.\n",
        "\n",
        "    Expects an extra kwarg `low_res` to condition on a low-resolution image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, in_channels, *args, **kwargs):\n",
        "        super().__init__(image_size, in_channels * 2, *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, timesteps, low_res=None, **kwargs):\n",
        "        _, _, new_height, new_width = x.shape\n",
        "        upsampled = F.interpolate(low_res, (new_height, new_width), mode=\"bilinear\")\n",
        "        x = torch.cat([x, upsampled], dim=1)\n",
        "        return super().forward(x, timesteps, **kwargs)\n",
        "\n",
        "\n",
        "class EncoderUNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The half UNet model with attention and timestep embedding.\n",
        "\n",
        "    For usage, see UNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        use_checkpoint=False,\n",
        "        use_fp16=False,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_new_attention_order=False,\n",
        "        pool=\"adaptive\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.dtype = torch.float16 if use_fp16 else torch.float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        ch = int(channel_mult[0] * model_channels)\n",
        "        self.input_blocks = nn.ModuleList(\n",
        "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
        "        )\n",
        "        self._feature_size = ch\n",
        "        input_block_chans = [ch]\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=int(mult * model_channels),\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = int(mult * model_channels)\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads,\n",
        "                            num_head_channels=num_head_channels,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        )\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            down=True,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Downsample(\n",
        "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "            AttentionBlock(\n",
        "                ch,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                num_heads=num_heads,\n",
        "                num_head_channels=num_head_channels,\n",
        "                use_new_attention_order=use_new_attention_order,\n",
        "            ),\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "        self.pool = pool\n",
        "        if pool == \"adaptive\":\n",
        "            self.out = nn.Sequential(\n",
        "                normalization_gg(ch),\n",
        "                nn.SiLU(),\n",
        "                nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
        "                nn.Flatten(),\n",
        "            )\n",
        "        elif pool == \"attention\":\n",
        "            assert num_head_channels != -1\n",
        "            self.out = nn.Sequential(\n",
        "                normalization_gg(ch),\n",
        "                nn.SiLU(),\n",
        "                AttentionPool2d(\n",
        "                    (image_size // ds), ch, num_head_channels, 128\n",
        "                ),\n",
        "                nn.Linear(128, self.out_channels),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "        elif pool == \"spatial\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2048, self.out_channels),\n",
        "            )\n",
        "        elif pool == \"spatial_v2\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048),\n",
        "                #nn.ReLU(),\n",
        "                #normalization(2048),\n",
        "                #nn.SiLU(),\n",
        "                nn.Linear(2048, self.out_channels, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n",
        "\n",
        "    def convert_to_fp16(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float16.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f16)\n",
        "        self.middle_block.apply(convert_module_to_f16)\n",
        "\n",
        "    def convert_to_fp32(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float32.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f32)\n",
        "        self.middle_block.apply(convert_module_to_f32)\n",
        "\n",
        "    def forward(self, x, timesteps):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "\n",
        "        :param x: an [N x C x ...] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :return: an [N x K] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "        results = []\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            if self.pool.startswith(\"spatial\"):\n",
        "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "        h = self.middle_block(h, emb)\n",
        "        if self.pool.startswith(\"spatial\"):\n",
        "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "            h = torch.cat(results, axis=-1)\n",
        "            return self.out(h)\n",
        "        else:\n",
        "            h = h.type(x.dtype)\n",
        "            return self.out(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QKP61jCeB4cA"
      },
      "outputs": [],
      "source": [
        "# @title sinusoidal time emb\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/util.py\n",
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device)\n",
        "        args = timesteps[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2: embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    else: embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
        "    return embedding # [N, dim]\n",
        "\n",
        "\n",
        "# @title rotary_embedding\n",
        "import torch\n",
        "def rotary_embedding(seq_len, dim):\n",
        "    # theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "    theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
        "    # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    pos = torch.arange(seq_len).unsqueeze(1)\n",
        "    angles = pos * theta # [seq_len, 1] * [dim // 2] = [seq_len, dim // 2]\n",
        "    return torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim]\n",
        "\n",
        "def apply_rotary_embedding(embeddings, rotary_emb):\n",
        "    batch, seq_len, dim = embeddings.shape\n",
        "    # if rotary_emb.shape[0] < seq_len: print(\"rotary_emb.shape[0] < seq_len\")\n",
        "    rotary_emb = rotary_emb[:seq_len, :].unsqueeze(0).expand(batch, -1, -1)\n",
        "    embeddings = embeddings.view(batch, seq_len, dim // 2, 2)\n",
        "    rotary_emb = rotary_emb.view(batch, seq_len, dim // 2, 2)\n",
        "    # rotated_embeddings = torch.einsum('...ij,...ij->...ij', embeddings, rotary_emb)\n",
        "    rotated_embeddings = embeddings * rotary_emb\n",
        "    return rotated_embeddings.view(*rotated_embeddings.shape[:-2], dim)\n",
        "\n",
        "\n",
        "seq_len = 10\n",
        "dim = 64  # Must be even\n",
        "batch_size = 2\n",
        "rotary_emb = rotary_embedding(seq_len, dim)\n",
        "input_embeddings = torch.randn(batch_size, seq_len, dim)\n",
        "output_embeddings = apply_rotary_embedding(input_embeddings, rotary_emb)\n",
        "print(\"Input Embeddings:\", input_embeddings.shape)\n",
        "print(\"Rotary Embeddings:\", rotary_emb.shape)\n",
        "print(\"Output Embeddings:\", output_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "CEQjRDpSh-hh",
        "outputId": "93868375-7167-4abc-ae44-7f0d22cf918b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (16) must match the existing size (10) at non-singleton dimension 2.  Target sizes: [10, 3, 16, 16].  Tensor sizes: [10, 1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-842ba4a54d5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# t = (torch.rand(1, device=device) + torch.arange(batch, device=device)/batch) % (1 - eps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (16) must match the existing size (10) at non-singleton dimension 2.  Target sizes: [10, 3, 16, 16].  Tensor sizes: [10, 1]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch = 10\n",
        "eps = 1e-5\n",
        "x1 = torch.rand((batch,3,16,16), device=device)\n",
        "# t = (torch.rand(1, device=device) + torch.arange(batch, device=device)/batch) % (1 - eps)\n",
        "t = torch.rand(batch, device=device)\n",
        "t = t[:, None].expand(x1.shape)\n",
        "\n",
        "print(t)\n",
        "# print(torch.rand(1, device=device), torch.arange(batch, device=device)/batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vt8djkRrrELg"
      },
      "outputs": [],
      "source": [
        "# @title keishihara train\n",
        "# https://github.com/keishihara/flow-matching/blob/main/train_flow_matching_on_image.py\n",
        "    for epoch in range(args.n_epochs):\n",
        "        flow.train()\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1:2d}/{args.n_epochs}\")\n",
        "\n",
        "        for x_1, y in pbar:\n",
        "            x_1, y = x_1.to(device), y.to(device)\n",
        "\n",
        "            # Compute the probability path samples\n",
        "            x_0 = torch.randn_like(x_1)\n",
        "            t = torch.rand(x_1.size(0), device=device, dtype=x_1.dtype)\n",
        "            x_t, dx_t = path_sampler.sample(x_0, x_1, t)\n",
        "\n",
        "            flow.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Compute the conditional flow matching loss with class conditioning\n",
        "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "                vf_t = flow(t=t, x=x_t, y=y)\n",
        "                loss = F.mse_loss(vf_t, dx_t)\n",
        "\n",
        "            # Gradient scaling and backprop\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(flow.parameters(), max_norm=1.0)  # clip gradients\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DqgMPLjsVjg7"
      },
      "outputs": [],
      "source": [
        "# @title lebellig Flow_Matching train\n",
        "# https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "class OTFlowMatching:\n",
        "  def __init__(self, sig_min: float = 0.001) -> None:\n",
        "    super().__init__()\n",
        "    self.sig_min = sig_min\n",
        "    self.eps = 1e-5\n",
        "\n",
        "  def psi_t(self, x: torch.Tensor, x_1: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "    return (1 - (1 - self.sig_min) * t) * x + t * x_1\n",
        "\n",
        "  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:\n",
        "    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)\n",
        "    t = t[:, None].expand(x_1.shape)\n",
        "    x_0 = torch.randn_like(x_1)\n",
        "    v_psi = v_t(t[:,0], self.psi_t(x_0, x_1, t))\n",
        "    d_psi = x_1 - (1 - self.sig_min) * x_0\n",
        "    return torch.mean((v_psi - d_psi) ** 2)\n",
        "\n",
        "model = OTFlowMatching()\n",
        "net = Net(2, 2, [512]*5, 10).to(device)\n",
        "v_t = CondVF(net)\n",
        "\n",
        "losses = []\n",
        "optimizer = torch.optim.Adam(v_t.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in tqdm(range(5000), ncols=88):\n",
        "    for batch in dataloader:\n",
        "      x_1 = batch[0]\n",
        "      loss = model.loss(v_t, x_1)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses += [loss.detach()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VLhv7FyWZ8oG"
      },
      "outputs": [],
      "source": [
        "# @title drscotthawley FlowModels_colab.ipynb\n",
        "# https://colab.research.google.com/github/drscotthawley/blog/blob/main/extra/FlowModels_colab.ipynb\n",
        "\n",
        "def train(model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    batch_size=2048\n",
        "    pbar = tqdm(range(n_steps), leave=False)\n",
        "    for _ in pbar:\n",
        "        step += 1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 1 == 0:  # you could in theory not draw new points with each step, though we will.\n",
        "            source_samples = create_source_data(batch_size).to(device)\n",
        "            target_samples = create_target_data(batch_size) # this function also supports fm models from scratch\n",
        "            # target_samples = integrate_path(pretrained_model, source_samples, step_fn=rk4_step, warp_fn=warp_time, n_steps=20)\n",
        "\n",
        "        t = torch.rand(source_samples.size(0), 1).to(device) # random times for training\n",
        "        if warp_fn: t = warp_fn(t)  # time warp here (different from use in integrator!) helps focus \"coverage\" i.e. sampling the space\n",
        "\n",
        "        interpolated_samples = source_samples * (1 - t) + target_samples * t\n",
        "        v = model(interpolated_samples, t)\n",
        "        line_directions = target_samples - source_samples\n",
        "        loss = loss_fn(v, line_directions)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # pbar.set_description(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4g}')\n",
        "        pbar.set_description(f'Epoch [{epoch + 1}], Loss: {loss.item():.4g}')\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        model.eval()\n",
        "        clear_output(wait=True)  # Clear previous plots\n",
        "        viz(val_points, target_samples[:val_points.shape[0]], model)  # don't need rk4 for rect model viz b/c paths r straight\n",
        "        plt.show()\n",
        "        plt.close()  # Close the figure to free memory\n",
        "        model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "step, n_steps = 0, 100\n",
        "device = next(model.parameters()).device\n",
        "for epoch in range(40):\n",
        "    train(model, optimizer, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rStmiIIRPj9k"
      },
      "outputs": [],
      "source": [
        "# @title fair standalone_flow_matching\n",
        "# https://facebookresearch.github.io/flow_matching/notebooks/standalone_flow_matching.html\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    def __init__(self, dim = 2, h = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim + 1, h), nn.ELU(),\n",
        "            nn.Linear(h, h), nn.ELU(),\n",
        "            nn.Linear(h, h), nn.ELU(),\n",
        "            nn.Linear(h, dim))\n",
        "\n",
        "    def forward(self, t: Tensor, x_t: Tensor) -> Tensor:\n",
        "        return self.net(torch.cat((t, x_t), -1))\n",
        "\n",
        "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor) -> Tensor:\n",
        "        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n",
        "\n",
        "        return x_t + (t_end - t_start) * self(t=t_start + (t_end - t_start) / 2, x_t= x_t + self(x_t=x_t, t=t_start) * (t_end - t_start) / 2)\n",
        "\n",
        "\n",
        "flow = Flow()\n",
        "\n",
        "optimizer = torch.optim.Adam(flow.parameters(), 1e-2)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for _ in range(10000):\n",
        "    x_1 = Tensor(make_moons(256, noise=0.05)[0])\n",
        "    x_0 = torch.randn_like(x_1)\n",
        "    t = torch.rand(len(x_1), 1)\n",
        "    x_t = (1 - t) * x_0 + t * x_1\n",
        "    dx_t = x_1 - x_0\n",
        "    optimizer.zero_grad()\n",
        "    loss_fn(flow(t=t, x_t=x_t), dx_t).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "x = torch.randn(300, 2)\n",
        "n_steps = 8\n",
        "fig, axes = plt.subplots(1, n_steps + 1, figsize=(30, 4), sharex=True, sharey=True)\n",
        "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
        "\n",
        "axes[0].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10)\n",
        "axes[0].set_title(f't = {time_steps[0]:.2f}')\n",
        "axes[0].set_xlim(-3.0, 3.0)\n",
        "axes[0].set_ylim(-3.0, 3.0)\n",
        "\n",
        "for i in range(n_steps):\n",
        "    x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1])\n",
        "    axes[i + 1].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10)\n",
        "    axes[i + 1].set_title(f't = {time_steps[i + 1]:.2f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mFCFidMrLFEj"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_cost_matrix(X, Y):\n",
        "    \"\"\"Compute pairwise cost matrix (Euclidean distance).\"\"\"\n",
        "    X_norm = (X ** 2).sum(1).view(-1, 1)\n",
        "    Y_norm = (Y ** 2).sum(1).view(1, -1)\n",
        "    cost_matrix = X_norm + Y_norm - 2 * torch.mm(X, Y.T)\n",
        "    return cost_matrix\n",
        "\n",
        "def sinkhorn_algorithm(X, Y, , epsilon=1e-5, num_iters=50):\n",
        "    \"\"\"Compute the optimal transport plan using Sinkhorn algorithm.\"\"\"\n",
        "    cost_matrix = compute_cost_matrix(X, Y)\n",
        "    n, m = cost_matrix.shape\n",
        "    u = torch.ones(n, device=cost_matrix.device) / n\n",
        "    v = torch.ones(m, device=cost_matrix.device) / m\n",
        "    K = torch.exp(-cost_matrix / epsilon)\n",
        "    for _ in range(num_iters):\n",
        "        u = 1.0 / (K @ v)\n",
        "        v = 1.0 / (K.T @ u)\n",
        "    transport_plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    return transport_plan\n",
        "\n",
        "def ot_cfm_loss(x, y, epsilon=0.01, num_iters=50):\n",
        "    \"\"\"Compute the OT-CFM loss between source and target features.\"\"\"\n",
        "    cost_matrix = compute_cost_matrix(source_features, target_features)\n",
        "    # cost_matrix = torch.cdist(x, y, p=2)\n",
        "    transport_plan = sinkhorn_algorithm(cost_matrix, epsilon, num_iters)\n",
        "    # transport_plan = sinkhorn_algorithm(x,y, epsilon, num_iters)\n",
        "    ot_loss = (cost_matrix * transport_plan).sum()\n",
        "    return ot_loss\n",
        "\n",
        "\n",
        "def ot_cfm(X, Y, alpha=0.5): # src, tgt, mixing coef\n",
        "    \"\"\"Perform OT-based Classifier-Free Mixing.\"\"\"\n",
        "    transport_plan = sinkhorn_algorithm(X, Y)\n",
        "    # Match source samples to target using the OT plan\n",
        "    indices = transport_plan.argmax(dim=1)\n",
        "    matched_Y = Y[indices]\n",
        "    # Linear interpolation between X and matched_Y\n",
        "    mixed_samples = alpha * X + (1 - alpha) * matched_Y\n",
        "    return mixed_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate random source and target features\n",
        "source_features = torch.rand(32, 128)  # 32 samples, 128-dimensional features\n",
        "target_features = torch.rand(32, 128)\n",
        "\n",
        "# Compute OT-CFM loss\n",
        "loss = ot_cfm_loss(source_features, target_features)\n",
        "print(\"OT-CFM Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zpr53Gwo5tiR"
      },
      "outputs": [],
      "source": [
        "# @title stable diffusion unet save\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from ldm.modules.diffusionmodules.util import (\n",
        "#     conv_nd,\n",
        "#     avg_pool_nd,\n",
        "#     zero_module,\n",
        "#     normalization, # nn.GroupNorm\n",
        "#     timestep_embedding,\n",
        "# )\n",
        "# from ldm.modules.attention import SpatialTransformer # https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py\n",
        "\n",
        "class AttentionPool2d(nn.Module):\n",
        "    \"\"\"Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\"\"\"\n",
        "    def __init__(self, spacial_dim, embed_dim, num_heads_channels, output_dim = None,):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5)\n",
        "        # self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
        "        # self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
        "        self.qkv_proj = Conv1d(embed_dim, 3 * embed_dim, 1)\n",
        "        self.c_proj = Conv1d(embed_dim, output_dim or embed_dim, 1)\n",
        "        self.num_heads = embed_dim // num_heads_channels\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *_spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)  # NC(HW)\n",
        "        x = torch.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(1+HW)\n",
        "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(1+HW)\n",
        "        x = self.qkv_proj(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "from abc import abstractmethod\n",
        "class TimestepBlock(nn.Module):\n",
        "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"Apply the module to `x` given `emb` timestep embeddings.\"\"\"\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an extra input.\"\"\"\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock): x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer): x = layer(x, context)\n",
        "            else: x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then upsampling occurs in the inner-two dimensions.\"\"\"\n",
        "    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        else: x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv: x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D..\"\"\"\n",
        "    def __init__(self, channels, use_conv, dims=2, out_channels=None,padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial?]\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    \"\"\"A residual block that can optionally change the number of channels.\n",
        "    :param channels: the number of input channels.\n",
        "    :param emb_channels: the number of timestep embedding channels.\n",
        "    :param dropout: the rate of dropout.\n",
        "    :param out_channels: if specified, the number of out channels.\n",
        "    :param use_conv: if True and out_channels is specified, use a spatial convolution instead of a smaller 1x1 convolution to change the channels in the skip connection.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param up: if True, use this block for upsampling.\n",
        "    :param down: if True, use this block for downsampling.\"\"\"\n",
        "    def __init__(self, channels, emb_channels, dropout, out_channels=None, use_conv=False, use_scale_shift_norm=False, dims=2, up=False, down=False,):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels), nn.SiLU(), conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        self.updown = up or down\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(), nn.Linear(emb_channels, 2 * self.out_channels if use_scale_shift_norm else self.out_channels,),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(self.out_channels), nn.SiLU(), nn.Dropout(p=dropout),\n",
        "            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels: self.skip_connection = nn.Identity()\n",
        "        elif use_conv: self.skip_connection = conv_nd(dims, channels, self.out_channels, 3, padding=1)\n",
        "        else: self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, emb_ch]\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x) # norm, act\n",
        "            h = self.h_upd(h) # up/ down sample\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h) # conv\n",
        "        else:\n",
        "            h = self.in_layers(x) # norm, act, conv\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype) # act, lin\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h # [N, C, ...]\n",
        "\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\"\"\"\n",
        "    def __init__(self, ch, n_heads=1, d_head=-1,):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        if d_head == -1: self.n_heads = n_heads\n",
        "        else: self.n_heads = ch // d_head\n",
        "        self.norm = nn.GroupNorm(min(32,ch),ch)\n",
        "        # self.qkv = conv_nd(1, ch, ch * 3, 1)\n",
        "        self.qkv = nn.Conv1d(ch, ch * 3, 1)\n",
        "        self.attention = QKVAttention(self.n_heads)\n",
        "        # self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "        # self.proj_out = zero_module(conv_nd(1, ch, ch, 1))\n",
        "        self.proj_out = nn.Conv1d(ch, ch, 1)\n",
        "        for weight in self.proj_out.parameters(): torch.nn.init.zeros_(weight)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.flatten(2) # [b,c,h*w] ~ [b,d_model,T]\n",
        "        qkv = self.qkv(self.norm(x)) # [b,3c,h*w]\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial) # [b,c,h,w]\n",
        "# norm att lin res\n",
        "\n",
        "# encoderlayer att drop res norm; lin drop res norm\n",
        "\n",
        "\n",
        "# import math\n",
        "# class QKVAttentionLegacy(nn.Module):\n",
        "#     \"\"\"A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\"\"\"\n",
        "#     def __init__(self, n_heads):\n",
        "#         super().__init__()\n",
        "#         self.n_heads = n_heads\n",
        "\n",
        "#     def forward(self, qkv): # [N, H*3*C, T]\n",
        "#         bs, width, length = qkv.shape\n",
        "#         assert width % (3 * self.n_heads) == 0\n",
        "#         ch = width // (3 * self.n_heads)\n",
        "\n",
        "#         q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1) # [N*H, C, T]\n",
        "#         scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "#         weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)  # More stable with f16 than dividing afterwards\n",
        "#         weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "#         a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
        "\n",
        "#         return a.reshape(bs, -1, length) # [N, H*C, T]\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv): # [N, 3*H*C, T]\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=1) # [N, H*C, T]\n",
        "        scale = ch**(-1/4) # scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),)  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length) # [N, H*C, T]\n",
        "\n",
        "    def forward(self, qkv): # [b,3c,h*w]\n",
        "        batch, ccc, hw = qkv.shape\n",
        "        d_head = ccc // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1) # [b,c,h*w]\n",
        "        scale = d_head**(-1/4)\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", (q * scale).view(batch * self.n_heads, d_head, hw),\n",
        "            (k * scale).view(batch * self.n_heads, d_head, hw),)  # More stable with f16 than dividing afterwards\n",
        "        q, k = (q * scale).view(batch * self.n_heads, d_head, hw), (k * scale).view(batch * self.n_heads, d_head, hw)\n",
        "\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(batch * self.n_heads, d_head, hw))\n",
        "        return a.reshape(batch, -1, hw) # [N, H*C, T]\n",
        "\n",
        "        sim = Q @ K.transpose(2, 3) * self.scale\n",
        "        # if mask!=None: sim.masked_fill_(~mask.unsqueeze(1).repeat(1,self.n_heads), -torch.finfo(sim.dtype).max) # -1e10, 3.4028234663852886e+38 [batch, h,w]?-> [batch,n_heads, h*w]?\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = attn @ V # [batch*n_heads, h*w, d_head]\n",
        "        out = out.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"The full UNet model with attention and timestep embedding.\n",
        "    :param model_channels: base channel count for the model.\n",
        "    :param num_res_blocks: number of residual blocks per downsample.\n",
        "    :param attention_resolutions: a collection of downsample rates at which\n",
        "        attention will take place. May be a set, list, or tuple.\n",
        "        For example, if this contains 4, then at 4x downsampling, attention\n",
        "        will be used.\n",
        "    :param channel_mult: channel multiplier for each level of the UNet.\n",
        "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
        "        downsampling.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param num_classes: if specified (as an int), then this model will be\n",
        "        class-conditional with `num_classes` classes.\n",
        "    :param num_heads: the number of attention heads in each attention layer.\n",
        "    :param num_heads_channels: if specified, ignore num_heads and instead use a fixed channel width per attention head.\n",
        "    :param num_heads_upsample: works with num_heads to set a different number\n",
        "                               of heads for upsampling. Deprecated.\n",
        "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
        "    :param resblock_updown: use residual blocks for up/downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_classes=None,\n",
        "        num_heads=-1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_spatial_transformer=False,    # custom transformer support\n",
        "        transformer_depth=1,              # custom transformer support\n",
        "        context_dim=None,                 # custom transformer support\n",
        "        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n",
        "        legacy=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if use_spatial_transformer:\n",
        "            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n",
        "\n",
        "        if context_dim is not None:\n",
        "            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n",
        "            from omegaconf.listconfig import ListConfig\n",
        "            if type(context_dim) == ListConfig:\n",
        "                context_dim = list(context_dim)\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        if num_heads == -1:\n",
        "            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n",
        "\n",
        "        if num_head_channels == -1:\n",
        "            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_classes = num_classes\n",
        "        self.dtype = torch.float16 # float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "        self.predict_codebook_ids = n_embed is not None\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])\n",
        "        self._feature_size = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult): # 1,2,4,8\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    if num_head_channels == -1:\n",
        "                        dim_head = ch // num_heads\n",
        "                    else:\n",
        "                        num_heads = ch // num_head_channels\n",
        "                        dim_head = num_head_channels\n",
        "                    if legacy:\n",
        "                        #num_heads = 1\n",
        "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,)\n",
        "                        if not use_spatial_transformer else SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim)\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1: # last level no res\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, down=True,)\n",
        "                        if resblock_updown\n",
        "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        if num_head_channels == -1:\n",
        "            dim_head = ch // num_heads\n",
        "        else:\n",
        "            num_heads = ch // num_head_channels\n",
        "            dim_head = num_head_channels\n",
        "        if legacy:\n",
        "            #num_heads = 1\n",
        "            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,)\n",
        "            if not use_spatial_transformer else SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    if num_head_channels == -1:\n",
        "                        dim_head = ch // num_heads\n",
        "                    else:\n",
        "                        num_heads = ch // num_head_channels\n",
        "                        dim_head = num_head_channels\n",
        "                    if legacy:\n",
        "                        #num_heads = 1\n",
        "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads_upsample, num_head_channels=dim_head,)\n",
        "                        if not use_spatial_transformer else SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim)\n",
        "                    )\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, up=True,)\n",
        "                        if resblock_updown else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n",
        "        )\n",
        "        if self.predict_codebook_ids:\n",
        "            self.id_predictor = nn.Sequential(\n",
        "            normalization(ch), conv_nd(dims, model_channels, n_embed, 1),\n",
        "            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None, y=None,**kwargs):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "        :param x: an [N x C x ...] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :param context: conditioning plugged in via crossattn\n",
        "        :param y: an [N] Tensor of labels, if class-conditional.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        assert (y is not None) == (self.num_classes is not None), \"must specify y if and only if the model is class-conditional\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            assert y.shape == (x.shape[0],)\n",
        "            emb = emb + self.label_emb(y)\n",
        "\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "        for module in self.output_blocks:\n",
        "            h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "        h = h.type(x.dtype)\n",
        "        if self.predict_codebook_ids:\n",
        "            return self.id_predictor(h)\n",
        "        else:\n",
        "            return self.out(h)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"The full UNet model with attention and timestep embedding.\n",
        "    :param channel_mult: channel multiplier for each level of the UNet.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param num_heads: the number of attention heads in each attention layer.\n",
        "    :param num_heads_channels: if specified, ignore num_heads and instead use a fixed channel width per attention head.\n",
        "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, model_channels=32, out_channels=None, num_res_blocks=1,\n",
        "        # attention_resolutions,\n",
        "        attention_resolutions=(1, 2, 4),#(1, 2, 4, 8),\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4),#(1, 2, 4, 8),\n",
        "        dims=2, num_heads=-1, use_scale_shift_norm=False,):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels # base channel count for the model\n",
        "        out_channels = out_channels or in_channels\n",
        "        self.num_res_blocks = num_res_blocks # number of residual blocks per downsample\n",
        "        self.attention_resolutions = attention_resolutions # collection of downsample rates at which attention will take place. May be a set, list, or tuple. For example, if this contains 4, then at 4x downsampling, attention will be used\n",
        "        transformer_depth=1\n",
        "        context_dim=16\n",
        "\n",
        "        # self.dtype = torch.float16 # float32\n",
        "        # self.n_heads = n_heads\n",
        "        # self.head_dim = d_model // n_heads\n",
        "        # self.num_heads = num_heads\n",
        "        # self.num_head_channels = model_channels // num_heads\n",
        "        self.num_head_channels = num_head_channels = 4\n",
        "        self.num_heads = model_channels // num_head_channels\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])\n",
        "        # input_block_chans = [model_channels]\n",
        "        ch = model_channels # track num channels\n",
        "        # ds = 1 # track down sampling ammount\n",
        "        for level, mult in enumerate(channel_mult): # 1,2,4,8\n",
        "            for _ in range(num_res_blocks):\n",
        "                print(level, mult, \"level, mult; res; ch:\", ch,model_channels * mult)\n",
        "                # print(level, mult, \"level, mult;res;ch\", ch)\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = mult * model_channels\n",
        "                # if ds in attention_resolutions:\n",
        "                # dim_head = ch // num_heads\n",
        "                num_heads = ch // num_head_channels\n",
        "                dim_head = num_head_channels\n",
        "\n",
        "                # print(ds, \"ds ;attn; ch\", ch)\n",
        "                layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,))\n",
        "                # layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                # input_block_chans.append(ch)\n",
        "            # if level != len(channel_mult) - 1: # while not last level; last level no down sample\n",
        "            # out_ch = ch\n",
        "            # self.input_blocks.append(TimestepEmbedSequential(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, updown='down',)))\n",
        "            print(level, \"lv;down;ch\", ch)\n",
        "            # layers.append(Downsample(ch, dims=dims, out_channels=out_ch),)\n",
        "            layers.append(Downsample(ch, dims=dims, out_channels=ch),)\n",
        "            # ch = out_ch\n",
        "            # input_block_chans.append(ch)\n",
        "            # ds *= 2\n",
        "\n",
        "# 3,64,64 conv (res attn down) (res attn down) (res attn)\n",
        "# chn inc after res\n",
        "# (res attn res)\n",
        "\n",
        "        # dim_head = ch // num_heads\n",
        "        num_heads = ch // num_head_channels\n",
        "        dim_head = num_head_channels\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            Downsample(in_ch, dims=dims, out_channels=out_ch),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,),\n",
        "            # SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            Upsample(in_ch, dims=dims, out_channels=in_ch),\n",
        "        )\n",
        "        # print(\"unet init input_block_chans\", input_block_chans) # [64, 64, 64, 128, 128, 256]\n",
        "        print(\"UP\")\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            # for i in range(num_res_blocks + 1):\n",
        "            for i in range(num_res_blocks):\n",
        "                # ich = input_block_chans.pop()\n",
        "\n",
        "                # if level != len(channel_mult) - 1:\n",
        "                # out_ch = ch\n",
        "                # layers.append(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, updown='up',))\n",
        "                print(level, \"lv;up;ch\", ch)\n",
        "                # layers.append(Upsample(ch, dims=dims, out_channels=out_ch),)\n",
        "                layers.append(Upsample(ch, dims=dims, out_channels=ch),)\n",
        "                # ds //= 2\n",
        "\n",
        "                # print(level, mult, \"level, mult; res; ch,ich:\", ch,ich,ch + ich,model_channels * mult)\n",
        "                print(level, mult, \"level, mult; res; in,out:\", model_channels * mult*2,model_channels * mult)\n",
        "                # layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                # layers = [ResBlock(int(model_channels * mult*3/2), time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                layers = [ResBlock(model_channels * mult*2, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = model_channels * mult\n",
        "                # if ds in attention_resolutions:\n",
        "                # dim_head = ch // num_heads\n",
        "                num_heads = ch // num_head_channels\n",
        "                dim_head = num_head_channels\n",
        "                # print(ds, \"ds ;attn; ch\", ch)\n",
        "                layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,))\n",
        "                # layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))\n",
        "                # # if level and i == num_res_blocks:\n",
        "                # # if level != num_res_blocks:\n",
        "                # if level != len(channel_mult) - 1:\n",
        "                #     out_ch = ch\n",
        "                #     # layers.append(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, updown='up',))\n",
        "                #     print(level, \"lv;up;ch\", ch)\n",
        "                #     layers.append(Upsample(ch, dims=dims, out_channels=out_ch),)\n",
        "                #     ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"unet init\",dims, model_channels, out_channels) # 2 64 None\n",
        "        self.out = nn.Sequential(normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),)\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None): # [N, C, ...]\n",
        "        \"\"\":param timesteps: a 1-D batch of timesteps.\n",
        "        :param context: conditioning plugged in via crossattn\"\"\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        # h = x.type(self.dtype)\n",
        "        h = x\n",
        "        for module in self.input_blocks:\n",
        "            # print(\"unet fwd\", module)\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "        for i, module in enumerate(self.output_blocks):\n",
        "            print(\"unet fwd\", h.shape,hs[-1].shape)\n",
        "            # h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = torch.cat([h, hs[-i-1]], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "        # h = h.type(x.dtype)\n",
        "        return self.out(h)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderUNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The half UNet model with attention and timestep embedding.\n",
        "    For usage, see UNet.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        pool=\"adaptive\",\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.dtype = torch.float16 # float16 float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])\n",
        "        self._feature_size = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=num_head_channels,))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(TimestepEmbedSequential(\n",
        "                        ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, down=True,)\n",
        "                        if resblock_updown\n",
        "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    ))\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            AttentionBlock(ch, num_heads=num_heads, num_head_channels=num_head_channels,),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "        self.pool = pool\n",
        "        if pool == \"adaptive\":\n",
        "            self.out = nn.Sequential(\n",
        "                normalization(ch), nn.SiLU(), nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
        "                nn.Flatten(),\n",
        "            )\n",
        "        elif pool == \"attention\":\n",
        "            assert num_head_channels != -1\n",
        "            self.out = nn.Sequential(\n",
        "                normalization(ch), nn.SiLU(),\n",
        "                AttentionPool2d((image_size // ds), ch, num_head_channels, out_channels),\n",
        "            )\n",
        "        elif pool == \"spatial\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048), nn.ReLU(),\n",
        "                nn.Linear(2048, self.out_channels),\n",
        "            )\n",
        "        elif pool == \"spatial_v2\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048), normalization(2048), nn.SiLU(),\n",
        "                nn.Linear(2048, self.out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, timesteps): # [N, C, ...], [N]\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "        results = []\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            if self.pool.startswith(\"spatial\"):\n",
        "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "        h = self.middle_block(h, emb)\n",
        "        if self.pool.startswith(\"spatial\"):\n",
        "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "            h = torch.cat(results, axis=-1)\n",
        "        else: h = h.type(x.dtype)\n",
        "        return self.out(h) # [N, K]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bJsY-p6yyCkr"
      },
      "outputs": [],
      "source": [
        "# @title SpatialTransformer save\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py\n",
        "from inspect import isfunction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def uniq(arr):\n",
        "    return{el: True for el in arr}.keys()\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def max_neg_value(t):\n",
        "    return -torch.finfo(t.dtype).max\n",
        "\n",
        "\n",
        "def init_(tensor):\n",
        "    dim = tensor.shape[-1]\n",
        "    std = 1 / math.sqrt(dim)\n",
        "    tensor.uniform_(-std, std)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# feedforward\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = default(dim_out, dim)\n",
        "        project_in = nn.Sequential(\n",
        "            nn.Linear(dim, inner_dim),\n",
        "            nn.GELU()\n",
        "        ) if not glu else GEGLU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            project_in,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(inner_dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n",
        "        k = k.softmax(dim=-1)\n",
        "        context = torch.einsum('bhdn,bhen->bhde', k, v)\n",
        "        out = torch.einsum('bhde,bhdn->bhen', context, q)\n",
        "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class SpatialSelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = self.norm(x)\n",
        "        q, k, v = self.q(h_), self.k(h_), self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
        "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
        "\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
        "        w_ = rearrange(w_, 'b i j -> b j i')\n",
        "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
        "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
        "        h_ = self.proj_out(h_)\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout),)\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is a self-attention\n",
        "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
        "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is self-attn if context is none\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
        "\n",
        "    def _forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"Transformer block for image-like data.\n",
        "    First, project the input (aka embedding) and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\"\"\"\n",
        "    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.proj_in = nn.Conv2d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n",
        "        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim) for d in range(depth)])\n",
        "        self.proj_out = zero_module(nn.Conv2d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "    def forward(self, x, context=None): # note: if no context is given, cross-attention defaults to self-attention\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context=context)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wVirzVXjfM7",
        "outputId": "edc3d8b3-ac63-4e15-a2a0-0f2f13a3e2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title unet me attn\n",
        "# https://github.com/milesial/Pytorch-UNet/blob/master/unet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, mid_ch=None, bias=False):\n",
        "        super().__init__()\n",
        "        if mid_ch==None: mid_ch = out_ch\n",
        "        act = nn.ReLU(inplace=True) # ReLU GELU SiLU ELU\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(mid_ch), act,\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(out_ch), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_ch, out_ch),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "                nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=1),)\n",
        "        else: self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)#, in_ch // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1) # [c,h,w]\n",
        "        diffX, diffY = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        return self.conv(torch.cat([x2, x1], dim=1))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py\n",
        "        wf=6 # width factor, 2^6 = 64 -> 1024\n",
        "        depth=4\n",
        "        self.inc = DoubleConv(in_ch, 2 ** wf)\n",
        "        self.down_list = nn.ModuleList([Down(2 ** (wf + i), 2 ** (wf + i+1)) for i in range(depth)])\n",
        "        self.up_list = nn.ModuleList([Up(2 ** (wf + i+1), 2 ** (wf + i)) for i in reversed(range(depth))])\n",
        "        self.outc = nn.Conv2d(2 ** wf, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        x = self.inc(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            blocks.append(x)\n",
        "            x = down(x)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "        return self.outc(x)\n",
        "\n",
        "\n",
        "unet = UNet(3, 5)\n",
        "x=torch.rand(4,3,64,64)\n",
        "# x=torch.rand(4,3,128,128)\n",
        "# x=torch.rand(4,3,28,28)\n",
        "out = unet(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYuDUKoWrvjl",
        "outputId": "e44ededd-e08d-4723-c9fb-b2bb2ef8e598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 32, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# @title test res, up, down\n",
        "\n",
        "in_ch, time_embed_dim, out_ch = 32,16,64\n",
        "res = ResBlock(in_ch, time_embed_dim, out_channels=out_ch, dims=2, use_scale_shift_norm=False)\n",
        "x=torch.rand((4,32,16,16))\n",
        "temb=torch.rand((4,16))\n",
        "out = res(x, temb)\n",
        "\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_embed_dim, context_dim, num_heads=None, dim_head=8, dims=2, use_scale_shift_norm=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        if num_heads==None: num_heads = out_ch // dim_head\n",
        "        layers = [\n",
        "            ResBlock(in_ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm),\n",
        "            AttentionBlock(out_ch, num_heads=num_heads, num_head_channels=dim_head,),\n",
        "            SpatialTransformer(out_ch, num_heads, dim_head, depth=1, context_dim=context_dim),\n",
        "            Downsample(out_ch, dims=dims, out_channels=out_ch),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, context=None):\n",
        "        return self.seq(x, temb, context)\n",
        "\n",
        "down=Down(32,32,16, 16)\n",
        "x=torch.rand((4,32,16,16))\n",
        "temb=torch.rand((4,16))\n",
        "cond=torch.rand((4,16))\n",
        "out = down(x, temb,cond)\n",
        "print(out.shape)\n",
        "# print(down)\n",
        "\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_embed_dim, context_dim, num_heads=None, dim_head=8, dims=2, use_scale_shift_norm=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        if num_heads==None: num_heads = out_ch // dim_head\n",
        "        layers = [\n",
        "            Upsample(in_ch, dims=dims, out_channels=in_ch),\n",
        "            ResBlock(in_ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm),\n",
        "            SpatialTransformer(out_ch, num_heads, dim_head, depth=1, context_dim=context_dim),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, context=None):\n",
        "        return self.seq(x, temb, context)\n",
        "\n",
        "up=Up(64,32,16, 16)\n",
        "x=torch.rand((4,64,16,16))\n",
        "temb=torch.rand((4,16))\n",
        "cond=torch.rand((4,16))\n",
        "out = up(x, temb,cond)\n",
        "print(out.shape)\n",
        "# print(down)\n",
        "\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, conv_dim=2, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            Downsample(in_ch, conv_dim=conv_dim, out_ch=in_ch),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch, conv_dim=conv_dim),\n",
        "            # AttentionBlock(out_ch, n_head=n_head, d_head=d_head),\n",
        "            SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, cond=None):\n",
        "        return self.seq(x, temb, cond)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, conv_dim=2):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            # Upsample(in_ch, conv_dim=conv_dim, out_ch=in_ch),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch, conv_dim=conv_dim),\n",
        "            SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            Upsample(out_ch, conv_dim=conv_dim, out_ch=out_ch),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, cond=None):\n",
        "        return self.seq(x, temb, cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pscpq6FqF5M",
        "outputId": "aa518dcd-2320-4d8e-c4fe-c269591ffa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title RoPE\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "        rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "        x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "        rot_x = x * rot_emb\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "\n",
        "dim=16\n",
        "seq_len=512\n",
        "rope = RoPE(dim, seq_len, base=10000)\n",
        "x = torch.rand(4,64,dim)\n",
        "out = rope(x)\n",
        "t = torch.rand(4,1)\n",
        "\n",
        "# [batch] -> []\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "# rot_emb = rope.rot_emb\n",
        "# print(rot_emb.shape)\n",
        "# print(rot_emb[:7])\n",
        "# # rot_emb = rot_emb.reshape(seq_len, dim // 2, 2)\n",
        "# # print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3oBrwquUT1fV"
      },
      "outputs": [],
      "source": [
        "# @title 2D RoPE\n",
        "# https://github.com/naver-ai/rope-vit/blob/main/models/vit_rope.py\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/deit\n",
        "# https://github.com/meta-llama/codellama/blob/main/llama/model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "# from timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from deit.models_v2 import vit_models, Layer_scale_init_Block, Attention\n",
        "\n",
        "def init_random_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
        "    freqs_x = []\n",
        "    freqs_y = []\n",
        "    mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    for i in range(num_heads):\n",
        "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
        "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
        "        fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
        "        freqs_x.append(fx)\n",
        "        freqs_y.append(fy)\n",
        "    freqs_x = torch.stack(freqs_x, dim=0)\n",
        "    freqs_y = torch.stack(freqs_y, dim=0)\n",
        "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
        "    return freqs\n",
        "\n",
        "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
        "    N = t_x.shape[0]\n",
        "    depth = freqs.shape[1]\n",
        "    # No float 16 for this range\n",
        "    with torch.cuda.amp.autocast(enabled=False):\n",
        "        freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 100.0):\n",
        "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "\n",
        "    t_x, t_y = init_t_xy(end_x, end_y)\n",
        "    freqs_x = torch.outer(t_x, freqs_x)\n",
        "    freqs_y = torch.outer(t_y, freqs_y)\n",
        "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
        "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
        "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
        "\n",
        "def init_t_xy(end_x: int, end_y: int):\n",
        "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
        "    t_x = (t % end_x).float()\n",
        "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
        "    return t_x, t_y\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
        "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
        "\n",
        "\n",
        "class RoPEAttention(Attention):\n",
        "    \"\"\"Multi-head Attention block with rotary position embeddings.\"\"\"\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q[:, :, 1:], k[:, :, 1:] = apply_rotary_emb(q[:, :, 1:], k[:, :, 1:], freqs_cis=freqs_cis)\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class RoPE_Layer_scale_init_Block(Layer_scale_init_Block):\n",
        "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "    # with slight modifications\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs[\"Attention_block\"] = RoPEAttention\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), freqs_cis=freqs_cis))\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class rope_vit_models(vit_models):\n",
        "    def __init__(self, rope_theta=100.0, rope_mixed=False, use_ape=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        img_size = kwargs['img_size'] if 'img_size' in kwargs else 224\n",
        "        patch_size = kwargs['patch_size'] if 'patch_size' in kwargs else 16\n",
        "        num_heads = kwargs['num_heads'] if 'num_heads' in kwargs else 12\n",
        "        embed_dim = kwargs['embed_dim'] if 'embed_dim' in kwargs else 768\n",
        "        mlp_ratio = kwargs['mlp_ratio'] if 'mlp_ratio' in kwargs else 4.\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        self.use_ape = use_ape\n",
        "        if not self.use_ape:\n",
        "            self.pos_embed = None\n",
        "\n",
        "        self.rope_mixed = rope_mixed\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            self.compute_cis = partial(compute_mixed_cis, num_heads=self.num_heads)\n",
        "\n",
        "            freqs = []\n",
        "            for i, _ in enumerate(self.blocks):\n",
        "                freqs.append(\n",
        "                    init_random_2d_freqs(dim=embed_dim // num_heads, num_heads=num_heads, theta=rope_theta)\n",
        "                )\n",
        "            freqs = torch.stack(freqs, dim=1).view(2, len(self.blocks), -1)\n",
        "            self.freqs = nn.Parameter(freqs.clone(), requires_grad=True)\n",
        "\n",
        "            t_x, t_y = init_t_xy(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.register_buffer('freqs_t_x', t_x)\n",
        "            self.register_buffer('freqs_t_y', t_y)\n",
        "        else:\n",
        "            self.compute_cis = partial(compute_axial_cis, dim=embed_dim//num_heads, theta=rope_theta)\n",
        "\n",
        "            freqs_cis = self.compute_cis(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.freqs_cis = freqs_cis\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'freqs'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.use_ape:\n",
        "            pos_embed = self.pos_embed\n",
        "            if pos_embed.shape[-2] != x.shape[-2]:\n",
        "                img_size = self.patch_embed.img_size\n",
        "                patch_size = self.patch_embed.patch_size\n",
        "                pos_embed = pos_embed.view(\n",
        "                    1, (img_size[1] // patch_size[1]), (img_size[0] // patch_size[0]), self.embed_dim\n",
        "                ).permute(0, 3, 1, 2)\n",
        "                pos_embed = F.interpolate(\n",
        "                    pos_embed, size=(H // patch_size[1], W // patch_size[0]), mode='bicubic', align_corners=False\n",
        "                )\n",
        "                pos_embed = pos_embed.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            x = x + pos_embed\n",
        "\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            if self.freqs_t_x.shape[0] != x.shape[1] - 1:\n",
        "                t_x, t_y = init_t_xy(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "                t_x, t_y = t_x.to(x.device), t_y.to(x.device)\n",
        "            else:\n",
        "                t_x, t_y = self.freqs_t_x, self.freqs_t_y\n",
        "            freqs_cis = self.compute_cis(self.freqs, t_x, t_y)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis[i])\n",
        "        else:\n",
        "            if self.freqs_cis.shape[0] != x.shape[1] - 1:\n",
        "                freqs_cis = self.compute_cis(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "            else:\n",
        "                freqs_cis = self.freqs_cis\n",
        "            freqs_cis = freqs_cis.to(x.device)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def adjust_pos_embed_size(model, state_dict):\n",
        "    # interpolate position embedding\n",
        "    if 'pos_embed' in state_dict:\n",
        "        pos_embed_checkpoint = state_dict['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "        # only the position tokens are interpolated\n",
        "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "        pos_tokens = torch.nn.functional.interpolate(\n",
        "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "        state_dict['pos_embed'] = new_pos_embed\n",
        "\n",
        "    return state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LHq-jLNIXQ76"
      },
      "outputs": [],
      "source": [
        "# @title CircularEmb\n",
        "\n",
        "class CircularEmb(nn.Module): # Circular Positional Embeddings\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        # self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        mult = torch.tensor([1,2,4,8,16,32,64,128,256,512,1024,2048], device=device)\n",
        "        # mult = torch.tensor([1,2,3,5,7,11,13,17,19,23,29,31,37,41,43,47], device=device)\n",
        "        self.theta = 2 * torch.pi * mult[:dim//2]\n",
        "\n",
        "    def forward(self, pos): # [batch,T] in [0,1]\n",
        "        # angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        # angles = (pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2)\n",
        "\n",
        "# ciremb = CircularEmb(dim=8)\n",
        "# # pos = torch.linspace(0,1,seq_len).to(device).repeat(4,5,1)\n",
        "# pos = torch.linspace(0,0.3,seq_len).to(device).repeat(4,5,1)/24\n",
        "# out = ciremb(pos)\n",
        "# print(out.shape)\n",
        "# print(out[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wdRZxhI7-Xuh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock down\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\"\"\"\n",
        "    def __init__(self, in_ch, n_head=1, d_head=8):\n",
        "        super().__init__()\n",
        "        # self.n_head = n_head\n",
        "        self.n_head = in_ch // num_head_in_ch\n",
        "        self.norm = normalization(in_ch)\n",
        "        self.qkv = conv_nd(1, in_ch, in_ch * 3, 1)\n",
        "        # self.qkv = Conv1D(in_ch, in_ch * 3, 1)\n",
        "        self.attention = QKVAttention(self.n_head)\n",
        "        # self.proj_out = zero_module(conv_nd(1, in_ch, in_ch, 1))\n",
        "        self.proj_out = conv_nd(1, in_ch, in_ch, 1)\n",
        "        nn.init.zeros_(self.proj_out.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv): # [N, 3*H*C, T]\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "\n",
        "\n",
        "        # from einops import rearrange\n",
        "        # from einops.layers.torch import Rearrange\n",
        "        # q = rearrange(q, 'b n (h d) -> b h n d', h = self.n_heads)\n",
        "        # q = rearrange(q, 'n (3 h c) t -> 3 n (h c) t', h = self.n_heads, c = ch)\n",
        "\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=1) # [N, H*C, T]\n",
        "        scale = ch**(-1/4)\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),)  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight, dim=-1)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "\n",
        "        return a.reshape(bs, -1, length) # [N, H*C, T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K821B-KoHISD"
      },
      "outputs": [],
      "source": [
        "# @title test inspect.signature\n",
        "# model = Downsample(8)\n",
        "# model = ResBlock(32,8)\n",
        "# model = SpatialTransformer(32,32,32)\n",
        "\n",
        "import inspect\n",
        "# # class Seq(nn.Sequential, TimestepBlock):\n",
        "class Seq(nn.Sequential):\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            args = [x]\n",
        "            if 'emb' in params: args.append(emb)\n",
        "            if 'cond' in params: args.append(cond)\n",
        "            x = layer(*args)\n",
        "            # print('x' in sig.parameters.keys())\n",
        "            # if isinstance(layer, TimestepBlock): x = layer(x, emb)\n",
        "            # elif isinstance(layer, SpatialTransformer): x = layer(x, cond)\n",
        "            # else: x = layer(x)\n",
        "            # pass\n",
        "        return x\n",
        "# sig = inspect.signature(model.forward)\n",
        "# print(sig)\n",
        "# print(sig.parameters.items())\n",
        "# print(sig.parameters.keys())\n",
        "# print('x' in sig.parameters.keys())\n",
        "# # print(type(sig))\n",
        "\n",
        "model = Seq(\n",
        "    Downsample(8),\n",
        "    ResBlock(32,8),\n",
        "    SpatialTransformer(32,32,32),\n",
        ")\n",
        "x = torch.rand(4)\n",
        "emb = torch.rand(4)\n",
        "cond = torch.rand(4)\n",
        "out = model(x, emb,cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIxTx8tT-nKH",
        "outputId": "ef51c676-a1c1-46bd-8d65-a114a9029756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48.8 ns ± 0.365 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
          ]
        }
      ],
      "source": [
        "# model = Downsample(8)\n",
        "model = ResBlock(32,8)\n",
        "# model = SpatialTransformer(32,32,32)\n",
        "# %timeit inspect.signature(model.forward).parameters.keys()\n",
        "# %timeit isinstance(model, TimestepBlock)\n",
        "%timeit model.emb\n",
        "# 14.4 µs 14.4 µs 16.7 µs; 80.5 ns 79.9 ns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUyK5FbdKFFV",
        "outputId": "8d81ff71-5511-4c81-a14c-9ae596c471f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def fun(a,b):\n",
            "    return a*b \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def fun(a,b):\n",
        "    return a*b\n",
        "print(inspect.getsource(fun))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uPcm3g4mpD2h"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}