{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/latent-flow-model/blob/main/flow_matching_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVWgSlV22V7M",
        "outputId": "f243a93c-75c4-423f-8442-f433e7d14342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title SpatialTransformer down\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0.):\n",
        "        super().__init__()\n",
        "        d_model = d_head * n_heads\n",
        "        self.n_heads, self.d_head, self.d_model = n_heads, d_head, d_model\n",
        "        if cond_dim == None: cond_dim = query_dim\n",
        "        self.scale = d_head ** -0.5\n",
        "        self.n_heads = n_heads\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.k = nn.Linear(cond_dim, d_model, bias=False)\n",
        "        self.v = nn.Linear(cond_dim, d_model, bias=False)\n",
        "        self.lin = nn.Sequential(nn.Linear(d_model, query_dim), nn.Dropout(drop),)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, h*w, n_heads*d_head]\n",
        "        # print(\"crossattn fwd x\", x.shape)\n",
        "        h = self.n_heads\n",
        "        if cond==None: cond=x\n",
        "        Q, K, V = self.q(x), self.k(cond), self.v(cond)\n",
        "        Q, K, V = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (Q, K, V)) # [batch*n_heads, h*w, d_head]\n",
        "        sim = Q @ K.transpose(1, 2) * self.scale\n",
        "        if mask!=None: sim.masked_fill_(~mask.flatten(1).repeat(h,1).unsqueeze(1), -torch.finfo(sim.dtype).max) # -1e10, 3.4028234663852886e+38\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = attn @ V # [batch*n_heads, h*w, d_head]\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.lin(out) # [batch, h*w, n_heads*d_head]\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, h*w, n_heads*d_head]\n",
        "        h = self.n_heads\n",
        "        batch_size = x.shape[0]\n",
        "        if cond==None: cond=x\n",
        "        Q = self.q(x).reshape(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, n_heads, h*w, d_head]\n",
        "        K = self.k(cond).reshape(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.v(cond).reshape(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        sim = Q @ K.transpose(2, 3) * self.scale\n",
        "        # if mask!=None: sim.masked_fill_(~mask.unsqueeze(1).repeat(1,self.n_heads), -torch.finfo(sim.dtype).max) # -1e10, 3.4028234663852886e+38 [batch, h,w]?-> [batch,n_heads, h*w]?\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = attn @ V # [batch*n_heads, h*w, d_head]\n",
        "        out = out.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        return self.lin(out) # [batch, h*w, n_heads*d_head]\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_head, drop=0., cond_dim=None):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=d_model, n_heads=n_heads, d_head=d_head, drop=drop)  # is a self-attention\n",
        "        self.attn2 = CrossAttention(query_dim=d_model, cond_dim=cond_dim, n_heads=n_heads, d_head=d_head, drop=drop)  # is self-attn if cond is none\n",
        "        self.norm1, self.norm2, self.norm3 = nn.RMSNorm(d_model), nn.RMSNorm(d_model), nn.RMSNorm(d_model) # RMSNorm LayerNorm\n",
        "        inner_dim = d_model * 4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, inner_dim), nn.GELU(), # nn.GELU() GEGLU(d_model, inner_dim)\n",
        "            nn.Dropout(drop), nn.Linear(inner_dim, d_model),)\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), cond=cond) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"Transformer block for image-like data.\n",
        "    First, project the input (aka embedding) and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\"\"\"\n",
        "    def __init__(self, in_ch, n_heads, d_head, depth=1, drop=0., cond_dim=None):\n",
        "        super().__init__()\n",
        "        self.cond=True\n",
        "        self.in_ch = in_ch\n",
        "        d_model = n_heads * d_head\n",
        "        self.norm = nn.GroupNorm(num_groups=min(32,in_ch), num_channels=in_ch)\n",
        "        self.proj_in = nn.Conv2d(in_ch, d_model, kernel_size=1, stride=1, padding=0)\n",
        "        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(d_model, n_heads, d_head, drop=drop, cond_dim=cond_dim) for d in range(depth)])\n",
        "        self.proj_out = zero_module(nn.Conv2d(d_model, in_ch, kernel_size=1, stride=1, padding=0))\n",
        "        # self.proj_out = nn.Conv2d(d_model, in_ch, kernel_size=1, stride=1, padding=0)\n",
        "        # nn.init.zeros_(self.proj_out.weight)\n",
        "\n",
        "    def forward(self, x, cond=None): # note: if no cond is given, cross-attention defaults to self-attention\n",
        "        b, c, h, w = x.shape\n",
        "        if cond!=None: cond = cond.unsqueeze(1).repeat(1,self.in_ch,1)\n",
        "        x_in = x\n",
        "        x = self.proj_in(self.norm(x))\n",
        "        # x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, cond=cond)\n",
        "        # x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "        x = x.transpose(1, 2).reshape(b, c, h, w)\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "batch=4\n",
        "in_ch=32 # assert in_ch % 32 == 0 for groupnorm32\n",
        "cond_dim=16\n",
        "h=w=16\n",
        "n_heads, d_head = 2, 8\n",
        "# sptf = SpatialTransformer(in_ch=32, n_heads=8, d_head=4, depth=1, drop=0., cond_dim=16)\n",
        "sptf = SpatialTransformer(in_ch, n_heads, d_head, depth=1, cond_dim=cond_dim)\n",
        "x = torch.rand(batch,in_ch,h,w)\n",
        "# cond = torch.rand(4,64,16)\n",
        "cond = torch.rand(4,16)\n",
        "# cond = torch.rand(batch,cond_dim).unsqueeze(1).repeat(1,in_ch,1)\n",
        "out = sptf(x, cond)\n",
        "print(out.shape) # batch,in_ch,h,w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V_kbZc8imad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f5f42d-8461-4b97-83a5-03c09225fc24",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
            "        1.0000])\n",
            "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [9.9383e-01, 9.9994e-01, 1.0000e+00, 1.0000e+00, 1.1088e-01, 1.1111e-02,\n",
            "         1.1111e-03, 1.1111e-04],\n",
            "        [9.7541e-01, 9.9975e-01, 1.0000e+00, 1.0000e+00, 2.2040e-01, 2.2220e-02,\n",
            "         2.2222e-03, 2.2222e-04],\n",
            "        [9.4496e-01, 9.9944e-01, 9.9999e-01, 1.0000e+00, 3.2719e-01, 3.3327e-02,\n",
            "         3.3333e-03, 3.3333e-04],\n",
            "        [9.0285e-01, 9.9901e-01, 9.9999e-01, 1.0000e+00, 4.2996e-01, 4.4430e-02,\n",
            "         4.4444e-03, 4.4444e-04],\n",
            "        [8.4961e-01, 9.9846e-01, 9.9998e-01, 1.0000e+00, 5.2742e-01, 5.5527e-02,\n",
            "         5.5555e-03, 5.5556e-04],\n",
            "        [7.8589e-01, 9.9778e-01, 9.9998e-01, 1.0000e+00, 6.1837e-01, 6.6617e-02,\n",
            "         6.6666e-03, 6.6667e-04],\n",
            "        [7.1247e-01, 9.9698e-01, 9.9997e-01, 1.0000e+00, 7.0170e-01, 7.7699e-02,\n",
            "         7.7777e-03, 7.7778e-04],\n",
            "        [6.3028e-01, 9.9605e-01, 9.9996e-01, 1.0000e+00, 7.7637e-01, 8.8772e-02,\n",
            "         8.8888e-03, 8.8889e-04],\n",
            "        [5.4030e-01, 9.9500e-01, 9.9995e-01, 1.0000e+00, 8.4147e-01, 9.9833e-02,\n",
            "         9.9998e-03, 1.0000e-03]])\n"
          ]
        }
      ],
      "source": [
        "# @title stable diffusion util\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/util.py\n",
        "# adopted from\n",
        "# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n",
        "# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n",
        "# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import repeat\n",
        "\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        # freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=device)\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half) / half)#.to(device)\n",
        "        # args = timesteps[:, None].float() * freqs[None]\n",
        "        args = timesteps[:, None] * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2: embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    else: embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
        "    return embedding\n",
        "\n",
        "# # timesteps = torch.arange(0, 10)\n",
        "# timesteps = torch.linspace(0, 1, 10)\n",
        "# print(timesteps)\n",
        "# model_channels=8\n",
        "# t_emb = timestep_embedding(timesteps, model_channels, repeat_only=False)\n",
        "# print(t_emb)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "def conv_nd(dims, *args, **kwargs):\n",
        "    \"\"\"Create a 1D, 2D, or 3D convolution module.\"\"\"\n",
        "    if dims == 1: return nn.Conv1d(*args, **kwargs)\n",
        "    elif dims == 2: return nn.Conv2d(*args, **kwargs)\n",
        "    elif dims == 3: return nn.Conv3d(*args, **kwargs)\n",
        "\n",
        "\n",
        "def avg_pool_nd(dims, *args, **kwargs):\n",
        "    \"\"\"Create a 1D, 2D, or 3D average pooling module.\"\"\"\n",
        "    if dims == 1: return nn.AvgPool1d(*args, **kwargs)\n",
        "    elif dims == 2: return nn.AvgPool2d(*args, **kwargs)\n",
        "    elif dims == 3: return nn.AvgPool3d(*args, **kwargs)\n",
        "\n",
        "# conv_nd,\n",
        "# avg_pool_nd,\n",
        "# zero_module,\n",
        "# normalization, # nn.GroupNorm\n",
        "# timestep_embedding,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j9wUA7Vk0Y03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a8ddc2-fd1c-4726-ef92-3fb4e92008f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "        [ 6.4279e-01,  7.6604e-01,  1.1042e-01,  9.9388e-01,  1.7535e-02,\n",
            "          9.9985e-01,  2.7793e-03,  1.0000e+00,  4.4049e-04,  1.0000e+00],\n",
            "        [ 9.8481e-01,  1.7365e-01,  2.1949e-01,  9.7561e-01,  3.5065e-02,\n",
            "          9.9938e-01,  5.5586e-03,  9.9998e-01,  8.8098e-04,  1.0000e+00],\n",
            "        [ 8.6603e-01, -5.0000e-01,  3.2588e-01,  9.4541e-01,  5.2585e-02,\n",
            "          9.9862e-01,  8.3378e-03,  9.9997e-01,  1.3215e-03,  1.0000e+00],\n",
            "        [ 3.4202e-01, -9.3969e-01,  4.2828e-01,  9.0365e-01,  7.0088e-02,\n",
            "          9.9754e-01,  1.1117e-02,  9.9994e-01,  1.7620e-03,  1.0000e+00],\n",
            "        [-3.4202e-01, -9.3969e-01,  5.2544e-01,  8.5083e-01,  8.7569e-02,\n",
            "          9.9616e-01,  1.3896e-02,  9.9990e-01,  2.2025e-03,  1.0000e+00],\n",
            "        [-8.6603e-01, -5.0000e-01,  6.1618e-01,  7.8761e-01,  1.0502e-01,\n",
            "          9.9447e-01,  1.6675e-02,  9.9986e-01,  2.6429e-03,  1.0000e+00],\n",
            "        [-9.8481e-01,  1.7365e-01,  6.9938e-01,  7.1475e-01,  1.2245e-01,\n",
            "          9.9248e-01,  1.9454e-02,  9.9981e-01,  3.0834e-03,  1.0000e+00],\n",
            "        [-6.4279e-01,  7.6604e-01,  7.7402e-01,  6.3316e-01,  1.3983e-01,\n",
            "          9.9018e-01,  2.2233e-02,  9.9975e-01,  3.5239e-03,  9.9999e-01],\n",
            "        [ 1.7485e-07,  1.0000e+00,  8.3920e-01,  5.4382e-01,  1.5717e-01,\n",
            "          9.8757e-01,  2.5011e-02,  9.9969e-01,  3.9644e-03,  9.9999e-01]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2)\n",
        "\n",
        "rotemb = RotEmb(10)\n",
        "seq_len=10\n",
        "pos = torch.linspace(0,1,seq_len).to(device)#.unsqueeze(-1)\n",
        "rot_emb = rotemb(pos)\n",
        "print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8YwHsHFURq0",
        "outputId": "b4776b09-9c2c-40f9-d54d-2527c6c5c200",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[0.0010, 0.0010, 0.0010],\n",
            "          [0.0010, 0.0010, 0.0010],\n",
            "          [0.0010, 0.0010, 0.0010]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0010], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# @title test init zero\n",
        "\n",
        "class Me(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # self.out = nn.Sequential(nn.SiLU(), conv_nd(2, in_ch=1, out_ch=1, stride=3, padding=1))\n",
        "        self.out = nn.Sequential(nn.GroupNorm(1,1),nn.SiLU(), conv_nd(2, 1, 1, 3, padding=1))\n",
        "        # self.in_ch = in_ch\n",
        "        for weight in self.out.parameters(): torch.nn.init.zeros_(weight)\n",
        "    def forward(self, x):\n",
        "        return self.out(x)\n",
        "\n",
        "me = Me()\n",
        "optimizer = torch.optim.AdamW(me.parameters(), lr=1e-3)\n",
        "\n",
        "for weight in me.parameters(): print(weight)\n",
        "x = torch.rand(1,16,16)\n",
        "y = torch.rand(1,16,16)\n",
        "with torch.amp.autocast('cuda'):\n",
        "    loss = F.mse_loss(me(x), y)\n",
        "# print(loss)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "# scaler.scale(loss).backward()\n",
        "# # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "# scaler.step(optim)\n",
        "# scaler.update()\n",
        "for weight in me.parameters(): print(weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMFVOgT4sJ-y",
        "outputId": "0ec3418e-6cfe-48f7-fe41-c4f7076cdf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15437585\n",
            "torch.Size([4, 1, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title stable diffusion unet down\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, conv_dim=2, padding=1):\n",
        "        super().__init__()\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.conv_dim = conv_dim\n",
        "        self.conv = conv_nd(conv_dim, in_ch, out_ch, 3, padding=padding)\n",
        "\n",
        "    def forward(self, x): # [N,C,...]\n",
        "        if self.conv_dim == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        else: x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        x = self.conv(x) # optional\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, conv_dim=2):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        stride = 2 if conv_dim != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        self.op = conv_nd(conv_dim, in_ch, out_ch, 3, stride=stride, padding=1) # optional\n",
        "        # self.op = avg_pool_nd(conv_dim, kernel_size=stride, stride=stride) # alternative\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial]\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, temb_dim, out_ch=None, scale_shift=False, conv_dim=2, updown=False, drop=0.):\n",
        "        super().__init__()\n",
        "        self.temb_dim = temb_dim # number of timestep embedding channels\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch = in_ch, out_ch\n",
        "        self.scale_shift = scale_shift\n",
        "        if updown=='up': self.h_upd, self.x_upd = Upsample(in_ch, conv_dim=conv_dim), Upsample(in_ch, conv_dim=conv_dim)\n",
        "        elif updown=='down': self.h_upd, self.x_upd = Downsample(in_ch, conv_dim=conv_dim), Downsample(in_ch, conv_dim=conv_dim)\n",
        "        else: self.h_upd = self.x_upd = nn.Identity()\n",
        "        self.in_layers = nn.Sequential(nn.GroupNorm(min(32,in_ch), in_ch), nn.SiLU(), self.h_upd, conv_nd(conv_dim, in_ch, out_ch, 3, padding=1),)\n",
        "\n",
        "        self.emb_layers = nn.Sequential(nn.SiLU(), nn.Linear(temb_dim, 2 * out_ch if scale_shift else out_ch),)\n",
        "        self.out_layers = nn.Sequential(\n",
        "            nn.GroupNorm(min(32,out_ch), out_ch), nn.SiLU(), nn.Dropout(drop),\n",
        "            zero_module(conv_nd(conv_dim, out_ch, out_ch, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if out_ch == in_ch: self.skip = nn.Identity() # no need to change chanels\n",
        "        else:\n",
        "            # self.skip = conv_nd(conv_dim, in_ch, out_ch, 3, padding=1) # spatial convolution to change the channels in the skip connection\n",
        "            self.skip = conv_nd(conv_dim, in_ch, out_ch, 1) # smaller 1x1 convolution to change the channels in the skip connection\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, temb_dim]\n",
        "        # print(\"res fwd x\", x.shape, self.in_ch, self.out_ch)\n",
        "        h = self.in_layers(x) # norm, act, h_upd, conv\n",
        "        x = self.x_upd(x)\n",
        "        emb_out = self.emb_layers(emb) # act, lin\n",
        "        # print(\"res fwd h emb_out\", h.shape, emb_out.shape)\n",
        "        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]\n",
        "        if self.scale_shift: # FiLM\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return h + self.skip(x) # [N, C, ...]\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, conv_dim=2, updown=False, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            Downsample(in_ch, conv_dim=conv_dim) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch, conv_dim=conv_dim),\n",
        "            SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            Upsample(out_ch, conv_dim=conv_dim) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, model_ch=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, conv_dim=2, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.model_ch = model_ch # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = model_ch // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(model_ch)\n",
        "        temb_dim = model_ch# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(model_ch, temb_dim), nn.SiLU(), nn.Linear(temb_dim, temb_dim))\n",
        "\n",
        "        self.input_blocks = Seq(conv_nd(conv_dim, in_ch, model_ch, 3, padding=1))\n",
        "\n",
        "        ch_list = [model_ch*2**i for i in range(depth+1)] # [32, 64, 128, 256]\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "\n",
        "        ch = 2*ch_list[-1] # 512\n",
        "        self.middle_block = Seq(\n",
        "            Downsample(ch_list[-1], conv_dim=conv_dim),\n",
        "            ResBlock(ch_list[-1], temb_dim, ch, conv_dim=conv_dim),\n",
        "            SpatialTransformer(ch, ch//d_head, d_head, cond_dim=cond_dim),\n",
        "            ResBlock(ch, temb_dim, ch_list[-1], conv_dim=conv_dim),\n",
        "            Upsample(ch_list[-1], conv_dim=conv_dim),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "\n",
        "        self.out = nn.Sequential(nn.GroupNorm(min(32,model_ch), model_ch), nn.SiLU(), zero_module(conv_nd(conv_dim, model_ch, out_ch, 3, padding=1)))\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, C, ...]\n",
        "        # t_emb = self.rotemb(t)\n",
        "        t_emb = timestep_embedding(t, self.model_ch, repeat_only=False)\n",
        "\n",
        "        emb = self.time_emb(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, temb_dim)\n",
        "        blocks = []\n",
        "        x = self.input_blocks(x, emb, cond)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, model_ch=16, cond_dim=cond_dim).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "# x=torch.rand((batch,3,16,16),device=device)\n",
        "x=torch.rand((batch,1,16,16),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# print(t)\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iJ1hbnSC12tA"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, d_list=[32, 64, 128, 256], act=nn.GELU(), drop=0.2): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.Dropout2d(drop), nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), act,\n",
        "            # nn.Dropout2d(drop), nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), act,\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x) # [batch, 3,64,64] -> [batch, c,h,w]\n",
        "\n",
        "class Deconv(nn.Module):\n",
        "    def __init__(self, d_list=[32, 64, 128, 256], act=nn.GELU()): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            # nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), act,\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.deconv(x) # [batch, c,h,w] -> [batch, 3,64,64]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "enc = conv(input)\n",
        "print(enc.shape)\n",
        "\n",
        "deconv = Deconv().to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "out = deconv(enc)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvBRca98P-GW"
      },
      "outputs": [],
      "source": [
        "# @title latent flow model\n",
        "class LFM(nn.Module): # latent flow model\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        d_list=[16, 32]\n",
        "        self.conv = Conv(d_list)\n",
        "        self.deconv = Deconv(d_list)\n",
        "        self.unet = UNet(in_ch=d_list[-1], model_ch=32, cond_dim=cond_dim, depth=3)\n",
        "\n",
        "    def loss(self, img, cond):\n",
        "        x1 = self.conv(img)\n",
        "        img_ = self.deconv(x1)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        fm_loss = otfm_loss(self.unet, x1, cond)\n",
        "        loss = ae_loss + fm_loss\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # def forward(self, x, t=None, cond=None): # [N, C, ...]\n",
        "\n",
        "\n",
        "    def sample(self, cond):\n",
        "        self.unet.eval()\n",
        "        f = lambda t, y: -self.unet(y, t.repeat(n_samples), cond.repeat(n_samples,1))\n",
        "        sampled_data = sample_reverse_time(f, num_samples=n_samples, timesteps=25, device=device)\n",
        "        return sampled_data\n",
        "\n",
        "\n",
        "# cond = cond_emb(torch.tensor([4], device=device))\n",
        "cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).to(torch.float)\n",
        "\n",
        "n_samples = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LFM().to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "batch_size = 128 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvgOmsCPaCk"
      },
      "outputs": [],
      "source": [
        "optim.param_groups[0][\"lr\"] = 1e-0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LogitNormal\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "class LogitNormal(dist.Distribution):\n",
        "    def __init__(self, mu=0, std=.5):\n",
        "        super().__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "        self._normal = dist.Normal(mu, std) # https://pytorch.org/docs/stable/distributions.html#normal\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        eps = self._normal.rsample(sample_shape)\n",
        "        return torch.sigmoid(eps) # https://en.wikipedia.org/wiki/Logit-normal_distribution\n",
        "\n",
        "logit_normal = LogitNormal()\n",
        "# samples = logit_normal.rsample((10,))\n",
        "# print(samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bf2bipgghY7O",
        "outputId": "0d6b2436-f3ab-4f19-ac67-7590a403cc4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py:56: UserWarning: <class '__main__.LogitNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfsabyM8P7q3",
        "outputId": "6eb52cff-f27f-42b2-dc46-6def3ee091bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-101-d21145c826ca>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.12641359865665436\n",
            "10 0.12543639540672302\n",
            "20 0.12510591745376587\n",
            "30 0.12913595139980316\n",
            "40 0.1209096908569336\n",
            "50 0.11881167441606522\n",
            "60 0.12068746238946915\n",
            "70 0.1179983913898468\n",
            "80 0.1248253807425499\n",
            "90 0.1247519701719284\n",
            "100 0.12128500640392303\n",
            "110 0.11929149180650711\n",
            "120 0.12015996873378754\n",
            "130 0.12169792503118515\n",
            "140 0.12142522633075714\n",
            "150 0.12353295087814331\n",
            "160 0.11631253361701965\n",
            "170 0.12235555797815323\n",
            "180 0.12026675045490265\n",
            "190 0.1169145256280899\n",
            "200 0.11783258616924286\n",
            "210 0.12045284360647202\n",
            "220 0.11468130350112915\n",
            "230 0.1232576072216034\n",
            "240 0.12157493829727173\n",
            "250 0.12999993562698364\n",
            "260 0.12112435698509216\n",
            "270 0.12153589725494385\n",
            "280 0.13087862730026245\n",
            "290 0.12363177537918091\n",
            "300 0.13031670451164246\n",
            "310 0.12800610065460205\n",
            "320 0.12402111291885376\n",
            "330 0.1256817728281021\n",
            "340 0.12508147954940796\n",
            "350 0.1221582368016243\n",
            "360 0.12295365333557129\n",
            "370 0.12894558906555176\n",
            "380 0.12507425248622894\n",
            "390 0.11661510914564133\n",
            "400 0.12189806997776031\n",
            "410 0.12819689512252808\n",
            "420 0.11631546914577484\n",
            "430 0.1255527138710022\n",
            "440 0.12404675781726837\n",
            "450 0.1291566789150238\n",
            "460 0.12026873230934143\n",
            "0 0.124703049659729\n",
            "10 0.12243454903364182\n",
            "20 0.13132476806640625\n",
            "30 0.12457214295864105\n",
            "40 0.11763488501310349\n",
            "50 0.11954906582832336\n",
            "60 0.11651772260665894\n",
            "70 0.1316172033548355\n",
            "80 0.12579993903636932\n",
            "90 0.1211114227771759\n",
            "100 0.11855703592300415\n",
            "110 0.1157071515917778\n",
            "120 0.12088172137737274\n",
            "130 0.1141994297504425\n",
            "140 0.13237278163433075\n",
            "150 0.12113398313522339\n",
            "160 0.11552517861127853\n",
            "170 0.12487197667360306\n",
            "180 0.1227506622672081\n",
            "190 0.12154924869537354\n",
            "200 0.12602967023849487\n",
            "210 0.12273193150758743\n",
            "220 0.1211383119225502\n",
            "230 0.12188301980495453\n",
            "240 0.12158210575580597\n",
            "250 0.1257210373878479\n",
            "260 0.12638896703720093\n",
            "270 0.12716646492481232\n",
            "280 0.13260206580162048\n",
            "290 0.1179574653506279\n",
            "300 0.11964986473321915\n",
            "310 0.12577703595161438\n",
            "320 0.12197667360305786\n",
            "330 0.12033703923225403\n",
            "340 0.12350930273532867\n",
            "350 0.13019898533821106\n",
            "360 0.1233295276761055\n",
            "370 0.11957630515098572\n",
            "380 0.12049131095409393\n",
            "390 0.12516050040721893\n",
            "400 0.13009490072727203\n",
            "410 0.11405747383832932\n",
            "420 0.1287018358707428\n",
            "430 0.11872485280036926\n",
            "440 0.12482030689716339\n",
            "450 0.12520498037338257\n",
            "460 0.12224306911230087\n",
            "0 0.12582793831825256\n",
            "10 0.13166871666908264\n",
            "20 0.12578879296779633\n",
            "30 0.12519899010658264\n",
            "40 0.12000128626823425\n",
            "50 0.12004151940345764\n",
            "60 0.1276153326034546\n",
            "70 0.13143333792686462\n",
            "80 0.12862345576286316\n",
            "90 0.11944179236888885\n",
            "100 0.12417472898960114\n",
            "110 0.12845085561275482\n",
            "120 0.12546449899673462\n",
            "130 0.12264503538608551\n",
            "140 0.11958152800798416\n",
            "150 0.12448364496231079\n",
            "160 0.12787331640720367\n",
            "170 0.11749766767024994\n",
            "180 0.12312482297420502\n",
            "190 0.12354124337434769\n",
            "200 0.11295978724956512\n",
            "210 0.13349603116512299\n",
            "220 0.13059812784194946\n",
            "230 0.12285555899143219\n",
            "240 0.12327728420495987\n",
            "250 0.1227162703871727\n",
            "260 0.12521539628505707\n",
            "270 0.11785106360912323\n",
            "280 0.12859804928302765\n",
            "290 0.12794716656208038\n",
            "300 0.1273239254951477\n",
            "310 0.13067682087421417\n",
            "320 0.12308391183614731\n",
            "330 0.1231968104839325\n",
            "340 0.12302514910697937\n",
            "350 0.1238926500082016\n",
            "360 0.13342469930648804\n",
            "370 0.1259663999080658\n",
            "380 0.12801380455493927\n",
            "390 0.12194273620843887\n",
            "400 0.11578093469142914\n",
            "410 0.11378489434719086\n",
            "420 0.12660767138004303\n",
            "430 0.12716025114059448\n",
            "440 0.12405632436275482\n",
            "450 0.12342459708452225\n",
            "460 0.12521609663963318\n",
            "0 0.12633982300758362\n",
            "10 0.12457793951034546\n",
            "20 0.12985405325889587\n",
            "30 0.11874296516180038\n",
            "40 0.11687928438186646\n",
            "50 0.13287322223186493\n",
            "60 0.1132470890879631\n",
            "70 0.1210874617099762\n",
            "80 0.12570613622665405\n",
            "90 0.12344640493392944\n",
            "100 0.12281852215528488\n",
            "110 0.13214701414108276\n",
            "120 0.1357370764017105\n",
            "130 0.12893304228782654\n",
            "140 0.12357594072818756\n",
            "150 0.1234406977891922\n",
            "160 0.11253102123737335\n",
            "170 0.1322304606437683\n",
            "180 0.13142231106758118\n",
            "190 0.1175515279173851\n",
            "200 0.1236739456653595\n",
            "210 0.12628406286239624\n",
            "220 0.12186457961797714\n",
            "230 0.12882167100906372\n",
            "240 0.12463686615228653\n",
            "250 0.13254164159297943\n",
            "260 0.1177530288696289\n",
            "270 0.12607303261756897\n",
            "280 0.13128499686717987\n",
            "290 0.11486873775720596\n",
            "300 0.1265120506286621\n",
            "310 0.12852445244789124\n",
            "320 0.12047653645277023\n",
            "330 0.12048977613449097\n",
            "340 0.12438542395830154\n",
            "350 0.1289208084344864\n",
            "360 0.12492288649082184\n",
            "370 0.1232525110244751\n",
            "380 0.12824471294879913\n",
            "390 0.1218351274728775\n",
            "400 0.11353375017642975\n",
            "410 0.12259772419929504\n",
            "420 0.12440714240074158\n",
            "430 0.1118394210934639\n",
            "440 0.12096931040287018\n",
            "450 0.1258838176727295\n",
            "460 0.11666904389858246\n",
            "0 0.12161862105131149\n",
            "10 0.1281736195087433\n",
            "20 0.12569445371627808\n",
            "30 0.12928274273872375\n",
            "40 0.12240617722272873\n",
            "50 0.12383907288312912\n",
            "60 0.1276043951511383\n",
            "70 0.13327577710151672\n",
            "80 0.1242733970284462\n",
            "90 0.12263087928295135\n",
            "100 0.1206035315990448\n",
            "110 0.1319904327392578\n",
            "120 0.12734875082969666\n",
            "130 0.1163443997502327\n",
            "140 0.12143339216709137\n",
            "150 0.1312054544687271\n",
            "160 0.12188675999641418\n",
            "170 0.11119743436574936\n",
            "180 0.1281680166721344\n",
            "190 0.12414512038230896\n",
            "200 0.12742720544338226\n",
            "210 0.12649276852607727\n",
            "220 0.1282672882080078\n",
            "230 0.12773343920707703\n",
            "240 0.12318272143602371\n",
            "250 0.11885930597782135\n",
            "260 0.12282788008451462\n",
            "270 0.12948325276374817\n",
            "280 0.1238628700375557\n",
            "290 0.12820486724376678\n",
            "300 0.11937224864959717\n",
            "310 0.12164846062660217\n",
            "320 0.130284383893013\n",
            "330 0.12019915878772736\n",
            "340 0.11963242292404175\n",
            "350 0.1310994178056717\n",
            "360 0.12815462052822113\n",
            "370 0.1257486641407013\n",
            "380 0.13141342997550964\n",
            "390 0.12529809772968292\n",
            "400 0.11938652396202087\n",
            "410 0.12002183496952057\n",
            "420 0.1278754323720932\n",
            "430 0.12407095730304718\n",
            "440 0.12630963325500488\n",
            "450 0.11780804395675659\n",
            "460 0.1206093579530716\n",
            "0 0.11685016006231308\n",
            "10 0.13055063784122467\n",
            "20 0.12866747379302979\n",
            "30 0.12413452565670013\n",
            "40 0.12180742621421814\n",
            "50 0.12733420729637146\n",
            "60 0.11344635486602783\n",
            "70 0.11999251693487167\n",
            "80 0.11942895501852036\n",
            "90 0.12579642236232758\n",
            "100 0.1245836541056633\n",
            "110 0.12562814354896545\n",
            "120 0.11464250832796097\n",
            "130 0.12420693784952164\n",
            "140 0.11916275322437286\n",
            "150 0.11747715622186661\n",
            "160 0.11999040842056274\n",
            "170 0.12536153197288513\n",
            "180 0.12299199402332306\n",
            "190 0.1254468858242035\n",
            "200 0.12107877433300018\n",
            "210 0.1203409880399704\n",
            "220 0.11833855509757996\n",
            "230 0.1223890632390976\n",
            "240 0.1284760981798172\n",
            "250 0.1304946094751358\n",
            "260 0.12653091549873352\n",
            "270 0.13158710300922394\n",
            "280 0.12740345299243927\n",
            "290 0.1251349151134491\n",
            "300 0.12530919909477234\n",
            "310 0.12198196351528168\n",
            "320 0.12811538577079773\n",
            "330 0.12695550918579102\n",
            "340 0.1284620463848114\n",
            "350 0.12057898938655853\n",
            "360 0.1362331211566925\n",
            "370 0.12674036622047424\n",
            "380 0.12711778283119202\n",
            "390 0.13342395424842834\n",
            "400 0.12405136227607727\n",
            "410 0.12787063419818878\n",
            "420 0.12431591749191284\n",
            "430 0.1246720477938652\n",
            "440 0.12807711958885193\n",
            "450 0.12459792196750641\n",
            "460 0.12341892719268799\n",
            "0 0.1229090690612793\n",
            "10 0.12715494632720947\n",
            "20 0.13121335208415985\n",
            "30 0.11697709560394287\n",
            "40 0.11956009268760681\n",
            "50 0.12149729579687119\n",
            "60 0.12988704442977905\n",
            "70 0.12329357862472534\n",
            "80 0.12008094787597656\n",
            "90 0.11972115933895111\n",
            "100 0.12152684479951859\n",
            "110 0.1300269365310669\n",
            "120 0.1312786489725113\n",
            "130 0.1194915845990181\n",
            "140 0.12053631246089935\n",
            "150 0.12047053128480911\n",
            "160 0.12042799592018127\n",
            "170 0.1252581924200058\n",
            "180 0.12259864062070847\n",
            "190 0.12339356541633606\n",
            "200 0.12390159070491791\n",
            "210 0.12115992605686188\n",
            "220 0.13262052834033966\n",
            "230 0.12585756182670593\n",
            "240 0.12095382809638977\n",
            "250 0.1203952357172966\n",
            "260 0.11914944648742676\n",
            "270 0.12094355374574661\n",
            "280 0.13117244839668274\n",
            "290 0.11664032191038132\n",
            "300 0.118604376912117\n",
            "310 0.11947041749954224\n",
            "320 0.1291123330593109\n",
            "330 0.12760785222053528\n",
            "340 0.11676767468452454\n",
            "350 0.11935114115476608\n",
            "360 0.13068446516990662\n",
            "370 0.1253075748682022\n",
            "380 0.12517724931240082\n",
            "390 0.12504450976848602\n",
            "400 0.1293235719203949\n",
            "410 0.13018068671226501\n",
            "420 0.129705548286438\n",
            "430 0.13355952501296997\n",
            "440 0.1282699704170227\n",
            "450 0.13010406494140625\n",
            "460 0.1282804012298584\n",
            "0 0.12137702852487564\n",
            "10 0.12643031775951385\n",
            "20 0.12010523676872253\n",
            "30 0.12281908094882965\n",
            "40 0.12630537152290344\n",
            "50 0.12748873233795166\n",
            "60 0.1201264038681984\n",
            "70 0.12766361236572266\n",
            "80 0.12275193631649017\n",
            "90 0.12098594754934311\n",
            "100 0.12806883454322815\n",
            "110 0.12472392618656158\n",
            "120 0.12235482782125473\n",
            "130 0.11686204373836517\n",
            "140 0.13243739306926727\n",
            "150 0.1379929780960083\n",
            "160 0.12022139132022858\n",
            "170 0.1200549528002739\n",
            "180 0.13082510232925415\n",
            "190 0.12589919567108154\n",
            "200 0.11691931635141373\n",
            "210 0.11915775388479233\n",
            "220 0.11679348349571228\n",
            "230 0.12087716907262802\n",
            "240 0.1238514631986618\n",
            "250 0.12280690670013428\n",
            "260 0.1204572394490242\n",
            "270 0.12214069813489914\n",
            "280 0.12041105329990387\n",
            "290 0.12261838465929031\n",
            "300 0.12227770686149597\n",
            "310 0.1272561252117157\n",
            "320 0.11837366223335266\n",
            "330 0.11702261120080948\n",
            "340 0.12287985533475876\n",
            "350 0.12081996351480484\n",
            "360 0.1201491579413414\n",
            "370 0.11584145575761795\n",
            "380 0.12089652568101883\n",
            "390 0.11933951079845428\n",
            "400 0.12605705857276917\n",
            "410 0.12357665598392487\n",
            "420 0.1289283037185669\n",
            "430 0.11952871829271317\n",
            "440 0.13648155331611633\n",
            "450 0.1213570088148117\n",
            "460 0.12635771930217743\n",
            "0 0.1289437860250473\n",
            "10 0.12682899832725525\n",
            "20 0.13156209886074066\n",
            "30 0.11832944303750992\n",
            "40 0.12132337689399719\n",
            "50 0.12797117233276367\n",
            "60 0.12815365195274353\n",
            "70 0.12623506784439087\n",
            "80 0.1260436773300171\n",
            "90 0.12038915604352951\n",
            "100 0.12299823760986328\n",
            "110 0.1271098107099533\n",
            "120 0.12249083071947098\n",
            "130 0.12254060804843903\n",
            "140 0.12800905108451843\n",
            "150 0.12608811259269714\n",
            "160 0.12797611951828003\n",
            "170 0.1229555681347847\n",
            "180 0.13294649124145508\n",
            "190 0.12058223783969879\n",
            "200 0.12107934057712555\n",
            "210 0.11868292838335037\n",
            "220 0.12392687797546387\n",
            "230 0.11783349514007568\n",
            "240 0.12503625452518463\n",
            "250 0.11803701519966125\n",
            "260 0.12196561694145203\n",
            "270 0.12363146990537643\n",
            "280 0.12226258218288422\n",
            "290 0.1144094169139862\n",
            "300 0.12865513563156128\n",
            "310 0.11421164870262146\n",
            "320 0.11885379999876022\n",
            "330 0.13913026452064514\n",
            "340 0.1192055344581604\n",
            "350 0.11498013138771057\n",
            "360 0.12718364596366882\n",
            "370 0.138224795460701\n",
            "380 0.11373894661664963\n",
            "390 0.1151786521077156\n",
            "400 0.12248478084802628\n",
            "410 0.12440212070941925\n",
            "420 0.11825357377529144\n",
            "430 0.13211296498775482\n",
            "440 0.12819087505340576\n",
            "450 0.12359756976366043\n",
            "460 0.13552558422088623\n",
            "0 0.12781959772109985\n",
            "10 0.1166120171546936\n",
            "20 0.12544523179531097\n",
            "30 0.12464304268360138\n",
            "40 0.12977713346481323\n",
            "50 0.13151875138282776\n",
            "60 0.12582947313785553\n",
            "70 0.11841954290866852\n",
            "80 0.1310439258813858\n",
            "90 0.1228514015674591\n",
            "100 0.13212092220783234\n",
            "110 0.12391950935125351\n",
            "120 0.12126214802265167\n",
            "130 0.12400802969932556\n",
            "140 0.11782047897577286\n",
            "150 0.12069159746170044\n",
            "160 0.12894217669963837\n",
            "170 0.12131863832473755\n",
            "180 0.1262563019990921\n",
            "190 0.11647205799818039\n",
            "200 0.12095951288938522\n",
            "210 0.126936674118042\n",
            "220 0.1327698528766632\n",
            "230 0.12074794620275497\n",
            "240 0.13068649172782898\n",
            "250 0.12157460302114487\n",
            "260 0.12309916317462921\n",
            "270 0.11266124248504639\n",
            "280 0.12181335687637329\n",
            "290 0.12888586521148682\n",
            "300 0.12143893539905548\n",
            "310 0.12174776941537857\n",
            "320 0.12309259921312332\n",
            "330 0.1124451607465744\n",
            "340 0.12370970845222473\n",
            "350 0.12439941614866257\n",
            "360 0.12210047245025635\n",
            "370 0.1359238177537918\n",
            "380 0.12178203463554382\n",
            "390 0.12654636800289154\n",
            "400 0.12376435101032257\n",
            "410 0.12575575709342957\n",
            "420 0.11533516645431519\n",
            "430 0.1353124976158142\n",
            "440 0.1292034387588501\n",
            "450 0.12822845578193665\n",
            "460 0.12780138850212097\n",
            "0 0.12706339359283447\n",
            "10 0.12945470213890076\n",
            "20 0.12010209262371063\n",
            "30 0.11756658554077148\n",
            "40 0.12129238992929459\n",
            "50 0.12808063626289368\n",
            "60 0.11792094260454178\n",
            "70 0.1248522400856018\n",
            "80 0.1296256184577942\n",
            "90 0.12047926336526871\n",
            "100 0.1235431358218193\n",
            "110 0.12483906000852585\n",
            "120 0.12182746082544327\n",
            "130 0.12246447056531906\n",
            "140 0.12736466526985168\n",
            "150 0.12434203922748566\n",
            "160 0.12810328602790833\n",
            "170 0.12723968923091888\n",
            "180 0.13066203892230988\n",
            "190 0.12155342102050781\n",
            "200 0.12124170362949371\n",
            "210 0.11734572052955627\n",
            "220 0.12111939489841461\n",
            "230 0.12336847186088562\n",
            "240 0.11112932860851288\n",
            "250 0.12454812973737717\n",
            "260 0.12835557758808136\n",
            "270 0.11964139342308044\n",
            "280 0.12237200886011124\n",
            "290 0.12607109546661377\n",
            "300 0.12660127878189087\n",
            "310 0.12315165996551514\n",
            "320 0.11265873163938522\n",
            "330 0.11981108784675598\n",
            "340 0.11949746310710907\n",
            "350 0.12259110808372498\n",
            "360 0.12042340636253357\n",
            "370 0.1257459819316864\n",
            "380 0.12670692801475525\n",
            "390 0.12003185600042343\n",
            "400 0.12185047566890717\n",
            "410 0.1251201331615448\n",
            "420 0.1283741295337677\n",
            "430 0.12359252572059631\n",
            "440 0.13000130653381348\n",
            "450 0.12419939041137695\n",
            "460 0.1338459849357605\n",
            "0 0.12766820192337036\n",
            "10 0.12558631598949432\n",
            "20 0.11839590966701508\n",
            "30 0.12286312878131866\n",
            "40 0.12119284272193909\n",
            "50 0.12557801604270935\n",
            "60 0.1326204389333725\n",
            "70 0.12390221655368805\n",
            "80 0.1172853335738182\n",
            "90 0.12816686928272247\n",
            "100 0.1286320835351944\n",
            "110 0.12564271688461304\n",
            "120 0.1347644329071045\n",
            "130 0.12126383185386658\n",
            "140 0.12887857854366302\n",
            "150 0.12556394934654236\n",
            "160 0.11932585388422012\n",
            "170 0.12475210428237915\n",
            "180 0.11988568305969238\n",
            "190 0.12760668992996216\n",
            "200 0.11472849547863007\n",
            "210 0.12361551076173782\n",
            "220 0.11638325452804565\n",
            "230 0.12485674768686295\n",
            "240 0.12806951999664307\n",
            "250 0.13139081001281738\n",
            "260 0.12331525981426239\n",
            "270 0.1273355782032013\n",
            "280 0.12008188664913177\n",
            "290 0.12646640837192535\n",
            "300 0.11880355328321457\n",
            "310 0.1232113316655159\n",
            "320 0.12654897570610046\n",
            "330 0.12332895398139954\n",
            "340 0.126713827252388\n",
            "350 0.11632806062698364\n",
            "360 0.12477629631757736\n",
            "370 0.12807506322860718\n",
            "380 0.11655877530574799\n",
            "390 0.13008034229278564\n",
            "400 0.1232413500547409\n",
            "410 0.12843164801597595\n",
            "420 0.1242256909608841\n",
            "430 0.12836617231369019\n",
            "440 0.11991671472787857\n",
            "450 0.12270112335681915\n",
            "460 0.12029638886451721\n",
            "0 0.1250419169664383\n",
            "10 0.12965676188468933\n",
            "20 0.12328214198350906\n",
            "30 0.1171000599861145\n",
            "40 0.12636849284172058\n",
            "50 0.11924553662538528\n",
            "60 0.12220463156700134\n",
            "70 0.12149959057569504\n",
            "80 0.11997315287590027\n",
            "90 0.12818661332130432\n",
            "100 0.12309707701206207\n",
            "110 0.12107037007808685\n",
            "120 0.11796702444553375\n",
            "130 0.12771287560462952\n",
            "140 0.12648096680641174\n",
            "150 0.11782907694578171\n",
            "160 0.11763570457696915\n",
            "170 0.12515667080879211\n",
            "180 0.1305128037929535\n",
            "190 0.12491584569215775\n",
            "200 0.11595292389392853\n",
            "210 0.12726689875125885\n",
            "220 0.13076704740524292\n",
            "230 0.12819556891918182\n",
            "240 0.1265231817960739\n",
            "250 0.11892654746770859\n",
            "260 0.11847306042909622\n",
            "270 0.11051364243030548\n",
            "280 0.11190719902515411\n",
            "290 0.12401357293128967\n",
            "300 0.12610945105552673\n",
            "310 0.11705073714256287\n",
            "320 0.122846819460392\n",
            "330 0.1218511164188385\n",
            "340 0.12355205416679382\n",
            "350 0.12087500095367432\n",
            "360 0.11855463683605194\n",
            "370 0.13060574233531952\n",
            "380 0.13770437240600586\n",
            "390 0.12971118092536926\n",
            "400 0.1229444369673729\n",
            "410 0.1170404702425003\n",
            "420 0.1259617805480957\n",
            "430 0.12603655457496643\n",
            "440 0.130807563662529\n",
            "450 0.12448268383741379\n",
            "460 0.12216608971357346\n",
            "0 0.1252875030040741\n",
            "10 0.12081295251846313\n",
            "20 0.12202683091163635\n",
            "30 0.1273002028465271\n",
            "40 0.12600171566009521\n",
            "50 0.11602660268545151\n",
            "60 0.12350425124168396\n",
            "70 0.11562804132699966\n",
            "80 0.11735997349023819\n",
            "90 0.12126671522855759\n",
            "100 0.12068066000938416\n",
            "110 0.12425614893436432\n",
            "120 0.12119079381227493\n",
            "130 0.12293708324432373\n",
            "140 0.12794944643974304\n",
            "150 0.12632322311401367\n",
            "160 0.12902629375457764\n",
            "170 0.12434542179107666\n",
            "180 0.12607745826244354\n",
            "190 0.12229526042938232\n",
            "200 0.12779316306114197\n",
            "210 0.12001016736030579\n",
            "220 0.12781396508216858\n",
            "230 0.12107694149017334\n",
            "240 0.1184254139661789\n",
            "250 0.12629863619804382\n",
            "260 0.12022053450345993\n",
            "270 0.1281488537788391\n",
            "280 0.1223902553319931\n",
            "290 0.1318112462759018\n",
            "300 0.11733704060316086\n",
            "310 0.12486754357814789\n",
            "320 0.12307606637477875\n",
            "330 0.1215289756655693\n",
            "340 0.12618622183799744\n",
            "350 0.13063944876194\n",
            "360 0.13221506774425507\n",
            "370 0.12275399267673492\n",
            "380 0.11871562898159027\n",
            "390 0.12595048546791077\n",
            "400 0.1285082995891571\n",
            "410 0.12883010506629944\n",
            "420 0.1177847608923912\n",
            "430 0.12766893208026886\n",
            "440 0.11640715599060059\n",
            "450 0.1322069615125656\n",
            "460 0.12904086709022522\n",
            "0 0.12606382369995117\n",
            "10 0.12851499021053314\n",
            "20 0.12233805656433105\n",
            "30 0.12573912739753723\n",
            "40 0.12752395868301392\n",
            "50 0.12327219545841217\n",
            "60 0.12636592984199524\n",
            "70 0.11771885305643082\n",
            "80 0.12052501738071442\n",
            "90 0.11803300678730011\n",
            "100 0.12713822722434998\n",
            "110 0.12911736965179443\n",
            "120 0.12662062048912048\n",
            "130 0.12425466626882553\n",
            "140 0.12340976297855377\n",
            "150 0.11778036504983902\n",
            "160 0.12224888801574707\n",
            "170 0.1209946945309639\n",
            "180 0.1200423315167427\n",
            "190 0.12405841052532196\n",
            "200 0.12787564098834991\n",
            "210 0.1206245869398117\n",
            "220 0.12710221111774445\n",
            "230 0.12464237958192825\n",
            "240 0.12941443920135498\n",
            "250 0.13368085026741028\n",
            "260 0.13153180480003357\n",
            "270 0.12201356142759323\n",
            "280 0.1267891824245453\n",
            "290 0.12625013291835785\n",
            "300 0.1266331672668457\n",
            "310 0.11126624047756195\n",
            "320 0.12051411718130112\n",
            "330 0.12536072731018066\n",
            "340 0.11833412945270538\n",
            "350 0.1172107383608818\n",
            "360 0.12198956310749054\n",
            "370 0.13095299899578094\n",
            "380 0.1282045543193817\n",
            "390 0.13139143586158752\n",
            "400 0.123422771692276\n",
            "410 0.12415265291929245\n",
            "420 0.11790898442268372\n",
            "430 0.12050817161798477\n",
            "440 0.12182556092739105\n",
            "450 0.13500100374221802\n",
            "460 0.13133969902992249\n",
            "0 0.117775559425354\n",
            "10 0.11824540793895721\n",
            "20 0.12367213517427444\n",
            "30 0.12334398925304413\n",
            "40 0.11848670244216919\n",
            "50 0.11846141517162323\n",
            "60 0.13417652249336243\n",
            "70 0.12478277087211609\n",
            "80 0.11858762800693512\n",
            "90 0.12270420789718628\n",
            "100 0.11904548853635788\n",
            "110 0.12097132205963135\n",
            "120 0.12415998429059982\n",
            "130 0.13347919285297394\n",
            "140 0.11853299289941788\n",
            "150 0.12856392562389374\n",
            "160 0.12274863570928574\n",
            "170 0.1277688890695572\n",
            "180 0.1229572594165802\n",
            "190 0.1261751651763916\n",
            "200 0.12573716044425964\n",
            "210 0.11622387915849686\n",
            "220 0.11875550448894501\n",
            "230 0.12711657583713531\n",
            "240 0.11834074556827545\n",
            "250 0.12308526784181595\n",
            "260 0.12031282484531403\n",
            "270 0.1264609843492508\n",
            "280 0.12470325827598572\n",
            "290 0.12168974429368973\n",
            "300 0.1315566450357437\n",
            "310 0.12149914354085922\n",
            "320 0.12520645558834076\n",
            "330 0.10969751328229904\n",
            "340 0.11454026401042938\n",
            "350 0.12484508007764816\n",
            "360 0.11830256879329681\n",
            "370 0.12592749297618866\n",
            "380 0.13298694789409637\n",
            "390 0.12410956621170044\n",
            "400 0.12236273288726807\n",
            "410 0.12828972935676575\n",
            "420 0.11868025362491608\n",
            "430 0.11878004670143127\n",
            "440 0.11986245214939117\n",
            "450 0.1288537085056305\n",
            "460 0.12816202640533447\n",
            "0 0.11877856403589249\n",
            "10 0.13008302450180054\n",
            "20 0.1276426911354065\n",
            "30 0.12408369034528732\n",
            "40 0.12609784305095673\n",
            "50 0.1196320652961731\n",
            "60 0.12647980451583862\n",
            "70 0.11896966397762299\n",
            "80 0.12383115291595459\n",
            "90 0.12459000945091248\n",
            "100 0.1292751133441925\n",
            "110 0.12379079312086105\n",
            "120 0.12127222120761871\n",
            "130 0.1207977682352066\n",
            "140 0.11825743317604065\n",
            "150 0.12669697403907776\n",
            "160 0.12539023160934448\n",
            "170 0.12083978950977325\n",
            "180 0.12678425014019012\n",
            "190 0.1262863278388977\n",
            "200 0.12229689955711365\n",
            "210 0.1293565332889557\n",
            "220 0.12411200255155563\n",
            "230 0.12235046178102493\n",
            "240 0.13087989389896393\n",
            "250 0.13205373287200928\n",
            "260 0.12354011088609695\n",
            "270 0.12563756108283997\n",
            "280 0.12319648265838623\n",
            "290 0.1254485547542572\n",
            "300 0.12040290981531143\n",
            "310 0.13158422708511353\n",
            "320 0.11650428920984268\n",
            "330 0.1270599067211151\n",
            "340 0.12430160492658615\n",
            "350 0.12576694786548615\n",
            "360 0.12420757114887238\n",
            "370 0.13021358847618103\n",
            "380 0.12096738815307617\n",
            "390 0.11834907531738281\n",
            "400 0.1218423917889595\n",
            "410 0.12275443971157074\n",
            "420 0.11980213969945908\n",
            "430 0.1266854703426361\n",
            "440 0.12442797422409058\n",
            "450 0.1247686892747879\n",
            "460 0.11723409593105316\n",
            "0 0.12164726108312607\n",
            "10 0.12619858980178833\n",
            "20 0.12013503909111023\n",
            "30 0.12541469931602478\n",
            "40 0.127826988697052\n",
            "50 0.11786159873008728\n",
            "60 0.13163989782333374\n",
            "70 0.12600861489772797\n",
            "80 0.12963292002677917\n",
            "90 0.12796282768249512\n",
            "100 0.12105932831764221\n",
            "110 0.12835407257080078\n",
            "120 0.13141955435276031\n",
            "130 0.12445710599422455\n",
            "140 0.12205091118812561\n",
            "150 0.12547090649604797\n",
            "160 0.12990011274814606\n",
            "170 0.1265759915113449\n",
            "180 0.12003067880868912\n",
            "190 0.11576205492019653\n",
            "200 0.12510894238948822\n",
            "210 0.12767256796360016\n",
            "220 0.12467251718044281\n",
            "230 0.12649424374103546\n",
            "240 0.13007420301437378\n",
            "250 0.1265248954296112\n",
            "260 0.12649762630462646\n",
            "270 0.12171479314565659\n",
            "280 0.12312111258506775\n",
            "290 0.12717315554618835\n",
            "300 0.1233593299984932\n",
            "310 0.13414053618907928\n",
            "320 0.12088073790073395\n",
            "330 0.13024435937404633\n",
            "340 0.12174161523580551\n",
            "350 0.11974373459815979\n",
            "360 0.13287830352783203\n",
            "370 0.12144524604082108\n",
            "380 0.12763723731040955\n",
            "390 0.13164204359054565\n",
            "400 0.12842561304569244\n",
            "410 0.12633107602596283\n",
            "420 0.11641573160886765\n",
            "430 0.12717914581298828\n",
            "440 0.13049155473709106\n",
            "450 0.12932871282100677\n",
            "460 0.1226578876376152\n",
            "0 0.11986622214317322\n",
            "10 0.12623529136180878\n",
            "20 0.12098254263401031\n",
            "30 0.12799465656280518\n",
            "40 0.1184760183095932\n",
            "50 0.12602874636650085\n",
            "60 0.1276177614927292\n",
            "70 0.1287638396024704\n",
            "80 0.11916326731443405\n",
            "90 0.1254386454820633\n",
            "100 0.1299692690372467\n",
            "110 0.12305255234241486\n",
            "120 0.12376084923744202\n",
            "130 0.11869838833808899\n",
            "140 0.12459853291511536\n",
            "150 0.12534892559051514\n",
            "160 0.12247580289840698\n",
            "170 0.1295813024044037\n",
            "180 0.12605690956115723\n",
            "190 0.12512555718421936\n",
            "200 0.12907195091247559\n",
            "210 0.12875109910964966\n",
            "220 0.11800937354564667\n",
            "230 0.12599784135818481\n",
            "240 0.1223241463303566\n",
            "250 0.11444389075040817\n",
            "260 0.11509085446596146\n",
            "270 0.12709414958953857\n",
            "280 0.12246239185333252\n",
            "290 0.1263056844472885\n",
            "300 0.12213696539402008\n",
            "310 0.12677575647830963\n",
            "320 0.12516364455223083\n",
            "330 0.12983351945877075\n",
            "340 0.12425999343395233\n",
            "350 0.11642827838659286\n",
            "360 0.12454923987388611\n",
            "370 0.1193300187587738\n",
            "380 0.12233541905879974\n",
            "390 0.12810876965522766\n",
            "400 0.1235351413488388\n",
            "410 0.12023971974849701\n",
            "420 0.12266593426465988\n",
            "430 0.11907152831554413\n",
            "440 0.1220250353217125\n",
            "450 0.12445249408483505\n",
            "460 0.12521931529045105\n",
            "0 0.12970179319381714\n",
            "10 0.11586925387382507\n",
            "20 0.12093956768512726\n",
            "30 0.1207454651594162\n",
            "40 0.1266544908285141\n",
            "50 0.11860542744398117\n",
            "60 0.1256175935268402\n",
            "70 0.12020403146743774\n",
            "80 0.12367846071720123\n",
            "90 0.12742382287979126\n",
            "100 0.12892203032970428\n",
            "110 0.12561647593975067\n",
            "120 0.1202317625284195\n",
            "130 0.12036187201738358\n",
            "140 0.12368585169315338\n",
            "150 0.12795883417129517\n",
            "160 0.12140922248363495\n",
            "170 0.12182120233774185\n",
            "180 0.12963822484016418\n",
            "190 0.12412513792514801\n",
            "200 0.11796360462903976\n",
            "210 0.13207803666591644\n",
            "220 0.12316185235977173\n",
            "230 0.11386473476886749\n",
            "240 0.12003792077302933\n",
            "250 0.12539786100387573\n",
            "260 0.11358039081096649\n",
            "270 0.12449280917644501\n",
            "280 0.12371088564395905\n",
            "290 0.12414504587650299\n",
            "300 0.13340309262275696\n",
            "310 0.11120551079511642\n",
            "320 0.12271629273891449\n",
            "330 0.12561284005641937\n",
            "340 0.12440640479326248\n",
            "350 0.12061157822608948\n",
            "360 0.13178172707557678\n",
            "370 0.12452103197574615\n",
            "380 0.11883462965488434\n",
            "390 0.12469750642776489\n",
            "400 0.12494783103466034\n",
            "410 0.11821937561035156\n",
            "420 0.12273997068405151\n",
            "430 0.11990935355424881\n",
            "440 0.12378314137458801\n",
            "450 0.1212850958108902\n",
            "460 0.11854685842990875\n",
            "0 0.12306903302669525\n",
            "10 0.11834316700696945\n",
            "20 0.120343878865242\n",
            "30 0.1247362345457077\n",
            "40 0.11948806792497635\n",
            "50 0.12645675241947174\n",
            "60 0.12729468941688538\n",
            "70 0.12414363026618958\n",
            "80 0.12868466973304749\n",
            "90 0.13172262907028198\n",
            "100 0.1217162013053894\n",
            "110 0.14162224531173706\n",
            "120 0.12239280343055725\n",
            "130 0.11947116255760193\n",
            "140 0.12323737889528275\n",
            "150 0.12441523373126984\n",
            "160 0.11940652132034302\n",
            "170 0.12227531522512436\n",
            "180 0.11893893778324127\n",
            "190 0.11713580042123795\n",
            "200 0.1267595738172531\n",
            "210 0.12531696259975433\n",
            "220 0.13101981580257416\n",
            "230 0.12520119547843933\n",
            "240 0.12163884937763214\n",
            "250 0.1212950274348259\n",
            "260 0.12375374883413315\n",
            "270 0.12497285008430481\n",
            "280 0.1216360330581665\n",
            "290 0.1261385828256607\n",
            "300 0.13030074536800385\n",
            "310 0.12367706000804901\n",
            "320 0.12060801684856415\n",
            "330 0.11618882417678833\n",
            "340 0.13306549191474915\n",
            "350 0.12318958342075348\n",
            "360 0.1233416274189949\n",
            "370 0.1263878345489502\n",
            "380 0.12498082965612411\n",
            "390 0.12444165349006653\n",
            "400 0.12665274739265442\n",
            "410 0.12874934077262878\n",
            "420 0.12738783657550812\n",
            "430 0.12519888579845428\n",
            "440 0.1244698017835617\n",
            "450 0.12369086593389511\n",
            "460 0.12950675189495087\n",
            "0 0.13228768110275269\n",
            "10 0.12415408343076706\n",
            "20 0.12343895435333252\n",
            "30 0.11764445155858994\n",
            "40 0.12514401972293854\n",
            "50 0.12224158644676208\n",
            "60 0.13126911222934723\n",
            "70 0.12183496356010437\n",
            "80 0.12188754230737686\n",
            "90 0.1192355751991272\n",
            "100 0.12385661900043488\n",
            "110 0.11991631984710693\n",
            "120 0.11886648088693619\n",
            "130 0.12384922802448273\n",
            "140 0.12581858038902283\n",
            "150 0.1199939027428627\n",
            "160 0.11988675594329834\n",
            "170 0.12480705976486206\n",
            "180 0.13346831500530243\n",
            "190 0.13338559865951538\n",
            "200 0.12809687852859497\n",
            "210 0.11681842803955078\n",
            "220 0.12285701930522919\n",
            "230 0.1334816813468933\n",
            "240 0.1138790026307106\n",
            "250 0.12192759662866592\n",
            "260 0.11950543522834778\n",
            "270 0.11942104250192642\n",
            "280 0.12719669938087463\n",
            "290 0.13325342535972595\n",
            "300 0.11716525256633759\n",
            "310 0.127998948097229\n",
            "320 0.11722961068153381\n",
            "330 0.10712526738643646\n",
            "340 0.12979304790496826\n",
            "350 0.12627635896205902\n",
            "360 0.12212961912155151\n",
            "370 0.12915346026420593\n",
            "380 0.12180285155773163\n",
            "390 0.12050994485616684\n",
            "400 0.1206599622964859\n",
            "410 0.12289644032716751\n",
            "420 0.12637105584144592\n",
            "430 0.12388359755277634\n",
            "440 0.11981760710477829\n",
            "450 0.12242338061332703\n",
            "460 0.11898140609264374\n",
            "0 0.12159819155931473\n",
            "10 0.12481719255447388\n",
            "20 0.1331106275320053\n",
            "30 0.12152400612831116\n",
            "40 0.1248588040471077\n",
            "50 0.12420161068439484\n",
            "60 0.12149606645107269\n",
            "70 0.11715636402368546\n",
            "80 0.1287919133901596\n",
            "90 0.13147422671318054\n",
            "100 0.1225142553448677\n",
            "110 0.1250515729188919\n",
            "120 0.122837133705616\n",
            "130 0.13161152601242065\n",
            "140 0.12315905094146729\n",
            "150 0.12417665123939514\n",
            "160 0.12783955037593842\n",
            "170 0.12154649198055267\n",
            "180 0.11527136713266373\n",
            "190 0.12025882303714752\n",
            "200 0.12377255409955978\n",
            "210 0.12275531888008118\n",
            "220 0.11924788355827332\n",
            "230 0.11804570257663727\n",
            "240 0.12911438941955566\n",
            "250 0.12504741549491882\n",
            "260 0.12990526854991913\n",
            "270 0.12418806552886963\n",
            "280 0.12940913438796997\n",
            "290 0.1269112229347229\n",
            "300 0.12780025601387024\n",
            "310 0.1305837631225586\n",
            "320 0.13563650846481323\n",
            "330 0.11911861598491669\n",
            "340 0.1293925940990448\n",
            "350 0.11873900890350342\n",
            "360 0.11596351116895676\n",
            "370 0.1287887543439865\n",
            "380 0.1202978864312172\n",
            "390 0.12526550889015198\n",
            "400 0.12793654203414917\n",
            "410 0.13441675901412964\n",
            "420 0.12805400788784027\n",
            "430 0.12616229057312012\n",
            "440 0.12058083713054657\n",
            "450 0.12014473974704742\n",
            "460 0.12474825978279114\n",
            "0 0.12020988762378693\n",
            "10 0.12403527647256851\n",
            "20 0.13174498081207275\n",
            "30 0.11999452859163284\n",
            "40 0.12632787227630615\n",
            "50 0.12634402513504028\n",
            "60 0.12252023816108704\n",
            "70 0.12052369117736816\n",
            "80 0.1266648769378662\n",
            "90 0.11338365077972412\n",
            "100 0.12755462527275085\n",
            "110 0.11856541037559509\n",
            "120 0.12150588631629944\n",
            "130 0.11833862960338593\n",
            "140 0.12019378691911697\n",
            "150 0.1235537901520729\n",
            "160 0.1226472407579422\n",
            "170 0.12173441052436829\n",
            "180 0.1254509836435318\n",
            "190 0.13060158491134644\n",
            "200 0.12074156105518341\n",
            "210 0.13045305013656616\n",
            "220 0.12181094288825989\n",
            "230 0.12109238654375076\n",
            "240 0.11643598973751068\n",
            "250 0.12826362252235413\n",
            "260 0.11651761084794998\n",
            "270 0.11543293297290802\n",
            "280 0.12565977871418\n",
            "290 0.11684706807136536\n",
            "300 0.12605276703834534\n",
            "310 0.1251690834760666\n",
            "320 0.11975836753845215\n",
            "330 0.12271235883235931\n",
            "340 0.11881622672080994\n",
            "350 0.11864667385816574\n",
            "360 0.12153789401054382\n",
            "370 0.12718437612056732\n",
            "380 0.13025562465190887\n",
            "390 0.12130733579397202\n",
            "400 0.11464852839708328\n",
            "410 0.12221923470497131\n",
            "420 0.12711931765079498\n",
            "430 0.11600866913795471\n",
            "440 0.12554025650024414\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def otfm_loss(model, x1, cond, sig_min = 0.001, eps = 1e-5): # https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "    batch = x1.size(0)\n",
        "    # t = torch.rand((batch,), device=device) % (1 - eps)\n",
        "    t = logit_normal.rsample((batch,)).to(device)\n",
        "    t_ = t[...,None,None,None]\n",
        "    x0 = torch.randn_like(x1)\n",
        "    # print(\"otfm_loss\", x0.shape, t.shape)\n",
        "    psi_t = (1 - (1-sig_min)*t_)*x0 + t_*x1 # ψt(x) = (1 − (1 − σmin)t)x + tx1, (22)\n",
        "    v_psi = model(psi_t, t, cond) # vt(ψt(x0))\n",
        "    d_psi = x1 - (1 - sig_min) * x0 #\n",
        "    return F.mse_loss(v_psi, d_psi) # LCFM(θ)\n",
        "\n",
        "# batch = 4\n",
        "# model = UNetModel(cond_dim=16).to(device)\n",
        "# x1=torch.rand((batch,3,16,16),device=device)\n",
        "# t = torch.rand((batch,1), device=device) # in [0,1] [N,1]\n",
        "# # print(t)\n",
        "# cond=torch.rand((batch,16),device=device)\n",
        "# loss = otfm_loss(model, x1, cond)\n",
        "\n",
        "\n",
        "def train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "        # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.amp.autocast('cuda'): # torch.amp.GradScaler('cuda')\n",
        "        # with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "            # x1 = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            x1 = F.interpolate(x1, size=(16,16))#.repeat(1,3,1,1)\n",
        "            loss = otfm_loss(model, x1, cond)\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        if i % 10 == 0: print(i,loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "# x1 = F.interpolate(x1, size=(28,28))\n",
        "# F.avg_pool2d(input, kernel_size, stride=None, padding=0)\n",
        "\n",
        "for epoch in range(40):\n",
        "    train(model, optim, train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "6a7f492c-7796-44bb-d0ba-63477af7769d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>██▅▅▅▃▃▁▃▃▂▃▃▃▂▄▂▁▃▃▂▂▄▃▂▂▂▂▃▂▄▂▂▃▃▂▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.12422</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swift-deluge-10</strong> at: <a href='https://wandb.ai/bobdole/lfm/runs/okil4kpj' target=\"_blank\">https://wandb.ai/bobdole/lfm/runs/okil4kpj</a><br> View project at: <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">https://wandb.ai/bobdole/lfm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250217_122354-okil4kpj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250217_144808-gy61ur6l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/lfm/runs/gy61ur6l' target=\"_blank\">hopeful-resonance-11</a></strong> to <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">https://wandb.ai/bobdole/lfm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/lfm/runs/gy61ur6l' target=\"_blank\">https://wandb.ai/bobdole/lfm/runs/gy61ur6l</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"lfm\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6KfhGOPSJC4",
        "outputId": "c0750dc7-49ca-4a46-f758-7d4b08cac569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torchdiffeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFc76OzlFnE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "78c6b8c8-9d1d-4af6-fc7a-2c267899c740"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Sampling chatgpt\n",
        "\n",
        "# def sample_reverse_time(model, num_samples=1000, timesteps=50, device='cpu'):\n",
        "#     model.eval()\n",
        "#     dt = 1.0 / timesteps\n",
        "#     # x = torch.randn(num_samples, model.net[0].in_features - 1, device=device)\n",
        "#     x = torch.randn(num_samples, 1,16,16, device=device)\n",
        "#     for i in range(timesteps, 0, -1): # [timesteps,...,1]\n",
        "#         t = torch.full((num_samples, 1), i * dt, device=device)  # Current time # [num_samples, 1] 1.\n",
        "\n",
        "#         v = model(x, t)\n",
        "#         x = x - dt * v # Euler update\n",
        "\n",
        "#         # k1 = model(x, t)\n",
        "#         # k2 = model(x - 0.5 * dt * k1, t - 0.5 * dt)\n",
        "#         # k3 = model(x - 0.5 * dt * k2, t - 0.5 * dt)\n",
        "#         # k4 = model(x - dt * k3, t - dt)\n",
        "#         # x = x - (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4) # RK4 update\n",
        "\n",
        "#     return x\n",
        "\n",
        "from torchdiffeq import odeint # https://github.com/rtqichen/torchdiffeq\n",
        "def sample_reverse_time(model, num_samples=1, timesteps=25, device='cpu'):\n",
        "    # model.eval()\n",
        "    # x_init = torch.randn(num_samples, model.net[0].in_features - 1, device=device)\n",
        "    x_init = torch.randn(num_samples, 1,16,16, device=device)\n",
        "    t_span = torch.tensor([1.0, 0.0], device=device)  # Reverse from t=1 to t=0\n",
        "    # t_span = torch.linspace(1.0, 0.0, timesteps, device=device)  # Use timesteps to augment t_span\n",
        "    # t_span = torch.cat([torch.rand(timesteps-1, device=device).sort()[0], torch.tensor([1.],device=device)])\n",
        "    # print(t_span)\n",
        "    x_samples = odeint(model, x_init, t_span, method='dopri5') # dopri5 euler rk4\n",
        "    return x_samples[-1] # Final sampled data\n",
        "\n",
        "# relative rtol and absolute atol error tolerance.\n",
        "# RKAdaptiveStepsizeODESolver(func, y0, rtol, atol, min_step=0, max_step=float('inf'), first_step=None, step_t=None, jump_t=None, safety=0.9, ifactor=10.0, dfactor=0.2, max_num_steps=2 ** 31 - 1, dtype=torch.float64, **kwargs)\n",
        "# https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/rk_common.py#L159\n",
        "\n",
        "# https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/odeint.py#L36\n",
        "# odeint(func, y0, t, *, rtol=1e-7, atol=1e-9, method=None, options=None, event_fn=None)\n",
        "\n",
        "\n",
        "\n",
        "class Func(nn.Module):\n",
        "    def __init__(self, model, cond):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.cond = cond\n",
        "\n",
        "    def forward(self, t, y):\n",
        "        n_samples = y.shape[0]\n",
        "        dydt = -self.model(y, t.repeat(n_samples), self.cond.repeat(n_samples,1)) # [n_samples, c,h,w], [n_samples], [n_samples, cond_dim]\n",
        "        return dydt\n",
        "\n",
        "\n",
        "# class ODEFunc(nn.Module): # https://github.com/rtqichen/torchdiffeq/blob/master/examples/ode_demo.py\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.net = nn.Sequential()\n",
        "#     def forward(self, t, y):\n",
        "#         return self.net(y**3)\n",
        "# # dy/dt = f(t, y)    y(t0) = y0\n",
        "# pred_y = odeint(func, batch_y0, batch_t).to(device) # nn.Module, [batch, *img], [batch]\n",
        "\n",
        "\n",
        "# cond = cond_emb(torch.tensor([4], device=device))\n",
        "cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).to(torch.float)\n",
        "\n",
        "n_samples = 1\n",
        "# sampled_data = sample_reverse_time(Func(model, cond), num_samples=n_samples, timesteps=25, device=device)\n",
        "f = lambda t, y: -model(y, t.repeat(n_samples), cond.repeat(n_samples,1))\n",
        "sampled_data = sample_reverse_time(f, num_samples=n_samples, timesteps=25, device=device)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(sampled_data.detach().cpu().squeeze())\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataiter = iter(train_data)\n",
        "x,y = next(dataiter)\n",
        "# print(x.shape)\n",
        "x=x.unsqueeze(0)\n",
        "x1 = F.interpolate(x, size=(16,16))#.repeat(1,3,1,1)\n",
        "plt.imshow(x1.squeeze())\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "nFhmfVnrOiQd",
        "outputId": "d1f69301-9ab1-4d28-9684-f0c9e212ed1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgGBIAw9vqzK"
      },
      "outputs": [],
      "source": [
        "num_samples=1000\n",
        "timesteps=5\n",
        "dt = 1.0 / timesteps\n",
        "for i in range(timesteps, 0, -1): # [timesteps,...,1]\n",
        "    # print(i)\n",
        "    t = torch.full((num_samples, 1), i * dt, device=device)  # Current time\n",
        "    print(t)\n",
        "    break\n",
        "# [num_samples, 1] 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGN0e0Mxe5UI",
        "outputId": "dd0fd02a-b824-49a4-ec75-929e537a2e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'unet.pkl', map_location=device).values()\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'unetch16depth4.pkl')"
      ],
      "metadata": {
        "id": "vrFxTtw4eFSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CiEv-YOg8PFq"
      },
      "outputs": [],
      "source": [
        "# @title Sampling lebellig\n",
        "# https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "n_samples = 10_000\n",
        "with torch.no_grad():\n",
        "    x_0 = torch.randn(n_samples, 2, device=device)\n",
        "    x_1_hat = v_t.decode(x_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x_1_hat = x_1_hat.cpu().numpy()\n",
        "plt.hist2d(x_1_hat[:, 0], x_1_hat[:, 1], bins=164)\n",
        "plt.show()\n",
        "\n",
        "# https://github.com/probabilists/zuko/blob/master/zuko/utils.py\n",
        "def unpack(x: Tensor, shapes: Sequence[Size]) -> Sequence[Tensor]:\n",
        "    r\"\"\"Unpacks a packed tensor.\n",
        "\n",
        "    Arguments:\n",
        "        x: A packed tensor, with shape :math:`(*, D)`.\n",
        "        shapes: A sequence of shapes :math:`S_i`, corresponding to the total number of\n",
        "            elements :math:`D`.\n",
        "\n",
        "    Returns:\n",
        "        The unpacked tensors, with shapes :math:`(*, S_i)`.\n",
        "\n",
        "    Example:\n",
        "        >>> x = torch.randn(26)\n",
        "        >>> y, z = unpack(x, ((1, 2, 3), (4, 5)))\n",
        "        >>> y.shape\n",
        "        torch.Size([1, 2, 3])\n",
        "        >>> z.shape\n",
        "        torch.Size([4, 5])\n",
        "    \"\"\"\n",
        "\n",
        "    sizes = [math.prod(s) for s in shapes]\n",
        "\n",
        "    x = x.split(sizes, -1)\n",
        "    x = (y.unflatten(-1, (*s, 1)) for y, s in zip(x, shapes))\n",
        "    x = (y.squeeze(-1) for y in x)\n",
        "\n",
        "    return tuple(x)\n",
        "\n",
        "def odeint(\n",
        "    f: Callable[[Tensor, Tensor], Tensor],\n",
        "    x: Union[Tensor, Sequence[Tensor]],\n",
        "    t0: Union[float, Tensor],\n",
        "    t1: Union[float, Tensor],\n",
        "    phi: Iterable[Tensor] = (),\n",
        "    atol: float = 1e-6,\n",
        "    rtol: float = 1e-5,\n",
        ") -> Union[Tensor, Sequence[Tensor]]:\n",
        "    r\"\"\"Integrates a system of first-order ordinary differential equations (ODEs)\n",
        "    .. math:: \\frac{dx}{dt} = f_\\phi(t, x) ,\n",
        "    from :math:`t_0` to :math:`t_1` using the adaptive Dormand-Prince method. The\n",
        "    output is the final state\n",
        "    .. math:: x(t_1) = x_0 + \\int_{t_0}^{t_1} f_\\phi(t, x(t)) ~ dt .\n",
        "    Gradients are propagated through :math:`x_0`, :math:`t_0`, :math:`t_1` and\n",
        "    :math:`\\phi` via the adaptive checkpoint adjoint (ACA) method.\n",
        "    References:\n",
        "        | Neural Ordinary Differential Equations (Chen el al., 2018) | https://arxiv.org/abs/1806.07366\n",
        "        | Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE (Zhuang et al., 2020) | https://arxiv.org/abs/2006.02493\n",
        "\n",
        "    Arguments:\n",
        "        f: A system of first-order ODEs :math:`f_\\phi`.\n",
        "        x: The initial state :math:`x_0`.\n",
        "        t0: The initial integration time :math:`t_0`.\n",
        "        t1: The final integration time :math:`t_1`.\n",
        "        phi: The parameters :math:`\\phi` of :math:`f_\\phi`.\n",
        "        atol: The absolute tolerance.\n",
        "        rtol: The relative tolerance.\n",
        "\n",
        "    Returns: The final state :math:`x(t_1)`.\n",
        "\n",
        "    Example:\n",
        "        >>> A = torch.randn(3, 3)\n",
        "        >>> f = lambda t, x: x @ A\n",
        "        >>> x0 = torch.randn(3)\n",
        "        >>> x1 = odeint(f, x0, 0.0, 1.0)\n",
        "        >>> x1\n",
        "        tensor([-1.4596,  0.5008,  1.5828])\n",
        "    \"\"\"\n",
        "\n",
        "    settings = (atol, rtol, torch.is_grad_enabled())\n",
        "    if torch.is_tensor(x):\n",
        "        x0 = x\n",
        "        g = f\n",
        "    else: shapes = [y.shape for y in x]\n",
        "\n",
        "        def pack(x: Iterable[Tensor]) -> Tensor:\n",
        "            return torch.cat([y.flatten() for y in x])\n",
        "\n",
        "        x0 = pack(x)\n",
        "        g = lambda t, x: pack(f(t, *unpack(x, shapes)))\n",
        "\n",
        "    t0 = torch.as_tensor(t0, dtype=x0.dtype, device=x0.device)\n",
        "    t1 = torch.as_tensor(t1, dtype=x0.dtype, device=x0.device)\n",
        "\n",
        "    assert not t0.shape and not t1.shape, \"'t0' and 't1' must be scalars\"\n",
        "\n",
        "    x1 = AdaptiveCheckpointAdjoint.apply(settings, g, x0, t0, t1, *phi)\n",
        "\n",
        "    if torch.is_tensor(x):\n",
        "        return x1\n",
        "    else:\n",
        "        return unpack(x1, shapes)\n",
        "\n",
        "\n",
        "\n",
        "def wrapper(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "    t = t * torch.ones(len(x), device=x.device)\n",
        "    return self(t, x)\n",
        "\n",
        "from zuko.utils import odeint\n",
        "def decode_t0_t1(self, x_0, t0, t1):\n",
        "    return odeint(self.wrapper, x_0, t0, t1, self.parameters())\n",
        "\n",
        "\n",
        "N_SAMPLES = 10_000\n",
        "N_STEPS = 100\n",
        "t_steps = torch.linspace(0, 1, N_STEPS, device=device)\n",
        "with torch.no_grad():\n",
        "    x_t = [torch.randn(n_samples, 2, device=device)]\n",
        "    for t in range(len(t_steps)-1):\n",
        "      x_t += [v_t.decode_t0_t1(x_t[-1], t_steps[t], t_steps[t+1])]\n",
        "\n",
        "# pad predictions\n",
        "x_t = [x_t[0]]*10 + x_t + [x_t[-1]] * 10\n",
        "\n",
        "x_t_numpy = np.array([x.detach().cpu().numpy() for x in x_t])\n",
        "filename = f\"{DATASET}_{MODEL}_{N_SAMPLES}_{N_STEPS}.npy\"\n",
        "np.save(filename, x_t_numpy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPcm3g4mpD2h"
      },
      "source": [
        "## save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xGntuhWs6lL_"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen\n",
        "# https://github.com/lucidrains/imagen-pytorch/blob/main/imagen_pytorch/imagen_pytorch.py\n",
        "# https://arxiv.org/pdf/2205.11487\n",
        "\n",
        "import math\n",
        "from random import random\n",
        "from beartype.typing import List, Union, Optional\n",
        "from beartype import beartype\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial, wraps\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch import nn, einsum\n",
        "from torch.amp import autocast\n",
        "from torch.special import expm1\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import kornia.augmentation as K\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n",
        "\n",
        "from imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def identity(t, *args, **kwargs):\n",
        "    return t\n",
        "\n",
        "def divisible_by(numer, denom):\n",
        "    return (numer % denom) == 0\n",
        "\n",
        "def first(arr, d = None):\n",
        "    if len(arr) == 0:\n",
        "        return d\n",
        "    return arr[0]\n",
        "\n",
        "def maybe(fn):\n",
        "    @wraps(fn)\n",
        "    def inner(x):\n",
        "        if not exists(x):\n",
        "            return x\n",
        "        return fn(x)\n",
        "    return inner\n",
        "\n",
        "def once(fn):\n",
        "    called = False\n",
        "    @wraps(fn)\n",
        "    def inner(x):\n",
        "        nonlocal called\n",
        "        if called:\n",
        "            return\n",
        "        called = True\n",
        "        return fn(x)\n",
        "    return inner\n",
        "\n",
        "print_once = once(print)\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "def cast_tuple(val, length = None):\n",
        "    if isinstance(val, list):\n",
        "        val = tuple(val)\n",
        "\n",
        "    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n",
        "\n",
        "    if exists(length):\n",
        "        assert len(output) == length\n",
        "\n",
        "    return output\n",
        "\n",
        "def compact(input_dict):\n",
        "    return {key: value for key, value in input_dict.items() if exists(value)}\n",
        "\n",
        "def maybe_transform_dict_key(input_dict, key, fn):\n",
        "    if key not in input_dict:\n",
        "        return input_dict\n",
        "\n",
        "    copied_dict = input_dict.copy()\n",
        "    copied_dict[key] = fn(copied_dict[key])\n",
        "    return copied_dict\n",
        "\n",
        "def cast_uint8_images_to_float(images):\n",
        "    if not images.dtype == torch.uint8:\n",
        "        return images\n",
        "    return images / 255\n",
        "\n",
        "def module_device(module):\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "def zero_init_(m):\n",
        "    nn.init.zeros_(m.weight)\n",
        "    if exists(m.bias):\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "def eval_decorator(fn):\n",
        "    def inner(model, *args, **kwargs):\n",
        "        was_training = model.training\n",
        "        model.eval()\n",
        "        out = fn(model, *args, **kwargs)\n",
        "        model.train(was_training)\n",
        "        return out\n",
        "    return inner\n",
        "\n",
        "def pad_tuple_to_length(t, length, fillvalue = None):\n",
        "    remain_length = length - len(t)\n",
        "    if remain_length <= 0:\n",
        "        return t\n",
        "    return (*t, *((fillvalue,) * remain_length))\n",
        "\n",
        "# helper classes\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "# tensor helpers\n",
        "\n",
        "def log(t, eps: float = 1e-12):\n",
        "    return torch.log(t.clamp(min = eps))\n",
        "\n",
        "def l2norm(t):\n",
        "    return F.normalize(t, dim = -1)\n",
        "\n",
        "def right_pad_dims_to(x, t):\n",
        "    padding_dims = x.ndim - t.ndim\n",
        "    if padding_dims <= 0:\n",
        "        return t\n",
        "    return t.view(*t.shape, *((1,) * padding_dims))\n",
        "\n",
        "def masked_mean(t, *, dim, mask = None):\n",
        "    if not exists(mask):\n",
        "        return t.mean(dim = dim)\n",
        "\n",
        "    denom = mask.sum(dim = dim, keepdim = True)\n",
        "    mask = rearrange(mask, 'b n -> b n 1')\n",
        "    masked_t = t.masked_fill(~mask, 0.)\n",
        "\n",
        "    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n",
        "\n",
        "def resize_image_to(\n",
        "    image,\n",
        "    target_image_size,\n",
        "    clamp_range = None,\n",
        "    mode = 'nearest'\n",
        "):\n",
        "    orig_image_size = image.shape[-1]\n",
        "\n",
        "    if orig_image_size == target_image_size:\n",
        "        return image\n",
        "\n",
        "    out = F.interpolate(image, target_image_size, mode = mode)\n",
        "\n",
        "    if exists(clamp_range):\n",
        "        out = out.clamp(*clamp_range)\n",
        "\n",
        "    return out\n",
        "\n",
        "def calc_all_frame_dims(\n",
        "    downsample_factors: List[int],\n",
        "    frames\n",
        "):\n",
        "    if not exists(frames):\n",
        "        return (tuple(),) * len(downsample_factors)\n",
        "\n",
        "    all_frame_dims = []\n",
        "\n",
        "    for divisor in downsample_factors:\n",
        "        assert divisible_by(frames, divisor)\n",
        "        all_frame_dims.append((frames // divisor,))\n",
        "\n",
        "    return all_frame_dims\n",
        "\n",
        "def safe_get_tuple_index(tup, index, default = None):\n",
        "    if len(tup) <= index:\n",
        "        return default\n",
        "    return tup[index]\n",
        "\n",
        "def pack_one_with_inverse(x, pattern):\n",
        "    packed, packed_shape = pack([x], pattern)\n",
        "\n",
        "    def inverse(x, inverse_pattern = None):\n",
        "        inverse_pattern = default(inverse_pattern, pattern)\n",
        "        return unpack(x, packed_shape, inverse_pattern)[0]\n",
        "\n",
        "    return packed, inverse\n",
        "\n",
        "# image normalization functions\n",
        "# ddpms expect images to be in the range of -1 to 1\n",
        "\n",
        "def normalize_neg_one_to_one(img):\n",
        "    return img * 2 - 1\n",
        "\n",
        "def unnormalize_zero_to_one(normed_img):\n",
        "    return (normed_img + 1) * 0.5\n",
        "\n",
        "# classifier free guidance functions\n",
        "\n",
        "def prob_mask_like(shape, prob, device):\n",
        "    if prob == 1:\n",
        "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
        "    elif prob == 0:\n",
        "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
        "    else:\n",
        "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n",
        "\n",
        "# for improved cfg, getting parallel and orthogonal components of cfg update\n",
        "\n",
        "def project(x, y):\n",
        "    x, inverse = pack_one_with_inverse(x, 'b *')\n",
        "    y, _ = pack_one_with_inverse(y, 'b *')\n",
        "\n",
        "    dtype = x.dtype\n",
        "    x, y = x.double(), y.double()\n",
        "    unit = F.normalize(y, dim = -1)\n",
        "\n",
        "    parallel = (x * unit).sum(dim = -1, keepdim = True) * unit\n",
        "    orthogonal = x - parallel\n",
        "\n",
        "    return inverse(parallel).to(dtype), inverse(orthogonal).to(dtype)\n",
        "\n",
        "# gaussian diffusion with continuous time helper functions and classes\n",
        "# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n",
        "\n",
        "@torch.jit.script\n",
        "def beta_linear_log_snr(t):\n",
        "    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n",
        "\n",
        "@torch.jit.script\n",
        "def alpha_cosine_log_snr(t, s: float = 0.008):\n",
        "    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n",
        "\n",
        "def log_snr_to_alpha_sigma(log_snr):\n",
        "    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n",
        "\n",
        "class GaussianDiffusionContinuousTimes(nn.Module):\n",
        "    def __init__(self, *, noise_schedule, timesteps = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        if noise_schedule == \"linear\":\n",
        "            self.log_snr = beta_linear_log_snr\n",
        "        elif noise_schedule == \"cosine\":\n",
        "            self.log_snr = alpha_cosine_log_snr\n",
        "        else:\n",
        "            raise ValueError(f'invalid noise schedule {noise_schedule}')\n",
        "\n",
        "        self.num_timesteps = timesteps\n",
        "\n",
        "    def get_times(self, batch_size, noise_level, *, device):\n",
        "        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n",
        "\n",
        "    def sample_random_times(self, batch_size, *, device):\n",
        "        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n",
        "\n",
        "    def get_condition(self, times):\n",
        "        return maybe(self.log_snr)(times)\n",
        "\n",
        "    def get_sampling_timesteps(self, batch, *, device):\n",
        "        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n",
        "        times = repeat(times, 't -> b t', b = batch)\n",
        "        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
        "        times = times.unbind(dim = -1)\n",
        "        return times\n",
        "\n",
        "    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n",
        "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n",
        "\n",
        "        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n",
        "        log_snr = self.log_snr(t)\n",
        "        log_snr_next = self.log_snr(t_next)\n",
        "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n",
        "\n",
        "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
        "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n",
        "\n",
        "        # c - as defined near eq 33\n",
        "        c = -expm1(log_snr - log_snr_next)\n",
        "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
        "\n",
        "        # following (eq. 33)\n",
        "        posterior_variance = (sigma_next ** 2) * c\n",
        "        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    def q_sample(self, x_start, t, noise = None):\n",
        "        dtype = x_start.dtype\n",
        "\n",
        "        if isinstance(t, float):\n",
        "            batch = x_start.shape[0]\n",
        "            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        log_snr = self.log_snr(t).type(dtype)\n",
        "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n",
        "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
        "\n",
        "        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n",
        "\n",
        "    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n",
        "        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n",
        "        batch = shape[0]\n",
        "\n",
        "        if isinstance(from_t, float):\n",
        "            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n",
        "\n",
        "        if isinstance(to_t, float):\n",
        "            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_from))\n",
        "\n",
        "        log_snr = self.log_snr(from_t)\n",
        "        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n",
        "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
        "\n",
        "        log_snr_to = self.log_snr(to_t)\n",
        "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n",
        "        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n",
        "\n",
        "        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n",
        "\n",
        "    def predict_start_from_v(self, x_t, t, v):\n",
        "        log_snr = self.log_snr(t)\n",
        "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
        "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
        "        return alpha * x_t - sigma * v\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        log_snr = self.log_snr(t)\n",
        "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
        "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
        "        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n",
        "\n",
        "# norms and residuals\n",
        "\n",
        "class ChanRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.gamma = nn.Parameter(torch.ones(dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = 1) * self.scale * self.gamma\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feats, stable = False, dim = -1):\n",
        "        super().__init__()\n",
        "        self.stable = stable\n",
        "        self.dim = dim\n",
        "\n",
        "        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        dtype, dim = x.dtype, self.dim\n",
        "\n",
        "        if self.stable:\n",
        "            x = x / x.amax(dim = dim, keepdim = True).detach()\n",
        "\n",
        "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
        "        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n",
        "        mean = torch.mean(x, dim = dim, keepdim = True)\n",
        "\n",
        "        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n",
        "\n",
        "ChanLayerNorm = partial(LayerNorm, dim = -3)\n",
        "\n",
        "class Always():\n",
        "    def __init__(self, val):\n",
        "        self.val = val\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.val\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class Parallel(nn.Module):\n",
        "    def __init__(self, *fns):\n",
        "        super().__init__()\n",
        "        self.fns = nn.ModuleList(fns)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [fn(x) for fn in self.fns]\n",
        "        return sum(outputs)\n",
        "\n",
        "# attention pooling\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        scale = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim, bias = False),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, latents, mask = None):\n",
        "        x = self.norm(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        b, h = x.shape[0], self.heads\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "\n",
        "        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n",
        "        kv_input = torch.cat((x, latents), dim = -2)\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "\n",
        "        # qk rmsnorm\n",
        "\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "\n",
        "        # similarities and masking\n",
        "\n",
        "        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, max_neg_value)\n",
        "\n",
        "        # attention\n",
        "\n",
        "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
        "        attn = attn.to(sim.dtype)\n",
        "\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        num_latents = 64,\n",
        "        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n",
        "        max_seq_len = 512,\n",
        "        ff_mult = 4\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.to_latents_from_mean_pooled_seq = None\n",
        "        if num_latents_mean_pooled > 0:\n",
        "            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n",
        "                LayerNorm(dim),\n",
        "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
        "                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        n, device = x.shape[1], x.device\n",
        "        pos_emb = self.pos_emb(torch.arange(n, device = device))\n",
        "\n",
        "        x_with_pos = x + pos_emb\n",
        "\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n",
        "\n",
        "        if exists(self.to_latents_from_mean_pooled_seq):\n",
        "            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n",
        "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
        "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x_with_pos, latents, mask = mask) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "# attention\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, *, dim_head = 64, heads = 8, context_dim = None, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim, bias = False), LayerNorm(dim),)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None, attn_bias = None):\n",
        "        b, n, device = *x.shape[:2], x.device\n",
        "        x = self.norm(x)\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n",
        "        # add null key / value for classifier free guidance in prior net\n",
        "\n",
        "        nk, nv = map(lambda t: repeat(t, 'd -> b 1 d', b = b), self.null_kv.unbind(dim = -2))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "\n",
        "        # add text conditioning, if present\n",
        "        if exists(context):\n",
        "            assert exists(self.to_context)\n",
        "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
        "            k = torch.cat((ck, k), dim = -2)\n",
        "            v = torch.cat((cv, v), dim = -2)\n",
        "\n",
        "        # qk rmsnorm\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "        # calculate query / key similarities\n",
        "        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n",
        "        # relative positional encoding (T5 style)\n",
        "        if exists(attn_bias): sim = sim + attn_bias\n",
        "\n",
        "        # masking\n",
        "        max_neg_value = -torch.finfo(sim.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = F.pad(mask, (1, 0), value = True)\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, max_neg_value)\n",
        "\n",
        "        # attention\n",
        "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
        "        attn = attn.to(sim.dtype)\n",
        "\n",
        "        # aggregate values\n",
        "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "# decoder\n",
        "\n",
        "def Upsample(dim, dim_out = None):\n",
        "    dim_out = default(dim_out, dim)\n",
        "\n",
        "    return nn.Sequential(nn.Upsample(scale_factor = 2, mode = 'nearest'), nn.Conv2d(dim, dim_out, 3, padding = 1))\n",
        "\n",
        "class PixelShuffleUpsample(nn.Module):\n",
        "    \"\"\"\n",
        "    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n",
        "    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, dim_out = None):\n",
        "        super().__init__()\n",
        "        dim_out = default(dim_out, dim)\n",
        "        conv = nn.Conv2d(dim, dim_out * 4, 1)\n",
        "        self.net = nn.Sequential(conv, nn.SiLU(), nn.PixelShuffle(2))\n",
        "        self.init_conv_(conv)\n",
        "\n",
        "    def init_conv_(self, conv):\n",
        "        o, i, h, w = conv.weight.shape\n",
        "        conv_weight = torch.empty(o // 4, i, h, w)\n",
        "        nn.init.kaiming_uniform_(conv_weight)\n",
        "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
        "\n",
        "        conv.weight.data.copy_(conv_weight)\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def Downsample(dim, dim_out = None):\n",
        "    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n",
        "    # named SP-conv in the paper, but basically a pixel unshuffle\n",
        "    dim_out = default(dim_out, dim)\n",
        "    return nn.Sequential(\n",
        "        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n",
        "        nn.Conv2d(dim * 4, dim_out, 1)\n",
        "    )\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n",
        "        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n",
        "        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n",
        "\n",
        "class LearnedSinusoidalPosEmb(nn.Module):\n",
        "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        assert (dim % 2) == 0\n",
        "        half_dim = dim // 2\n",
        "        self.weights = nn.Parameter(torch.randn(half_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, 'b -> b 1')\n",
        "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
        "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
        "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
        "        return fouriered\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_out,\n",
        "        norm = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm = ChanRMSNorm(dim) if norm else Identity()\n",
        "        self.activation = nn.SiLU()\n",
        "        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.activation(x)\n",
        "        return self.project(x)\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_out,\n",
        "        *,\n",
        "        cond_dim = None,\n",
        "        time_cond_dim = None,\n",
        "        linear_attn = False,\n",
        "        use_gca = False,\n",
        "        squeeze_excite = False,\n",
        "        **attn_kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.time_mlp = None\n",
        "        if exists(time_cond_dim):\n",
        "            self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_cond_dim, dim_out * 2),)\n",
        "        self.cross_attn = None\n",
        "        if exists(cond_dim):\n",
        "            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n",
        "            self.cross_attn = attn_klass(dim = dim_out, context_dim = cond_dim, **attn_kwargs)\n",
        "        self.block1 = Block(dim, dim_out)\n",
        "        self.block2 = Block(dim_out, dim_out)\n",
        "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None, cond = None):\n",
        "\n",
        "        scale_shift = None\n",
        "        if exists(self.time_mlp) and exists(time_emb):\n",
        "            time_emb = self.time_mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
        "            scale_shift = time_emb.chunk(2, dim = 1)\n",
        "\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.cross_attn):\n",
        "            assert exists(cond)\n",
        "            h = rearrange(h, 'b c h w -> b h w c')\n",
        "            h, ps = pack([h], 'b * c')\n",
        "            h = self.cross_attn(h, context = cond) + h\n",
        "            h, = unpack(h, ps, 'b * c')\n",
        "            h = rearrange(h, 'b h w c -> b c h w')\n",
        "\n",
        "        h = self.block2(h, scale_shift = scale_shift)\n",
        "\n",
        "        h = h * self.gca(h)\n",
        "\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        context_dim = None,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        norm_context = False,\n",
        "        scale = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        context_dim = default(context_dim, dim)\n",
        "\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n",
        "\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim, bias = False), LayerNorm(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context, mask = None):\n",
        "        b, n, device = *x.shape[:2], x.device\n",
        "        x = self.norm(x)\n",
        "        context = self.norm_context(context)\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n",
        "        # add null key / value for classifier free guidance in prior net\n",
        "        nk, nv = map(lambda t: repeat(t, 'd -> b h 1 d', h = self.heads,  b = b), self.null_kv.unbind(dim = -2))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "\n",
        "        # cosine sim attention\n",
        "\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "\n",
        "        # similarities\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        # masking\n",
        "\n",
        "        max_neg_value = -torch.finfo(sim.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = F.pad(mask, (1, 0), value = True)\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, max_neg_value)\n",
        "\n",
        "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
        "        attn = attn.to(sim.dtype)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearCrossAttention(CrossAttention):\n",
        "    def forward(self, x, context, mask = None):\n",
        "        b, n, device = *x.shape[:2], x.device\n",
        "        x = self.norm(x)\n",
        "        context = self.norm_context(context)\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = self.heads), (q, k, v))\n",
        "\n",
        "        # add null key / value for classifier free guidance in prior net\n",
        "\n",
        "        nk, nv = map(lambda t: repeat(t, 'd -> (b h) 1 d', h = self.heads,  b = b), self.null_kv.unbind(dim = -2))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "\n",
        "        # masking\n",
        "\n",
        "        max_neg_value = -torch.finfo(x.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = F.pad(mask, (1, 0), value = True)\n",
        "            mask = rearrange(mask, 'b n -> b n 1')\n",
        "            k = k.masked_fill(~mask, max_neg_value)\n",
        "            v = v.masked_fill(~mask, 0.)\n",
        "\n",
        "        # linear attention\n",
        "\n",
        "        q = q.softmax(dim = -1)\n",
        "        k = k.softmax(dim = -2)\n",
        "        q = q * self.scale\n",
        "        context = einsum('b n d, b n e -> b d e', k, v)\n",
        "        out = einsum('b n d, b d e -> b n e', q, context)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 32,\n",
        "        heads = 8,\n",
        "        dropout = 0.05,\n",
        "        context_dim = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "        self.norm = ChanLayerNorm(dim)\n",
        "\n",
        "        self.nonlin = nn.SiLU()\n",
        "\n",
        "        self.to_q = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
        "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
        "        )\n",
        "\n",
        "        self.to_k = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
        "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
        "        )\n",
        "\n",
        "        self.to_v = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
        "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
        "        )\n",
        "\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Conv2d(inner_dim, dim, 1, bias = False),\n",
        "            ChanLayerNorm(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, fmap, context = None):\n",
        "        h, x, y = self.heads, *fmap.shape[-2:]\n",
        "\n",
        "        fmap = self.norm(fmap)\n",
        "        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (q, k, v))\n",
        "\n",
        "        if exists(context):\n",
        "            assert exists(self.to_context)\n",
        "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
        "            ck, cv = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (ck, cv))\n",
        "            k = torch.cat((k, ck), dim = -2)\n",
        "            v = torch.cat((v, cv), dim = -2)\n",
        "\n",
        "        q = q.softmax(dim = -1)\n",
        "        k = k.softmax(dim = -2)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = einsum('b n d, b n e -> b d e', k, v)\n",
        "        out = einsum('b n d, b d e -> b n e', q, context)\n",
        "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
        "\n",
        "        out = self.nonlin(out)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GlobalContext(nn.Module):\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim_in,\n",
        "        dim_out\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        context = self.to_k(x)\n",
        "        x, context = map(lambda t: rearrange(t, 'b n ... -> b n (...)'), (x, context))\n",
        "        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n",
        "        out = rearrange(out, '... -> ... 1')\n",
        "        return self.net(out)\n",
        "\n",
        "def FeedForward(dim, mult = 2):\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        LayerNorm(dim),\n",
        "        nn.Linear(dim, hidden_dim, bias = False),\n",
        "        nn.GELU(),\n",
        "        LayerNorm(hidden_dim),\n",
        "        nn.Linear(hidden_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "def ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        ChanLayerNorm(dim),\n",
        "        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n",
        "        nn.GELU(),\n",
        "        ChanLayerNorm(hidden_dim),\n",
        "        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n",
        "    )\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth = 1,\n",
        "        heads = 8,\n",
        "        dim_head = 32,\n",
        "        ff_mult = 2,\n",
        "        context_dim = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, context = None):\n",
        "        x = rearrange(x, 'b c h w -> b h w c')\n",
        "        x, ps = pack([x], 'b * c')\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, context = context) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        x, = unpack(x, ps, 'b * c')\n",
        "        x = rearrange(x, 'b h w c -> b c h w')\n",
        "        return x\n",
        "\n",
        "class LinearAttentionTransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth = 1,\n",
        "        heads = 8,\n",
        "        dim_head = 32,\n",
        "        ff_mult = 2,\n",
        "        context_dim = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n",
        "                ChanFeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, context = None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, context = context) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_in,\n",
        "        kernel_sizes,\n",
        "        dim_out = None,\n",
        "        stride = 2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
        "        dim_out = default(dim_out, dim_in)\n",
        "\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "        num_scales = len(kernel_sizes)\n",
        "\n",
        "        # calculate the dimension at each scale\n",
        "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n",
        "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n",
        "\n",
        "        self.convs = nn.ModuleList([])\n",
        "        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n",
        "            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n",
        "        return torch.cat(fmaps, dim = 1)\n",
        "\n",
        "class UpsampleCombiner(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        enabled = False,\n",
        "        dim_ins = tuple(),\n",
        "        dim_outs = tuple()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n",
        "        assert len(dim_ins) == len(dim_outs)\n",
        "\n",
        "        self.enabled = enabled\n",
        "\n",
        "        if not self.enabled:\n",
        "            self.dim_out = dim\n",
        "            return\n",
        "\n",
        "        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n",
        "        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n",
        "\n",
        "    def forward(self, x, fmaps = None):\n",
        "        target_size = x.shape[-1]\n",
        "\n",
        "        fmaps = default(fmaps, tuple())\n",
        "\n",
        "        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n",
        "            return x\n",
        "\n",
        "        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n",
        "        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n",
        "        return torch.cat((x, *outs), dim = 1)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n",
        "        num_resnet_blocks = 1,\n",
        "        cond_dim = None,\n",
        "        num_image_tokens = 4,\n",
        "        num_time_tokens = 2,\n",
        "        learned_sinu_pos_emb_dim = 16,\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        cond_images_channels = 0,\n",
        "        channels = 3,\n",
        "        channels_out = None,\n",
        "        attn_dim_head = 64,\n",
        "        attn_heads = 8,\n",
        "        ff_mult = 2.,\n",
        "        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "        layer_attns = True,\n",
        "        layer_attns_depth = 1,\n",
        "        layer_mid_attns_depth = 1,\n",
        "        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "        layer_cross_attns = True,\n",
        "        use_linear_attn = False,\n",
        "        use_linear_cross_attn = False,\n",
        "        cond_on_text = True,\n",
        "        max_text_len = 256,\n",
        "        init_dim = None,\n",
        "        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n",
        "        init_cross_embed = True,\n",
        "        init_cross_embed_kernel_sizes = (3, 7, 15),\n",
        "        cross_embed_downsample = False,\n",
        "        cross_embed_downsample_kernel_sizes = (2, 4),\n",
        "        attn_pool_text = True,\n",
        "        attn_pool_num_latents = 32,\n",
        "        dropout = 0.,\n",
        "        memory_efficient = False,\n",
        "        init_conv_to_final_conv_residual = False,\n",
        "        use_global_context_attn = True,\n",
        "        scale_skip_connection = True,\n",
        "        final_resnet_block = True,\n",
        "        final_conv_kernel_size = 3,\n",
        "        self_cond = False,\n",
        "        resize_mode = 'nearest',\n",
        "        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # guide researchers\n",
        "\n",
        "        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n",
        "\n",
        "        if dim < 128:\n",
        "            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n",
        "\n",
        "        # save locals to take care of some hyperparameters for cascading DDPM\n",
        "\n",
        "        self._locals = locals()\n",
        "        self._locals.pop('self', None)\n",
        "        self._locals.pop('__class__', None)\n",
        "\n",
        "        # determine dimensions\n",
        "\n",
        "        self.channels = channels\n",
        "        self.channels_out = default(channels_out, channels)\n",
        "\n",
        "        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n",
        "        # (2) in self conditioning, one appends the predict x0 (x_start)\n",
        "        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n",
        "        init_dim = default(init_dim, dim)\n",
        "\n",
        "        self.self_cond = self_cond\n",
        "\n",
        "        # optional image conditioning\n",
        "\n",
        "        self.has_cond_image = cond_images_channels > 0\n",
        "        self.cond_images_channels = cond_images_channels\n",
        "\n",
        "        init_channels += cond_images_channels\n",
        "\n",
        "        # initial convolution\n",
        "\n",
        "        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        # time conditioning\n",
        "\n",
        "        cond_dim = default(cond_dim, dim)\n",
        "        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n",
        "\n",
        "        # embedding time for log(snr) noise from continuous version\n",
        "\n",
        "        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n",
        "        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n",
        "\n",
        "        self.to_time_hiddens = nn.Sequential(\n",
        "            sinu_pos_emb,\n",
        "            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        self.to_time_cond = nn.Sequential(\n",
        "            nn.Linear(time_cond_dim, time_cond_dim)\n",
        "        )\n",
        "\n",
        "        # project to time tokens as well as time hiddens\n",
        "\n",
        "        self.to_time_tokens = nn.Sequential(\n",
        "            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
        "            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
        "        )\n",
        "\n",
        "        # low res aug noise conditioning\n",
        "\n",
        "        self.lowres_cond = lowres_cond\n",
        "\n",
        "        if lowres_cond:\n",
        "            self.to_lowres_time_hiddens = nn.Sequential(\n",
        "                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n",
        "                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n",
        "                nn.SiLU()\n",
        "            )\n",
        "\n",
        "            self.to_lowres_time_cond = nn.Sequential(\n",
        "                nn.Linear(time_cond_dim, time_cond_dim)\n",
        "            )\n",
        "\n",
        "            self.to_lowres_time_tokens = nn.Sequential(\n",
        "                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
        "                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
        "            )\n",
        "\n",
        "        # normalizations\n",
        "\n",
        "        self.norm_cond = nn.LayerNorm(cond_dim)\n",
        "\n",
        "        # text encoding conditioning (optional)\n",
        "\n",
        "        self.text_to_cond = None\n",
        "\n",
        "        if cond_on_text:\n",
        "            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n",
        "            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n",
        "\n",
        "        # finer control over whether to condition on text encodings\n",
        "\n",
        "        self.cond_on_text = cond_on_text\n",
        "\n",
        "        # attention pooling\n",
        "\n",
        "        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n",
        "\n",
        "        # for classifier free guidance\n",
        "\n",
        "        self.max_text_len = max_text_len\n",
        "\n",
        "        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n",
        "        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n",
        "\n",
        "        # for non-attention based text conditioning at all points in the network where time is also conditioned\n",
        "\n",
        "        self.to_text_non_attn_cond = None\n",
        "\n",
        "        if cond_on_text:\n",
        "            self.to_text_non_attn_cond = nn.Sequential(\n",
        "                nn.LayerNorm(cond_dim),\n",
        "                nn.Linear(cond_dim, time_cond_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(time_cond_dim, time_cond_dim)\n",
        "            )\n",
        "\n",
        "        # attention related params\n",
        "\n",
        "        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n",
        "\n",
        "        num_layers = len(in_out)\n",
        "\n",
        "        # resnet block klass\n",
        "\n",
        "        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n",
        "\n",
        "        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n",
        "\n",
        "        layer_attns = cast_tuple(layer_attns, num_layers)\n",
        "        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n",
        "        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n",
        "\n",
        "        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n",
        "        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n",
        "\n",
        "        assert all([layers == num_layers for layers in list(map(len, (layer_attns, layer_cross_attns)))])\n",
        "\n",
        "        # downsample klass\n",
        "\n",
        "        downsample_klass = Downsample\n",
        "\n",
        "        if cross_embed_downsample:\n",
        "            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n",
        "\n",
        "        # initial resnet block (for memory efficient unet)\n",
        "\n",
        "        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, use_gca = use_global_context_attn) if memory_efficient else None\n",
        "\n",
        "        # scale for resnet skip connections\n",
        "\n",
        "        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n",
        "\n",
        "        # layers\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        layer_params = [num_resnet_blocks, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n",
        "        reversed_layer_params = list(map(reversed, layer_params))\n",
        "\n",
        "        # downsampling layers\n",
        "\n",
        "        skip_connect_dims = [] # keep track of skip connection dimensions\n",
        "\n",
        "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
        "\n",
        "            if layer_attn:\n",
        "                transformer_block_klass = TransformerBlock\n",
        "            elif layer_use_linear_attn:\n",
        "                transformer_block_klass = LinearAttentionTransformerBlock\n",
        "            else:\n",
        "                transformer_block_klass = Identity\n",
        "\n",
        "            current_dim = dim_in\n",
        "\n",
        "            # whether to pre-downsample, from memory efficient unet\n",
        "\n",
        "            pre_downsample = None\n",
        "\n",
        "            if memory_efficient:\n",
        "                pre_downsample = downsample_klass(dim_in, dim_out)\n",
        "                current_dim = dim_out\n",
        "\n",
        "            skip_connect_dims.append(current_dim)\n",
        "\n",
        "            # whether to do post-downsample, for non-memory efficient unet\n",
        "\n",
        "            post_downsample = None\n",
        "            if not memory_efficient:\n",
        "                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n",
        "\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                pre_downsample,\n",
        "                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim),\n",
        "                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
        "                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n",
        "                post_downsample\n",
        "            ]))\n",
        "\n",
        "        # middle layers\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "\n",
        "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim)\n",
        "        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n",
        "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim)\n",
        "\n",
        "        # upsample klass\n",
        "\n",
        "        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n",
        "\n",
        "        # upsampling layers\n",
        "\n",
        "        upsample_fmap_dims = []\n",
        "\n",
        "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n",
        "            is_last = ind == (len(in_out) - 1)\n",
        "\n",
        "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
        "\n",
        "            if layer_attn:\n",
        "                transformer_block_klass = TransformerBlock\n",
        "            elif layer_use_linear_attn:\n",
        "                transformer_block_klass = LinearAttentionTransformerBlock\n",
        "            else:\n",
        "                transformer_block_klass = Identity\n",
        "\n",
        "            skip_connect_dim = skip_connect_dims.pop()\n",
        "\n",
        "            upsample_fmap_dims.append(dim_out)\n",
        "\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim),\n",
        "                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
        "                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n",
        "                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n",
        "            ]))\n",
        "\n",
        "        # whether to combine feature maps from all upsample blocks before final resnet block out\n",
        "\n",
        "        self.upsample_combiner = UpsampleCombiner(\n",
        "            dim = dim,\n",
        "            enabled = combine_upsample_fmaps,\n",
        "            dim_ins = upsample_fmap_dims,\n",
        "            dim_outs = dim\n",
        "        )\n",
        "\n",
        "        # whether to do a final residual from initial conv to the final resnet block out\n",
        "\n",
        "        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n",
        "        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n",
        "\n",
        "        # final optional resnet block and convolution out\n",
        "\n",
        "        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, use_gca = True) if final_resnet_block else None\n",
        "\n",
        "        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n",
        "        final_conv_dim_in += (channels if lowres_cond else 0)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n",
        "\n",
        "        zero_init_(self.final_conv)\n",
        "\n",
        "        # resize mode\n",
        "\n",
        "        self.resize_mode = resize_mode\n",
        "\n",
        "\n",
        "    # forward with classifier free guidance\n",
        "\n",
        "    def forward_with_cond_scale(\n",
        "        self,\n",
        "        *args,\n",
        "        cond_scale = 1.,\n",
        "        remove_parallel_component = True,\n",
        "        keep_parallel_frac = 0.,\n",
        "        **kwargs\n",
        "    ):\n",
        "        logits = self.forward(*args, **kwargs)\n",
        "\n",
        "        if cond_scale == 1:\n",
        "            return logits\n",
        "\n",
        "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n",
        "\n",
        "        update = (logits - null_logits)\n",
        "\n",
        "        if remove_parallel_component:\n",
        "            parallel, orthogonal = project(update, logits)\n",
        "            update = orthogonal + parallel * keep_parallel_frac\n",
        "\n",
        "        return logits + update * (cond_scale - 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        time,\n",
        "        *,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_noise_times = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        self_cond = None,\n",
        "        cond_drop_prob = 0.\n",
        "    ):\n",
        "        batch_size, device = x.shape[0], x.device\n",
        "\n",
        "        # condition on self\n",
        "\n",
        "        if self.self_cond:\n",
        "            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n",
        "            x = torch.cat((x, self_cond), dim = 1)\n",
        "\n",
        "        # add low resolution conditioning, if present\n",
        "\n",
        "        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n",
        "        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n",
        "\n",
        "        if exists(lowres_cond_img):\n",
        "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
        "\n",
        "        # condition on input image\n",
        "\n",
        "        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n",
        "\n",
        "        if exists(cond_images):\n",
        "            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n",
        "            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n",
        "            x = torch.cat((cond_images, x), dim = 1)\n",
        "\n",
        "        # initial convolution\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # init conv residual\n",
        "\n",
        "        if self.init_conv_to_final_conv_residual:\n",
        "            init_conv_residual = x.clone()\n",
        "\n",
        "        # time conditioning\n",
        "\n",
        "        time_hiddens = self.to_time_hiddens(time)\n",
        "\n",
        "        # derive time tokens\n",
        "\n",
        "        time_tokens = self.to_time_tokens(time_hiddens)\n",
        "        t = self.to_time_cond(time_hiddens)\n",
        "\n",
        "        # add lowres time conditioning to time hiddens\n",
        "        # and add lowres time tokens along sequence dimension for attention\n",
        "\n",
        "        if self.lowres_cond:\n",
        "            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n",
        "            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n",
        "            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n",
        "\n",
        "            t = t + lowres_t\n",
        "            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n",
        "\n",
        "        # text conditioning\n",
        "\n",
        "        text_tokens = None\n",
        "\n",
        "        if exists(text_embeds) and self.cond_on_text:\n",
        "\n",
        "            # conditional dropout\n",
        "\n",
        "            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n",
        "\n",
        "            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n",
        "            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n",
        "\n",
        "            # calculate text embeds\n",
        "\n",
        "            text_tokens = self.text_to_cond(text_embeds)\n",
        "\n",
        "            text_tokens = text_tokens[:, :self.max_text_len]\n",
        "\n",
        "            if exists(text_mask):\n",
        "                text_mask = text_mask[:, :self.max_text_len]\n",
        "\n",
        "            text_tokens_len = text_tokens.shape[1]\n",
        "            remainder = self.max_text_len - text_tokens_len\n",
        "\n",
        "            if remainder > 0:\n",
        "                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n",
        "\n",
        "            if exists(text_mask):\n",
        "                if remainder > 0:\n",
        "                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n",
        "\n",
        "                text_mask = rearrange(text_mask, 'b n -> b n 1')\n",
        "                text_keep_mask_embed = text_mask & text_keep_mask_embed\n",
        "\n",
        "            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n",
        "\n",
        "            text_tokens = torch.where(\n",
        "                text_keep_mask_embed,\n",
        "                text_tokens,\n",
        "                null_text_embed\n",
        "            )\n",
        "\n",
        "            if exists(self.attn_pool):\n",
        "                text_tokens = self.attn_pool(text_tokens)\n",
        "\n",
        "            # extra non-attention conditioning by projecting and then summing text embeddings to time\n",
        "            # termed as text hiddens\n",
        "\n",
        "            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n",
        "\n",
        "            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n",
        "\n",
        "            null_text_hidden = self.null_text_hidden.to(t.dtype)\n",
        "\n",
        "            text_hiddens = torch.where(\n",
        "                text_keep_mask_hidden,\n",
        "                text_hiddens,\n",
        "                null_text_hidden\n",
        "            )\n",
        "\n",
        "            t = t + text_hiddens\n",
        "\n",
        "        # main conditioning tokens (c)\n",
        "\n",
        "        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n",
        "\n",
        "        # normalize conditioning tokens\n",
        "\n",
        "        c = self.norm_cond(c)\n",
        "\n",
        "        # initial resnet block (for memory efficient unet)\n",
        "\n",
        "        if exists(self.init_resnet_block):\n",
        "            x = self.init_resnet_block(x, t)\n",
        "\n",
        "        # go through the layers of the unet, down and up\n",
        "\n",
        "        hiddens = []\n",
        "\n",
        "        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "            if exists(pre_downsample):\n",
        "                x = pre_downsample(x)\n",
        "\n",
        "            x = init_block(x, t, c)\n",
        "\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = resnet_block(x, t)\n",
        "                hiddens.append(x)\n",
        "\n",
        "            x = attn_block(x, c)\n",
        "            hiddens.append(x)\n",
        "\n",
        "            if exists(post_downsample):\n",
        "                x = post_downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t, c)\n",
        "\n",
        "        if exists(self.mid_attn):\n",
        "            x = self.mid_attn(x)\n",
        "\n",
        "        x = self.mid_block2(x, t, c)\n",
        "\n",
        "        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n",
        "\n",
        "        up_hiddens = []\n",
        "\n",
        "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
        "            x = add_skip_connection(x)\n",
        "            x = init_block(x, t, c)\n",
        "\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = add_skip_connection(x)\n",
        "                x = resnet_block(x, t)\n",
        "\n",
        "            x = attn_block(x, c)\n",
        "            up_hiddens.append(x.contiguous())\n",
        "            x = upsample(x)\n",
        "\n",
        "        # whether to combine all feature maps from upsample blocks\n",
        "\n",
        "        x = self.upsample_combiner(x, up_hiddens)\n",
        "\n",
        "        # final top-most residual if needed\n",
        "\n",
        "        if self.init_conv_to_final_conv_residual:\n",
        "            x = torch.cat((x, init_conv_residual), dim = 1)\n",
        "\n",
        "        if exists(self.final_res_block):\n",
        "            x = self.final_res_block(x, t)\n",
        "\n",
        "        if exists(lowres_cond_img):\n",
        "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "# null unet\n",
        "\n",
        "class NullUnet(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.lowres_cond = False\n",
        "        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def cast_model_parameters(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "# predefined unets, with configs lining up with hyperparameters in appendix of paper\n",
        "\n",
        "class BaseUnet64(Unet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        default_kwargs = dict(\n",
        "            dim = 512,\n",
        "            dim_mults = (1, 2, 3, 4),\n",
        "            num_resnet_blocks = 3,\n",
        "            layer_attns = (False, True, True, True),\n",
        "            layer_cross_attns = (False, True, True, True),\n",
        "            attn_heads = 8,\n",
        "            ff_mult = 2.,\n",
        "            memory_efficient = False\n",
        "        )\n",
        "        super().__init__(*args, **{**default_kwargs, **kwargs})\n",
        "\n",
        "class SRUnet256(Unet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        default_kwargs = dict(\n",
        "            dim = 128,\n",
        "            dim_mults = (1, 2, 4, 8),\n",
        "            num_resnet_blocks = (2, 4, 8, 8),\n",
        "            layer_attns = (False, False, False, True),\n",
        "            layer_cross_attns = (False, False, False, True),\n",
        "            attn_heads = 8,\n",
        "            ff_mult = 2.,\n",
        "            memory_efficient = True\n",
        "        )\n",
        "        super().__init__(*args, **{**default_kwargs, **kwargs})\n",
        "\n",
        "class SRUnet1024(Unet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        default_kwargs = dict(\n",
        "            dim = 128,\n",
        "            dim_mults = (1, 2, 4, 8),\n",
        "            num_resnet_blocks = (2, 4, 8, 8),\n",
        "            layer_attns = False,\n",
        "            layer_cross_attns = (False, False, False, True),\n",
        "            attn_heads = 8,\n",
        "            ff_mult = 2.,\n",
        "            memory_efficient = True\n",
        "        )\n",
        "        super().__init__(*args, **{**default_kwargs, **kwargs})\n",
        "\n",
        "# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n",
        "\n",
        "class Imagen(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unets,\n",
        "        *,\n",
        "        image_sizes,                                # for cascading ddpm, image size at each stage\n",
        "        text_encoder_name = DEFAULT_T5_NAME,\n",
        "        text_embed_dim = None,\n",
        "        channels = 3,\n",
        "        timesteps = 1000,\n",
        "        cond_drop_prob = 0.1,\n",
        "        loss_type = 'l2',\n",
        "        noise_schedules = 'cosine',\n",
        "        pred_objectives = 'noise',\n",
        "        random_crop_sizes = None,\n",
        "        lowres_noise_schedule = 'linear',\n",
        "        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n",
        "        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n",
        "        condition_on_text = True,\n",
        "        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n",
        "        dynamic_thresholding = True,\n",
        "        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n",
        "        only_train_unet_number = None,\n",
        "        temporal_downsample_factor = 1,\n",
        "        resize_cond_video_frames = True,\n",
        "        resize_mode = 'nearest',\n",
        "        min_snr_loss_weight = True,                 # https://arxiv.org/abs/2303.09556\n",
        "        min_snr_gamma = 5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # loss\n",
        "\n",
        "        if loss_type == 'l1':\n",
        "            loss_fn = F.l1_loss\n",
        "        elif loss_type == 'l2':\n",
        "            loss_fn = F.mse_loss\n",
        "        elif loss_type == 'huber':\n",
        "            loss_fn = F.smooth_l1_loss\n",
        "\n",
        "        self.loss_type = loss_type\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        # conditioning hparams\n",
        "\n",
        "        self.condition_on_text = condition_on_text\n",
        "        self.unconditional = not condition_on_text\n",
        "\n",
        "        # channels\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "        # automatically take care of ensuring that first unet is unconditional\n",
        "        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n",
        "\n",
        "        unets = cast_tuple(unets)\n",
        "        num_unets = len(unets)\n",
        "\n",
        "        # determine noise schedules per unet\n",
        "\n",
        "        timesteps = cast_tuple(timesteps, num_unets)\n",
        "\n",
        "        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n",
        "\n",
        "        noise_schedules = cast_tuple(noise_schedules)\n",
        "        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n",
        "        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n",
        "\n",
        "        # construct noise schedulers\n",
        "\n",
        "        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n",
        "        self.noise_schedulers = nn.ModuleList([])\n",
        "\n",
        "        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n",
        "            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n",
        "            self.noise_schedulers.append(noise_scheduler)\n",
        "\n",
        "        # randomly cropping for upsampler training\n",
        "\n",
        "        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n",
        "        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n",
        "\n",
        "        # lowres augmentation noise schedule\n",
        "\n",
        "        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n",
        "\n",
        "        # ddpm objectives - predicting noise by default\n",
        "\n",
        "        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n",
        "\n",
        "        # get text encoder\n",
        "\n",
        "        self.text_encoder_name = text_encoder_name\n",
        "        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n",
        "\n",
        "        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n",
        "\n",
        "        # construct unets\n",
        "\n",
        "        self.unets = nn.ModuleList([])\n",
        "\n",
        "        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n",
        "        self.only_train_unet_number = only_train_unet_number\n",
        "\n",
        "        for ind, one_unet in enumerate(unets):\n",
        "            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n",
        "            is_first = ind == 0\n",
        "\n",
        "            one_unet = one_unet.cast_model_parameters(\n",
        "                lowres_cond = not is_first,\n",
        "                cond_on_text = self.condition_on_text,\n",
        "                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n",
        "                channels = self.channels,\n",
        "                channels_out = self.channels\n",
        "            )\n",
        "\n",
        "            self.unets.append(one_unet)\n",
        "\n",
        "        # unet image sizes\n",
        "\n",
        "        image_sizes = cast_tuple(image_sizes)\n",
        "        self.image_sizes = image_sizes\n",
        "\n",
        "        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n",
        "\n",
        "        self.sample_channels = cast_tuple(self.channels, num_unets)\n",
        "\n",
        "        # determine whether we are training on images or video\n",
        "\n",
        "        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n",
        "        self.is_video = is_video\n",
        "\n",
        "        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n",
        "\n",
        "        self.resize_to = resize_video_to if is_video else resize_image_to\n",
        "        self.resize_to = partial(self.resize_to, mode = resize_mode)\n",
        "\n",
        "        # temporal interpolation\n",
        "\n",
        "        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n",
        "        self.temporal_downsample_factor = temporal_downsample_factor\n",
        "\n",
        "        self.resize_cond_video_frames = resize_cond_video_frames\n",
        "        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n",
        "\n",
        "        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n",
        "        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n",
        "\n",
        "        # cascading ddpm related stuff\n",
        "\n",
        "        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n",
        "        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n",
        "\n",
        "        self.lowres_sample_noise_level = lowres_sample_noise_level\n",
        "        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n",
        "\n",
        "        # classifier free guidance\n",
        "\n",
        "        self.cond_drop_prob = cond_drop_prob\n",
        "        self.can_classifier_guidance = cond_drop_prob > 0.\n",
        "\n",
        "        # normalize and unnormalize image functions\n",
        "\n",
        "        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n",
        "        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n",
        "        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n",
        "\n",
        "        # dynamic thresholding\n",
        "\n",
        "        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n",
        "        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n",
        "\n",
        "        # min snr loss weight\n",
        "\n",
        "        min_snr_loss_weight = cast_tuple(min_snr_loss_weight, num_unets)\n",
        "        min_snr_gamma = cast_tuple(min_snr_gamma, num_unets)\n",
        "\n",
        "        assert len(min_snr_loss_weight) == len(min_snr_gamma) == num_unets\n",
        "        self.min_snr_gamma = tuple((gamma if use_min_snr else None) for use_min_snr, gamma in zip(min_snr_loss_weight, min_snr_gamma))\n",
        "\n",
        "        # one temp parameter for keeping track of device\n",
        "\n",
        "        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n",
        "\n",
        "        # default to device of unets passed in\n",
        "\n",
        "        self.to(next(self.unets.parameters()).device)\n",
        "\n",
        "    def force_unconditional_(self):\n",
        "        self.condition_on_text = False\n",
        "        self.unconditional = True\n",
        "\n",
        "        for unet in self.unets:\n",
        "            unet.cond_on_text = False\n",
        "\n",
        "\n",
        "    # gaussian diffusion methods\n",
        "\n",
        "    def p_mean_variance(\n",
        "        self,\n",
        "        unet,\n",
        "        x,\n",
        "        t,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        lowres_cond_img = None,\n",
        "        self_cond = None,\n",
        "        lowres_noise_times = None,\n",
        "        cond_scale = 1.,\n",
        "        cfg_remove_parallel_component = True,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        model_output = None,\n",
        "        t_next = None,\n",
        "        pred_objective = 'noise',\n",
        "        dynamic_threshold = True\n",
        "    ):\n",
        "        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n",
        "\n",
        "        video_kwargs = dict()\n",
        "        if self.is_video:\n",
        "            video_kwargs = dict(\n",
        "                cond_video_frames = cond_video_frames,\n",
        "                post_cond_video_frames = post_cond_video_frames,\n",
        "            )\n",
        "\n",
        "        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n",
        "            x,\n",
        "            noise_scheduler.get_condition(t),\n",
        "            text_embeds = text_embeds,\n",
        "            text_mask = text_mask,\n",
        "            cond_images = cond_images,\n",
        "            cond_scale = cond_scale,\n",
        "            remove_parallel_component = cfg_remove_parallel_component,\n",
        "            keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "            lowres_cond_img = lowres_cond_img,\n",
        "            self_cond = self_cond,\n",
        "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),\n",
        "            **video_kwargs\n",
        "        ))\n",
        "\n",
        "        if pred_objective == 'noise':\n",
        "            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n",
        "        elif pred_objective == 'x_start':\n",
        "            x_start = pred\n",
        "        elif pred_objective == 'v':\n",
        "            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n",
        "        else:\n",
        "            raise ValueError(f'unknown objective {pred_objective}')\n",
        "\n",
        "        if dynamic_threshold:\n",
        "            # following pseudocode in appendix\n",
        "            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n",
        "            s = torch.quantile(\n",
        "                rearrange(x_start, 'b ... -> b (...)').abs(),\n",
        "                self.dynamic_thresholding_percentile,\n",
        "                dim = -1\n",
        "            )\n",
        "\n",
        "            s.clamp_(min = 1.)\n",
        "            s = right_pad_dims_to(x_start, s)\n",
        "            x_start = x_start.clamp(-s, s) / s\n",
        "        else:\n",
        "            x_start.clamp_(-1., 1.)\n",
        "\n",
        "        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n",
        "        return mean_and_variance, x_start\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(\n",
        "        self,\n",
        "        unet,\n",
        "        x,\n",
        "        t,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        t_next = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        cond_scale = 1.,\n",
        "        cfg_remove_parallel_component = True,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        self_cond = None,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_noise_times = None,\n",
        "        pred_objective = 'noise',\n",
        "        dynamic_threshold = True\n",
        "    ):\n",
        "        b, *_, device = *x.shape, x.device\n",
        "\n",
        "        video_kwargs = dict()\n",
        "        if self.is_video:\n",
        "            video_kwargs = dict(\n",
        "                cond_video_frames = cond_video_frames,\n",
        "                post_cond_video_frames = post_cond_video_frames,\n",
        "            )\n",
        "\n",
        "        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n",
        "            unet,\n",
        "            x = x,\n",
        "            t = t,\n",
        "            t_next = t_next,\n",
        "            noise_scheduler = noise_scheduler,\n",
        "            text_embeds = text_embeds,\n",
        "            text_mask = text_mask,\n",
        "            cond_images = cond_images,\n",
        "            cond_scale = cond_scale,\n",
        "            cfg_remove_parallel_component = cfg_remove_parallel_component,\n",
        "            cfg_keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "            lowres_cond_img = lowres_cond_img,\n",
        "            self_cond = self_cond,\n",
        "            lowres_noise_times = lowres_noise_times,\n",
        "            pred_objective = pred_objective,\n",
        "            dynamic_threshold = dynamic_threshold,\n",
        "            **video_kwargs\n",
        "        )\n",
        "\n",
        "        noise = torch.randn_like(x)\n",
        "        # no noise when t == 0\n",
        "        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n",
        "        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
        "        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
        "        return pred, x_start\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(\n",
        "        self,\n",
        "        unet,\n",
        "        shape,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_noise_times = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        inpaint_images = None,\n",
        "        inpaint_videos = None,\n",
        "        inpaint_masks = None,\n",
        "        inpaint_resample_times = 5,\n",
        "        init_images = None,\n",
        "        skip_steps = None,\n",
        "        cond_scale = 1,\n",
        "        cfg_remove_parallel_component = False,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        pred_objective = 'noise',\n",
        "        dynamic_threshold = True,\n",
        "        use_tqdm = True\n",
        "    ):\n",
        "        device = self.device\n",
        "\n",
        "        batch = shape[0]\n",
        "        img = torch.randn(shape, device = device)\n",
        "\n",
        "        # video\n",
        "\n",
        "        is_video = len(shape) == 5\n",
        "        frames = shape[-3] if is_video else None\n",
        "        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n",
        "\n",
        "        # for initialization with an image or video\n",
        "\n",
        "        if exists(init_images):\n",
        "            img += init_images\n",
        "\n",
        "        # keep track of x0, for self conditioning\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        # prepare inpainting\n",
        "\n",
        "        inpaint_images = default(inpaint_videos, inpaint_images)\n",
        "\n",
        "        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n",
        "        resample_times = inpaint_resample_times if has_inpainting else 1\n",
        "\n",
        "        if has_inpainting:\n",
        "            inpaint_images = self.normalize_img(inpaint_images)\n",
        "            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n",
        "            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1], **resize_kwargs).bool()\n",
        "\n",
        "        # time\n",
        "\n",
        "        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n",
        "\n",
        "        # whether to skip any steps\n",
        "\n",
        "        skip_steps = default(skip_steps, 0)\n",
        "        timesteps = timesteps[skip_steps:]\n",
        "\n",
        "        # video conditioning kwargs\n",
        "\n",
        "        video_kwargs = dict()\n",
        "        if self.is_video:\n",
        "            video_kwargs = dict(\n",
        "                cond_video_frames = cond_video_frames,\n",
        "                post_cond_video_frames = post_cond_video_frames,\n",
        "            )\n",
        "\n",
        "        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n",
        "            is_last_timestep = times_next == 0\n",
        "\n",
        "            for r in reversed(range(resample_times)):\n",
        "                is_last_resample_step = r == 0\n",
        "\n",
        "                if has_inpainting:\n",
        "                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n",
        "                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n",
        "\n",
        "                self_cond = x_start if unet.self_cond else None\n",
        "\n",
        "                img, x_start = self.p_sample(\n",
        "                    unet,\n",
        "                    img,\n",
        "                    times,\n",
        "                    t_next = times_next,\n",
        "                    text_embeds = text_embeds,\n",
        "                    text_mask = text_mask,\n",
        "                    cond_images = cond_images,\n",
        "                    cond_scale = cond_scale,\n",
        "                    cfg_remove_parallel_component = cfg_remove_parallel_component,\n",
        "                    cfg_keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "                    self_cond = self_cond,\n",
        "                    lowres_cond_img = lowres_cond_img,\n",
        "                    lowres_noise_times = lowres_noise_times,\n",
        "                    noise_scheduler = noise_scheduler,\n",
        "                    pred_objective = pred_objective,\n",
        "                    dynamic_threshold = dynamic_threshold,\n",
        "                    **video_kwargs\n",
        "                )\n",
        "\n",
        "                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n",
        "                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n",
        "\n",
        "                    img = torch.where(\n",
        "                        self.right_pad_dims_to_datatype(is_last_timestep),\n",
        "                        img,\n",
        "                        renoised_img\n",
        "                    )\n",
        "\n",
        "        img.clamp_(-1., 1.)\n",
        "\n",
        "        # final inpainting\n",
        "\n",
        "        if has_inpainting:\n",
        "            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n",
        "\n",
        "        unnormalize_img = self.unnormalize_img(img)\n",
        "        return unnormalize_img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    @eval_decorator\n",
        "    @beartype\n",
        "    def sample(\n",
        "        self,\n",
        "        texts: Optional[List[str]] = None,\n",
        "        text_masks = None,\n",
        "        text_embeds = None,\n",
        "        video_frames = None,\n",
        "        cond_images = None,\n",
        "        cond_video_frames = None,\n",
        "        post_cond_video_frames = None,\n",
        "        inpaint_videos = None,\n",
        "        inpaint_images = None,\n",
        "        inpaint_masks = None,\n",
        "        inpaint_resample_times = 5,\n",
        "        init_images = None,\n",
        "        skip_steps = None,\n",
        "        batch_size = 1,\n",
        "        cond_scale = 1.,\n",
        "        cfg_remove_parallel_component = True,\n",
        "        cfg_keep_parallel_frac = 0.,\n",
        "        lowres_sample_noise_level = None,\n",
        "        start_at_unet_number = 1,\n",
        "        start_image_or_video = None,\n",
        "        stop_at_unet_number = None,\n",
        "        return_all_unet_outputs = False,\n",
        "        return_pil_images = False,\n",
        "        device = None,\n",
        "        use_tqdm = True,\n",
        "        use_one_unet_in_gpu = True\n",
        "    ):\n",
        "        device = default(device, self.device)\n",
        "        self.reset_unets_all_one_device(device = device)\n",
        "\n",
        "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n",
        "\n",
        "        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n",
        "            assert all([*map(len, texts)]), 'text cannot be empty'\n",
        "\n",
        "            with autocast('cuda', enabled = False):\n",
        "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n",
        "\n",
        "            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n",
        "\n",
        "        if not self.unconditional:\n",
        "            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n",
        "\n",
        "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n",
        "            batch_size = text_embeds.shape[0]\n",
        "\n",
        "        # inpainting\n",
        "\n",
        "        inpaint_images = default(inpaint_videos, inpaint_images)\n",
        "\n",
        "        if exists(inpaint_images):\n",
        "            if self.unconditional:\n",
        "                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n",
        "                    batch_size = inpaint_images.shape[0]\n",
        "\n",
        "            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n",
        "            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n",
        "\n",
        "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n",
        "        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n",
        "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n",
        "\n",
        "        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        is_cuda = next(self.parameters()).is_cuda\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n",
        "\n",
        "        num_unets = len(self.unets)\n",
        "\n",
        "        # condition scaling\n",
        "\n",
        "        cond_scale = cast_tuple(cond_scale, num_unets)\n",
        "\n",
        "        # add frame dimension for video\n",
        "\n",
        "        if self.is_video and exists(inpaint_images):\n",
        "            video_frames = inpaint_images.shape[2]\n",
        "\n",
        "            if inpaint_masks.ndim == 3:\n",
        "                inpaint_masks = repeat(inpaint_masks, 'b h w -> b f h w', f = video_frames)\n",
        "\n",
        "            assert inpaint_masks.shape[1] == video_frames\n",
        "\n",
        "        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n",
        "\n",
        "        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n",
        "\n",
        "        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n",
        "\n",
        "        # for initial image and skipping steps\n",
        "\n",
        "        init_images = cast_tuple(init_images, num_unets)\n",
        "        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n",
        "\n",
        "        skip_steps = cast_tuple(skip_steps, num_unets)\n",
        "\n",
        "        # handle starting at a unet greater than 1, for training only-upscaler training\n",
        "\n",
        "        if start_at_unet_number > 1:\n",
        "            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n",
        "            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n",
        "            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n",
        "\n",
        "            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n",
        "            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n",
        "            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n",
        "\n",
        "\n",
        "        # go through each unet in cascade\n",
        "\n",
        "        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):\n",
        "\n",
        "            if unet_number < start_at_unet_number:\n",
        "                continue\n",
        "\n",
        "            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n",
        "\n",
        "            context = self.one_unet_in_gpu(unet = unet) if is_cuda and use_one_unet_in_gpu else nullcontext()\n",
        "\n",
        "            with context:\n",
        "                # video kwargs\n",
        "\n",
        "                video_kwargs = dict()\n",
        "                if self.is_video:\n",
        "                    video_kwargs = dict(\n",
        "                        cond_video_frames = cond_video_frames,\n",
        "                        post_cond_video_frames = post_cond_video_frames,\n",
        "                    )\n",
        "\n",
        "                    video_kwargs = compact(video_kwargs)\n",
        "\n",
        "                if self.is_video and self.resize_cond_video_frames:\n",
        "                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n",
        "                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n",
        "\n",
        "                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n",
        "                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n",
        "\n",
        "                # low resolution conditioning\n",
        "\n",
        "                lowres_cond_img = lowres_noise_times = None\n",
        "                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n",
        "\n",
        "                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n",
        "\n",
        "                if unet.lowres_cond:\n",
        "                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n",
        "\n",
        "                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n",
        "\n",
        "                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n",
        "                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n",
        "\n",
        "                # init images or video\n",
        "\n",
        "                if exists(unet_init_images):\n",
        "                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n",
        "\n",
        "                # shape of stage\n",
        "\n",
        "                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n",
        "\n",
        "                img = self.p_sample_loop(\n",
        "                    unet,\n",
        "                    shape,\n",
        "                    text_embeds = text_embeds,\n",
        "                    text_mask = text_masks,\n",
        "                    cond_images = cond_images,\n",
        "                    inpaint_images = inpaint_images,\n",
        "                    inpaint_masks = inpaint_masks,\n",
        "                    inpaint_resample_times = inpaint_resample_times,\n",
        "                    init_images = unet_init_images,\n",
        "                    skip_steps = unet_skip_steps,\n",
        "                    cond_scale = unet_cond_scale,\n",
        "                    cfg_remove_parallel_component = cfg_remove_parallel_component,\n",
        "                    cfg_keep_parallel_frac = cfg_keep_parallel_frac,\n",
        "                    lowres_cond_img = lowres_cond_img,\n",
        "                    lowres_noise_times = lowres_noise_times,\n",
        "                    noise_scheduler = noise_scheduler,\n",
        "                    pred_objective = pred_objective,\n",
        "                    dynamic_threshold = dynamic_threshold,\n",
        "                    use_tqdm = use_tqdm,\n",
        "                    **video_kwargs\n",
        "                )\n",
        "\n",
        "                outputs.append(img)\n",
        "\n",
        "            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n",
        "                break\n",
        "\n",
        "        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n",
        "\n",
        "        if not return_pil_images:\n",
        "            return outputs[output_index]\n",
        "\n",
        "        if not return_all_unet_outputs:\n",
        "            outputs = outputs[-1:]\n",
        "\n",
        "        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n",
        "\n",
        "        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n",
        "\n",
        "        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n",
        "\n",
        "    @beartype\n",
        "    def p_losses(\n",
        "        self,\n",
        "        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n",
        "        x_start,\n",
        "        times,\n",
        "        *,\n",
        "        noise_scheduler,\n",
        "        lowres_cond_img = None,\n",
        "        lowres_aug_times = None,\n",
        "        text_embeds = None,\n",
        "        text_mask = None,\n",
        "        cond_images = None,\n",
        "        noise = None,\n",
        "        times_next = None,\n",
        "        pred_objective = 'noise',\n",
        "        min_snr_gamma = None,\n",
        "        random_crop_size = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        is_video = x_start.ndim == 5\n",
        "\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "        # normalize to [-1, 1]\n",
        "\n",
        "        x_start = self.normalize_img(x_start)\n",
        "        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n",
        "\n",
        "        # random cropping during training\n",
        "        # for upsamplers\n",
        "\n",
        "        if exists(random_crop_size):\n",
        "            if is_video:\n",
        "                frames = x_start.shape[2]\n",
        "                x_start, lowres_cond_img, noise = map(lambda t: rearrange(t, 'b c f h w -> (b f) c h w'), (x_start, lowres_cond_img, noise))\n",
        "\n",
        "            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n",
        "\n",
        "            # make sure low res conditioner and image both get augmented the same way\n",
        "            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n",
        "            x_start = aug(x_start)\n",
        "            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n",
        "            noise = aug(noise, params = aug._params)\n",
        "\n",
        "            if is_video:\n",
        "                x_start, lowres_cond_img, noise = map(lambda t: rearrange(t, '(b f) c h w -> b c f h w', f = frames), (x_start, lowres_cond_img, noise))\n",
        "\n",
        "        # get x_t\n",
        "\n",
        "        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n",
        "\n",
        "        # also noise the lowres conditioning image\n",
        "        # at sample time, they then fix the noise level of 0.1 - 0.3\n",
        "\n",
        "        lowres_cond_img_noisy = None\n",
        "        if exists(lowres_cond_img):\n",
        "            lowres_aug_times = default(lowres_aug_times, times)\n",
        "            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n",
        "\n",
        "        # time condition\n",
        "\n",
        "        noise_cond = noise_scheduler.get_condition(times)\n",
        "\n",
        "        # unet kwargs\n",
        "\n",
        "        unet_kwargs = dict(\n",
        "            text_embeds = text_embeds,\n",
        "            text_mask = text_mask,\n",
        "            cond_images = cond_images,\n",
        "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n",
        "            lowres_cond_img = lowres_cond_img_noisy,\n",
        "            cond_drop_prob = self.cond_drop_prob,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # self condition if needed\n",
        "\n",
        "        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n",
        "        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n",
        "        # access the member 'module' of the wrapped unet instance.\n",
        "        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n",
        "\n",
        "        if self_cond and random() < 0.5:\n",
        "            with torch.no_grad():\n",
        "                pred = unet.forward(\n",
        "                    x_noisy,\n",
        "                    noise_cond,\n",
        "                    **unet_kwargs\n",
        "                ).detach()\n",
        "\n",
        "                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n",
        "\n",
        "                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n",
        "\n",
        "        # get prediction\n",
        "\n",
        "        pred = unet.forward(\n",
        "            x_noisy,\n",
        "            noise_cond,\n",
        "            **unet_kwargs\n",
        "        )\n",
        "\n",
        "        # prediction objective\n",
        "\n",
        "        if pred_objective == 'noise':\n",
        "            target = noise\n",
        "        elif pred_objective == 'x_start':\n",
        "            target = x_start\n",
        "        elif pred_objective == 'v':\n",
        "            # derivation detailed in Appendix D of Progressive Distillation paper\n",
        "            # https://arxiv.org/abs/2202.00512\n",
        "            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n",
        "            target = alpha * noise - sigma * x_start\n",
        "        else:\n",
        "            raise ValueError(f'unknown objective {pred_objective}')\n",
        "\n",
        "        # losses\n",
        "\n",
        "        losses = self.loss_fn(pred, target, reduction = 'none')\n",
        "        losses = reduce(losses, 'b ... -> b', 'mean')\n",
        "\n",
        "        # min snr loss reweighting\n",
        "\n",
        "        snr = log_snr.exp()\n",
        "        maybe_clipped_snr = snr.clone()\n",
        "\n",
        "        if exists(min_snr_gamma):\n",
        "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
        "\n",
        "        if pred_objective == 'noise':\n",
        "            loss_weight = maybe_clipped_snr / snr\n",
        "        elif pred_objective == 'x_start':\n",
        "            loss_weight = maybe_clipped_snr\n",
        "        elif pred_objective == 'v':\n",
        "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
        "\n",
        "        losses = losses * loss_weight\n",
        "        return losses.mean()\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self,\n",
        "        images, # rename to images or video\n",
        "        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n",
        "        texts: Optional[List[str]] = None,\n",
        "        text_embeds = None,\n",
        "        text_masks = None,\n",
        "        unet_number = None,\n",
        "        cond_images = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        if self.is_video and images.ndim == 4:\n",
        "            images = rearrange(images, 'b c h w -> b c 1 h w')\n",
        "            kwargs.update(ignore_time = True)\n",
        "\n",
        "        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n",
        "        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n",
        "        unet_number = default(unet_number, 1)\n",
        "        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n",
        "\n",
        "        images = cast_uint8_images_to_float(images)\n",
        "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n",
        "\n",
        "        assert images.dtype == torch.float or images.dtype == torch.half, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n",
        "\n",
        "        unet_index = unet_number - 1\n",
        "\n",
        "        unet = default(unet, lambda: self.get_unet(unet_number))\n",
        "\n",
        "        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n",
        "\n",
        "        noise_scheduler      = self.noise_schedulers[unet_index]\n",
        "        min_snr_gamma        = self.min_snr_gamma[unet_index]\n",
        "        pred_objective       = self.pred_objectives[unet_index]\n",
        "        target_image_size    = self.image_sizes[unet_index]\n",
        "        random_crop_size     = self.random_crop_sizes[unet_index]\n",
        "        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n",
        "\n",
        "        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n",
        "\n",
        "        assert images.shape[1] == self.channels\n",
        "        assert h >= target_image_size and w >= target_image_size\n",
        "\n",
        "        frames              = images.shape[2] if is_video else None\n",
        "        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n",
        "        ignore_time         = kwargs.get('ignore_time', False)\n",
        "\n",
        "        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n",
        "        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n",
        "        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n",
        "\n",
        "        times = noise_scheduler.sample_random_times(b, device = device)\n",
        "\n",
        "        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n",
        "            assert all([*map(len, texts)]), 'text cannot be empty'\n",
        "            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n",
        "\n",
        "            with autocast('cuda', enabled = False):\n",
        "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n",
        "\n",
        "            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n",
        "\n",
        "        if not self.unconditional:\n",
        "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n",
        "\n",
        "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n",
        "        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n",
        "\n",
        "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n",
        "\n",
        "        # handle video frame conditioning\n",
        "\n",
        "        if self.is_video and self.resize_cond_video_frames:\n",
        "            downsample_scale = self.temporal_downsample_factor[unet_index]\n",
        "            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n",
        "            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n",
        "            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n",
        "\n",
        "        # handle low resolution conditioning\n",
        "\n",
        "        lowres_cond_img = lowres_aug_times = None\n",
        "        if exists(prev_image_size):\n",
        "            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n",
        "            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n",
        "\n",
        "            if self.per_sample_random_aug_noise_level:\n",
        "                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(b, device = device)\n",
        "            else:\n",
        "                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n",
        "                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n",
        "\n",
        "        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n",
        "\n",
        "        return self.p_losses(unet, images, times, text_embeds = text_embeds, text_mask = text_masks, cond_images = cond_images, noise_scheduler = noise_scheduler, lowres_cond_img = lowres_cond_img, lowres_aug_times = lowres_aug_times, pred_objective = pred_objective, min_snr_gamma = min_snr_gamma, random_crop_size = random_crop_size, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdRMD_VpK2Yp",
        "outputId": "599b1990-7614-48a7-e655-4c8ef83a6ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "down fwd torch.Size([4, 128, 64, 64])\n",
            "down fwd torch.Size([4, 256, 32, 32])\n",
            "down fwd torch.Size([4, 512, 16, 16])\n",
            "down fwd torch.Size([4, 1024, 8, 8])\n",
            "torch.Size([4, 5, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# @title unet me\n",
        "# https://github.com/milesial/Pytorch-UNet/blob/master/unet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, mid_ch=None, bias=False):\n",
        "        super().__init__()\n",
        "        if mid_ch==None: mid_ch = out_ch\n",
        "        act = nn.ReLU(inplace=True) # ReLU GELU SiLU ELU\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(mid_ch), act,\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(out_ch), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_ch, out_ch),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "                nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=1),)\n",
        "        else: self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)#, in_ch // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1) # [c,h,w]\n",
        "        diffX, diffY = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        return self.conv(torch.cat([x2, x1], dim=1))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super().__init__()\n",
        "        # self.in_ch, self.out_ch, self.bilinear = in_ch, out_ch, bilinear\n",
        "        # self.inc = DoubleConv(in_ch, 64)\n",
        "        # self.down1 = Down(64, 128)\n",
        "        # self.down2 = Down(128, 256)\n",
        "        # self.down3 = Down(256, 512)\n",
        "        # factor = 2 if bilinear else 1\n",
        "        # self.down4 = Down(512, 1024 // factor)\n",
        "        # self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        # self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        # self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        # self.up4 = Up(128, 64, bilinear)\n",
        "        # self.outc = nn.Conv2d(64, out_ch, kernel_size=1)\n",
        "\n",
        "        # https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py\n",
        "        wf=6 # width factor, 2^6 = 64 -> 1024\n",
        "        depth=4\n",
        "        self.inc = DoubleConv(in_ch, 2 ** wf)\n",
        "        self.down_list = nn.ModuleList([Down(2 ** (wf + i), 2 ** (wf + i+1)) for i in range(depth)])\n",
        "        self.up_list = nn.ModuleList([Up(2 ** (wf + i+1), 2 ** (wf + i)) for i in reversed(range(depth))])\n",
        "        self.outc = nn.Conv2d(2 ** wf, out_ch, kernel_size=1)\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x1 = self.inc(x)\n",
        "    #     x2 = self.down1(x1)\n",
        "    #     x3 = self.down2(x2)\n",
        "    #     x4 = self.down3(x3)\n",
        "    #     x5 = self.down4(x4)\n",
        "    #     print(x5.shape, x4.shape)\n",
        "    #     x = self.up1(x5, x4)\n",
        "    #     x = self.up2(x, x3)\n",
        "    #     x = self.up3(x, x2)\n",
        "    #     x = self.up4(x, x1)\n",
        "    #     logits = self.outc(x)\n",
        "    #     return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        x = self.inc(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            blocks.append(x)\n",
        "            x = down(x)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "        return self.outc(x)\n",
        "\n",
        "\n",
        "unet = UNet(3, 5)\n",
        "x=torch.rand(4,3,64,64)\n",
        "# x=torch.rand(4,3,128,128)\n",
        "# x=torch.rand(4,3,28,28)\n",
        "out = unet(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aQJWp_oTxWRz"
      },
      "outputs": [],
      "source": [
        "# @title facebook unet\n",
        "# https://github.com/facebookresearch/RCDM/blob/main/guided_diffusion_rcdm/unet.py\n",
        "from abc import abstractmethod\n",
        "import math\n",
        "import threading\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch._C import has_mkldnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from .fp16_util import convert_module_to_f16, convert_module_to_f32\n",
        "from .nn import (\n",
        "    checkpoint,\n",
        "    conv_nd,\n",
        "    linear,\n",
        "    avg_pool_nd,\n",
        "    zero_module,\n",
        "    normalization,\n",
        "    timestep_embedding,\n",
        ")\n",
        "from .nn import normalization as normalization_gg\n",
        "from .layers import SNConv2d, ccbn, bn, SNLinear, identity\n",
        "import functools\n",
        "\n",
        "class AttentionPool2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spacial_dim,\n",
        "        embed_dim,\n",
        "        num_heads_channels,\n",
        "        output_dim = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Parameter(\n",
        "            torch.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5\n",
        "        )\n",
        "        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
        "        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
        "        self.num_heads = embed_dim // num_heads_channels\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *_spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)  # NC(HW)\n",
        "        x = torch.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n",
        "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n",
        "        x = self.qkv_proj(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"Apply the module to `x` given `emb` timestep embeddings.\"\"\"\n",
        "\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an extra input.\"\"\"\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 upsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, dims=2, out_ch=None):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_ch = out_ch or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3:\n",
        "            x = F.interpolate(\n",
        "                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n",
        "            )\n",
        "        else:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 downsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, dims=2, out_ch=None):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_ch = out_ch or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2)\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(\n",
        "                dims, self.channels, self.out_ch, 3, stride=stride, padding=1\n",
        "            )\n",
        "        else:\n",
        "            assert self.channels == self.out_ch\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    \"\"\"\n",
        "    A residual block that can optionally change the number of channels.\n",
        "\n",
        "    :param channels: the number of input channels.\n",
        "    :param emb_channels: the number of timestep embedding channels.\n",
        "    :param dropout: the rate of dropout.\n",
        "    :param out_channels: if specified, the number of out channels.\n",
        "    :param use_conv: if True and out_channels is specified, use a spatial\n",
        "        convolution instead of a smaller 1x1 convolution to change the\n",
        "        channels in the skip connection.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param use_checkpoint: if True, use gradient checkpointing on this module.\n",
        "    :param up: if True, use this block for upsampling.\n",
        "    :param down: if True, use this block for downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        normalization,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        dims=2,\n",
        "        use_checkpoint=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "        instance_cond=False,\n",
        "        pretrained=False,\n",
        "        use_fp16=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.use_fp16= use_fp16\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization_gg(channels),\n",
        "            nn.SiLU(),\n",
        "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization_gg(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(\n",
        "                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = conv_nd(\n",
        "                dims, channels, self.out_channels, 3, padding=1\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the block to a Tensor, conditioned on a timestep embedding.\n",
        "\n",
        "        :param x: an [N x C x ...] Tensor of features.\n",
        "        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        return checkpoint(\n",
        "            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n",
        "        )\n",
        "\n",
        "    def _forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply conditional batch normalization if feat is not None\n",
        "        \"\"\"\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other.\n",
        "\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "        use_new_attention_order=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert (\n",
        "                channels % num_head_channels == 0\n",
        "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = normalization_gg(channels)\n",
        "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
        "        if use_new_attention_order:\n",
        "            # split qkv before split heads\n",
        "            self.attention = QKVAttention(self.num_heads)\n",
        "        else:\n",
        "            # split heads before split qkv\n",
        "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "\n",
        "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return checkpoint(self._forward, (x,), self.parameters(), False)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    \"\"\"\n",
        "    A counter for the `thop` package to count the operations in an\n",
        "    attention operation.\n",
        "    Meant to be used like:\n",
        "        macs, params = thop.profile(\n",
        "            model,\n",
        "            inputs=(inputs, timestamps),\n",
        "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
        "        )\n",
        "    \"\"\"\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    # We perform two matmuls with the same number of ops.\n",
        "    # The first computes the weight matrix, the second computes\n",
        "    # the combination of the value vectors.\n",
        "    matmul_ops = 2 * b * (num_spatial ** 2) * c\n",
        "    model.total_ops += torch.DoubleTensor([matmul_ops])\n",
        "\n",
        "\n",
        "class QKVAttentionLegacy(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"Apply QKV attention.\n",
        "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"Apply QKV attention.\n",
        "\n",
        "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"The full UNet model with attention and timestep embedding.\n",
        "    :param in_channels: channels in the input Tensor.\n",
        "    :param model_channels: base channel count for the model.\n",
        "    :param out_channels: channels in the output Tensor.\n",
        "    :param num_res_blocks: number of residual blocks per downsample.\n",
        "    :param attention_resolutions: a collection of downsample rates at which attention will take place. May be a set, list, or tuple.\n",
        "        For example, if this contains 4, then at 4x downsampling, attention will be used.\n",
        "    :param dropout: the dropout probability.\n",
        "    :param channel_mult: channel multiplier for each level of the UNet.\n",
        "    :param conv_resample: if True, use learned convolutions for upsampling and downsampling.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param num_classes: if specified (as an int), then this model will be class-conditional with `num_classes` classes.\n",
        "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
        "    :param num_heads: the number of attention heads in each attention layer.\n",
        "    :param num_heads_channels: if specified, ignore num_heads and instead use a fixed channel width per attention head.\n",
        "    :param num_heads_upsample: works with num_heads to set a different number of heads for upsampling. Deprecated.\n",
        "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
        "    :param resblock_updown: use residual blocks for up/downsampling.\n",
        "    :param use_new_attention_order: use a different attention pattern for potentially increased efficiency.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_classes=None,\n",
        "        use_checkpoint=False,\n",
        "        use_fp16=False,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_new_attention_order=False,\n",
        "        cross_replica=False,\n",
        "        mybn=False,\n",
        "        BN_eps=1e-5,\n",
        "        SN_eps=1e-12,\n",
        "        instance_cond=False,\n",
        "        G_param=\"SN\",\n",
        "        shared_dim=128,\n",
        "        dim_z=128,\n",
        "        shared_dim_feat=512,\n",
        "        G_shared=True,\n",
        "        norm_style=\"bn\",\n",
        "        ssl_dim=2048,\n",
        "        pretrained=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        self.ssl_dim = ssl_dim\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_classes = num_classes\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.dtype = torch.float16 if use_fp16 else torch.float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.which_bn = normalization_gg\n",
        "        self.shared = nn.Identity()\n",
        "        self.ssl_emb = nn.Identity()\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        if instance_cond:\n",
        "            self.ssl_emb = nn.Linear(self.ssl_dim, time_embed_dim)\n",
        "            if use_fp16:\n",
        "                self.ssl_emb = self.ssl_emb.half()\n",
        "\n",
        "        ch = input_ch = int(channel_mult[0] * model_channels)\n",
        "        self.input_blocks = nn.ModuleList(\n",
        "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
        "        )\n",
        "        self._feature_size = ch\n",
        "        input_block_chans = [ch]\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        normalization=self.which_bn,\n",
        "                        out_channels=int(mult * model_channels),\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                        instance_cond=instance_cond,\n",
        "                        pretrained=pretrained,\n",
        "                        use_fp16=use_fp16,\n",
        "                    )\n",
        "                ]\n",
        "                ch = int(mult * model_channels)\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads,\n",
        "                            num_head_channels=num_head_channels,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        )\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            normalization=self.which_bn,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            down=True,\n",
        "                            instance_cond=instance_cond,\n",
        "                            pretrained=pretrained,\n",
        "                            use_fp16=use_fp16,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Downsample(\n",
        "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                normalization=self.which_bn,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "                instance_cond=instance_cond,\n",
        "                pretrained=pretrained,\n",
        "                use_fp16=use_fp16,\n",
        "            ),\n",
        "            AttentionBlock(\n",
        "                ch,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                num_heads=num_heads,\n",
        "                num_head_channels=num_head_channels,\n",
        "                use_new_attention_order=use_new_attention_order,\n",
        "            ),\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                normalization=self.which_bn,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "                instance_cond=instance_cond,\n",
        "                pretrained=pretrained,\n",
        "                use_fp16=use_fp16,\n",
        "            ),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch + ich,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        normalization=self.which_bn,\n",
        "                        out_channels=int(model_channels * mult),\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                        instance_cond=instance_cond,\n",
        "                        pretrained=pretrained,\n",
        "                        use_fp16=use_fp16,\n",
        "                    )\n",
        "                ]\n",
        "                ch = int(model_channels * mult)\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads_upsample,\n",
        "                            num_head_channels=num_head_channels,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        )\n",
        "                    )\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            normalization=self.which_bn,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            up=True,\n",
        "                            instance_cond=instance_cond,\n",
        "                            pretrained=pretrained,\n",
        "                            use_fp16=use_fp16,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            normalization_gg(ch),\n",
        "            nn.SiLU(),\n",
        "            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "    def convert_to_fp16(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float16.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f16)\n",
        "        self.middle_block.apply(convert_module_to_f16)\n",
        "        self.output_blocks.apply(convert_module_to_f16)\n",
        "\n",
        "    def convert_to_fp32(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float32.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f32)\n",
        "        self.middle_block.apply(convert_module_to_f32)\n",
        "        self.output_blocks.apply(convert_module_to_f32)\n",
        "\n",
        "    def forward(self, x, timesteps, y=None, feat=None):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "        :param x: an [N x C x ...] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :param y: an [N] Tensor of labels, if class-conditional.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        assert (y is not None) == (\n",
        "            self.num_classes is not None\n",
        "        ), \"must specify y if and only if the model is class-conditional\"\n",
        "\n",
        "        hs = []\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            assert y.shape == (x.shape[0],)\n",
        "            emb = emb + self.label_emb(y)\n",
        "\n",
        "        if feat is not None:\n",
        "            emb = emb + self.ssl_emb(feat)\n",
        "\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb)\n",
        "        for module in self.output_blocks:\n",
        "            h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = module(h, emb)\n",
        "        h = h.type(x.dtype)\n",
        "        return self.out(h)\n",
        "\n",
        "\n",
        "class SuperResModel(UNetModel):\n",
        "    \"\"\"\n",
        "    A UNetModel that performs super-resolution.\n",
        "\n",
        "    Expects an extra kwarg `low_res` to condition on a low-resolution image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, in_channels, *args, **kwargs):\n",
        "        super().__init__(image_size, in_channels * 2, *args, **kwargs)\n",
        "\n",
        "    def forward(self, x, timesteps, low_res=None, **kwargs):\n",
        "        _, _, new_height, new_width = x.shape\n",
        "        upsampled = F.interpolate(low_res, (new_height, new_width), mode=\"bilinear\")\n",
        "        x = torch.cat([x, upsampled], dim=1)\n",
        "        return super().forward(x, timesteps, **kwargs)\n",
        "\n",
        "\n",
        "class EncoderUNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The half UNet model with attention and timestep embedding.\n",
        "\n",
        "    For usage, see UNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        use_checkpoint=False,\n",
        "        use_fp16=False,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_new_attention_order=False,\n",
        "        pool=\"adaptive\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.dtype = torch.float16 if use_fp16 else torch.float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        ch = int(channel_mult[0] * model_channels)\n",
        "        self.input_blocks = nn.ModuleList(\n",
        "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
        "        )\n",
        "        self._feature_size = ch\n",
        "        input_block_chans = [ch]\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=int(mult * model_channels),\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = int(mult * model_channels)\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads,\n",
        "                            num_head_channels=num_head_channels,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        )\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            down=True,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Downsample(\n",
        "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "            AttentionBlock(\n",
        "                ch,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                num_heads=num_heads,\n",
        "                num_head_channels=num_head_channels,\n",
        "                use_new_attention_order=use_new_attention_order,\n",
        "            ),\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "        self.pool = pool\n",
        "        if pool == \"adaptive\":\n",
        "            self.out = nn.Sequential(\n",
        "                normalization_gg(ch),\n",
        "                nn.SiLU(),\n",
        "                nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
        "                nn.Flatten(),\n",
        "            )\n",
        "        elif pool == \"attention\":\n",
        "            assert num_head_channels != -1\n",
        "            self.out = nn.Sequential(\n",
        "                normalization_gg(ch),\n",
        "                nn.SiLU(),\n",
        "                AttentionPool2d(\n",
        "                    (image_size // ds), ch, num_head_channels, 128\n",
        "                ),\n",
        "                nn.Linear(128, self.out_channels),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "        elif pool == \"spatial\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2048, self.out_channels),\n",
        "            )\n",
        "        elif pool == \"spatial_v2\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048),\n",
        "                #nn.ReLU(),\n",
        "                #normalization(2048),\n",
        "                #nn.SiLU(),\n",
        "                nn.Linear(2048, self.out_channels, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n",
        "\n",
        "    def convert_to_fp16(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float16.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f16)\n",
        "        self.middle_block.apply(convert_module_to_f16)\n",
        "\n",
        "    def convert_to_fp32(self):\n",
        "        \"\"\"\n",
        "        Convert the torso of the model to float32.\n",
        "        \"\"\"\n",
        "        self.input_blocks.apply(convert_module_to_f32)\n",
        "        self.middle_block.apply(convert_module_to_f32)\n",
        "\n",
        "    def forward(self, x, timesteps):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "\n",
        "        :param x: an [N x C x ...] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :return: an [N x K] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "        results = []\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            if self.pool.startswith(\"spatial\"):\n",
        "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "        h = self.middle_block(h, emb)\n",
        "        if self.pool.startswith(\"spatial\"):\n",
        "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "            h = torch.cat(results, axis=-1)\n",
        "            return self.out(h)\n",
        "        else:\n",
        "            h = h.type(x.dtype)\n",
        "            return self.out(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QKP61jCeB4cA"
      },
      "outputs": [],
      "source": [
        "# @title sinusoidal time emb\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/util.py\n",
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device)\n",
        "        args = timesteps[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2: embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    else: embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
        "    return embedding # [N, dim]\n",
        "\n",
        "\n",
        "# @title rotary_embedding\n",
        "import torch\n",
        "def rotary_embedding(seq_len, dim):\n",
        "    # theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "    theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
        "    # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    pos = torch.arange(seq_len).unsqueeze(1)\n",
        "    angles = pos * theta # [seq_len, 1] * [dim // 2] = [seq_len, dim // 2]\n",
        "    return torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim]\n",
        "\n",
        "def apply_rotary_embedding(embeddings, rotary_emb):\n",
        "    batch, seq_len, dim = embeddings.shape\n",
        "    # if rotary_emb.shape[0] < seq_len: print(\"rotary_emb.shape[0] < seq_len\")\n",
        "    rotary_emb = rotary_emb[:seq_len, :].unsqueeze(0).expand(batch, -1, -1)\n",
        "    embeddings = embeddings.view(batch, seq_len, dim // 2, 2)\n",
        "    rotary_emb = rotary_emb.view(batch, seq_len, dim // 2, 2)\n",
        "    # rotated_embeddings = torch.einsum('...ij,...ij->...ij', embeddings, rotary_emb)\n",
        "    rotated_embeddings = embeddings * rotary_emb\n",
        "    return rotated_embeddings.view(*rotated_embeddings.shape[:-2], dim)\n",
        "\n",
        "\n",
        "seq_len = 10\n",
        "dim = 64  # Must be even\n",
        "batch_size = 2\n",
        "rotary_emb = rotary_embedding(seq_len, dim)\n",
        "input_embeddings = torch.randn(batch_size, seq_len, dim)\n",
        "output_embeddings = apply_rotary_embedding(input_embeddings, rotary_emb)\n",
        "print(\"Input Embeddings:\", input_embeddings.shape)\n",
        "print(\"Rotary Embeddings:\", rotary_emb.shape)\n",
        "print(\"Output Embeddings:\", output_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "CEQjRDpSh-hh",
        "outputId": "93868375-7167-4abc-ae44-7f0d22cf918b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (16) must match the existing size (10) at non-singleton dimension 2.  Target sizes: [10, 3, 16, 16].  Tensor sizes: [10, 1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-842ba4a54d5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# t = (torch.rand(1, device=device) + torch.arange(batch, device=device)/batch) % (1 - eps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (16) must match the existing size (10) at non-singleton dimension 2.  Target sizes: [10, 3, 16, 16].  Tensor sizes: [10, 1]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch = 10\n",
        "eps = 1e-5\n",
        "x1 = torch.rand((batch,3,16,16), device=device)\n",
        "# t = (torch.rand(1, device=device) + torch.arange(batch, device=device)/batch) % (1 - eps)\n",
        "t = torch.rand(batch, device=device)\n",
        "t = t[:, None].expand(x1.shape)\n",
        "\n",
        "print(t)\n",
        "# print(torch.rand(1, device=device), torch.arange(batch, device=device)/batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vt8djkRrrELg"
      },
      "outputs": [],
      "source": [
        "# @title keishihara train\n",
        "# https://github.com/keishihara/flow-matching/blob/main/train_flow_matching_on_image.py\n",
        "    for epoch in range(args.n_epochs):\n",
        "        flow.train()\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1:2d}/{args.n_epochs}\")\n",
        "\n",
        "        for x_1, y in pbar:\n",
        "            x_1, y = x_1.to(device), y.to(device)\n",
        "\n",
        "            # Compute the probability path samples\n",
        "            x_0 = torch.randn_like(x_1)\n",
        "            t = torch.rand(x_1.size(0), device=device, dtype=x_1.dtype)\n",
        "            x_t, dx_t = path_sampler.sample(x_0, x_1, t)\n",
        "\n",
        "            flow.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Compute the conditional flow matching loss with class conditioning\n",
        "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "                vf_t = flow(t=t, x=x_t, y=y)\n",
        "                loss = F.mse_loss(vf_t, dx_t)\n",
        "\n",
        "            # Gradient scaling and backprop\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(flow.parameters(), max_norm=1.0)  # clip gradients\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DqgMPLjsVjg7"
      },
      "outputs": [],
      "source": [
        "# @title lebellig Flow_Matching train\n",
        "# https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "class OTFlowMatching:\n",
        "  def __init__(self, sig_min: float = 0.001) -> None:\n",
        "    super().__init__()\n",
        "    self.sig_min = sig_min\n",
        "    self.eps = 1e-5\n",
        "\n",
        "  def psi_t(self, x: torch.Tensor, x_1: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "    return (1 - (1 - self.sig_min) * t) * x + t * x_1\n",
        "\n",
        "  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:\n",
        "    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)\n",
        "    t = t[:, None].expand(x_1.shape)\n",
        "    x_0 = torch.randn_like(x_1)\n",
        "    v_psi = v_t(t[:,0], self.psi_t(x_0, x_1, t))\n",
        "    d_psi = x_1 - (1 - self.sig_min) * x_0\n",
        "    return torch.mean((v_psi - d_psi) ** 2)\n",
        "\n",
        "model = OTFlowMatching()\n",
        "net = Net(2, 2, [512]*5, 10).to(device)\n",
        "v_t = CondVF(net)\n",
        "\n",
        "losses = []\n",
        "optimizer = torch.optim.Adam(v_t.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in tqdm(range(5000), ncols=88):\n",
        "    for batch in dataloader:\n",
        "      x_1 = batch[0]\n",
        "      loss = model.loss(v_t, x_1)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses += [loss.detach()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VLhv7FyWZ8oG"
      },
      "outputs": [],
      "source": [
        "# @title drscotthawley FlowModels_colab.ipynb\n",
        "# https://colab.research.google.com/github/drscotthawley/blog/blob/main/extra/FlowModels_colab.ipynb\n",
        "\n",
        "def train(model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    batch_size=2048\n",
        "    pbar = tqdm(range(n_steps), leave=False)\n",
        "    for _ in pbar:\n",
        "        step += 1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 1 == 0:  # you could in theory not draw new points with each step, though we will.\n",
        "            source_samples = create_source_data(batch_size).to(device)\n",
        "            target_samples = create_target_data(batch_size) # this function also supports fm models from scratch\n",
        "            # target_samples = integrate_path(pretrained_model, source_samples, step_fn=rk4_step, warp_fn=warp_time, n_steps=20)\n",
        "\n",
        "        t = torch.rand(source_samples.size(0), 1).to(device) # random times for training\n",
        "        if warp_fn: t = warp_fn(t)  # time warp here (different from use in integrator!) helps focus \"coverage\" i.e. sampling the space\n",
        "\n",
        "        interpolated_samples = source_samples * (1 - t) + target_samples * t\n",
        "        v = model(interpolated_samples, t)\n",
        "        line_directions = target_samples - source_samples\n",
        "        loss = loss_fn(v, line_directions)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # pbar.set_description(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4g}')\n",
        "        pbar.set_description(f'Epoch [{epoch + 1}], Loss: {loss.item():.4g}')\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        model.eval()\n",
        "        clear_output(wait=True)  # Clear previous plots\n",
        "        viz(val_points, target_samples[:val_points.shape[0]], model)  # don't need rk4 for rect model viz b/c paths r straight\n",
        "        plt.show()\n",
        "        plt.close()  # Close the figure to free memory\n",
        "        model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "step, n_steps = 0, 100\n",
        "device = next(model.parameters()).device\n",
        "for epoch in range(40):\n",
        "    train(model, optimizer, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rStmiIIRPj9k"
      },
      "outputs": [],
      "source": [
        "# @title fair standalone_flow_matching\n",
        "# https://facebookresearch.github.io/flow_matching/notebooks/standalone_flow_matching.html\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    def __init__(self, dim = 2, h = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim + 1, h), nn.ELU(),\n",
        "            nn.Linear(h, h), nn.ELU(),\n",
        "            nn.Linear(h, h), nn.ELU(),\n",
        "            nn.Linear(h, dim))\n",
        "\n",
        "    def forward(self, t: Tensor, x_t: Tensor) -> Tensor:\n",
        "        return self.net(torch.cat((t, x_t), -1))\n",
        "\n",
        "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor) -> Tensor:\n",
        "        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n",
        "\n",
        "        return x_t + (t_end - t_start) * self(t=t_start + (t_end - t_start) / 2, x_t= x_t + self(x_t=x_t, t=t_start) * (t_end - t_start) / 2)\n",
        "\n",
        "\n",
        "flow = Flow()\n",
        "\n",
        "optimizer = torch.optim.Adam(flow.parameters(), 1e-2)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for _ in range(10000):\n",
        "    x_1 = Tensor(make_moons(256, noise=0.05)[0])\n",
        "    x_0 = torch.randn_like(x_1)\n",
        "    t = torch.rand(len(x_1), 1)\n",
        "    x_t = (1 - t) * x_0 + t * x_1\n",
        "    dx_t = x_1 - x_0\n",
        "    optimizer.zero_grad()\n",
        "    loss_fn(flow(t=t, x_t=x_t), dx_t).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "x = torch.randn(300, 2)\n",
        "n_steps = 8\n",
        "fig, axes = plt.subplots(1, n_steps + 1, figsize=(30, 4), sharex=True, sharey=True)\n",
        "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
        "\n",
        "axes[0].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10)\n",
        "axes[0].set_title(f't = {time_steps[0]:.2f}')\n",
        "axes[0].set_xlim(-3.0, 3.0)\n",
        "axes[0].set_ylim(-3.0, 3.0)\n",
        "\n",
        "for i in range(n_steps):\n",
        "    x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1])\n",
        "    axes[i + 1].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10)\n",
        "    axes[i + 1].set_title(f't = {time_steps[i + 1]:.2f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mFCFidMrLFEj"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_cost_matrix(X, Y):\n",
        "    \"\"\"Compute pairwise cost matrix (Euclidean distance).\"\"\"\n",
        "    X_norm = (X ** 2).sum(1).view(-1, 1)\n",
        "    Y_norm = (Y ** 2).sum(1).view(1, -1)\n",
        "    cost_matrix = X_norm + Y_norm - 2 * torch.mm(X, Y.T)\n",
        "    return cost_matrix\n",
        "\n",
        "def sinkhorn_algorithm(X, Y, , epsilon=1e-5, num_iters=50):\n",
        "    \"\"\"Compute the optimal transport plan using Sinkhorn algorithm.\"\"\"\n",
        "    cost_matrix = compute_cost_matrix(X, Y)\n",
        "    n, m = cost_matrix.shape\n",
        "    u = torch.ones(n, device=cost_matrix.device) / n\n",
        "    v = torch.ones(m, device=cost_matrix.device) / m\n",
        "    K = torch.exp(-cost_matrix / epsilon)\n",
        "    for _ in range(num_iters):\n",
        "        u = 1.0 / (K @ v)\n",
        "        v = 1.0 / (K.T @ u)\n",
        "    transport_plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    return transport_plan\n",
        "\n",
        "def ot_cfm_loss(x, y, epsilon=0.01, num_iters=50):\n",
        "    \"\"\"Compute the OT-CFM loss between source and target features.\"\"\"\n",
        "    cost_matrix = compute_cost_matrix(source_features, target_features)\n",
        "    # cost_matrix = torch.cdist(x, y, p=2)\n",
        "    transport_plan = sinkhorn_algorithm(cost_matrix, epsilon, num_iters)\n",
        "    # transport_plan = sinkhorn_algorithm(x,y, epsilon, num_iters)\n",
        "    ot_loss = (cost_matrix * transport_plan).sum()\n",
        "    return ot_loss\n",
        "\n",
        "\n",
        "def ot_cfm(X, Y, alpha=0.5): # src, tgt, mixing coef\n",
        "    \"\"\"Perform OT-based Classifier-Free Mixing.\"\"\"\n",
        "    transport_plan = sinkhorn_algorithm(X, Y)\n",
        "    # Match source samples to target using the OT plan\n",
        "    indices = transport_plan.argmax(dim=1)\n",
        "    matched_Y = Y[indices]\n",
        "    # Linear interpolation between X and matched_Y\n",
        "    mixed_samples = alpha * X + (1 - alpha) * matched_Y\n",
        "    return mixed_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate random source and target features\n",
        "source_features = torch.rand(32, 128)  # 32 samples, 128-dimensional features\n",
        "target_features = torch.rand(32, 128)\n",
        "\n",
        "# Compute OT-CFM loss\n",
        "loss = ot_cfm_loss(source_features, target_features)\n",
        "print(\"OT-CFM Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zpr53Gwo5tiR"
      },
      "outputs": [],
      "source": [
        "# @title stable diffusion unet save\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from ldm.modules.diffusionmodules.util import (\n",
        "#     conv_nd,\n",
        "#     avg_pool_nd,\n",
        "#     zero_module,\n",
        "#     normalization, # nn.GroupNorm\n",
        "#     timestep_embedding,\n",
        "# )\n",
        "# from ldm.modules.attention import SpatialTransformer # https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py\n",
        "\n",
        "class AttentionPool2d(nn.Module):\n",
        "    \"\"\"Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\"\"\"\n",
        "    def __init__(self, spacial_dim, embed_dim, num_heads_channels, output_dim = None,):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5)\n",
        "        # self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
        "        # self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
        "        self.qkv_proj = Conv1d(embed_dim, 3 * embed_dim, 1)\n",
        "        self.c_proj = Conv1d(embed_dim, output_dim or embed_dim, 1)\n",
        "        self.num_heads = embed_dim // num_heads_channels\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *_spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)  # NC(HW)\n",
        "        x = torch.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(1+HW)\n",
        "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(1+HW)\n",
        "        x = self.qkv_proj(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "from abc import abstractmethod\n",
        "class TimestepBlock(nn.Module):\n",
        "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"Apply the module to `x` given `emb` timestep embeddings.\"\"\"\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an extra input.\"\"\"\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock): x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer): x = layer(x, context)\n",
        "            else: x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then upsampling occurs in the inner-two dimensions.\"\"\"\n",
        "    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        else: x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv: x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D..\"\"\"\n",
        "    def __init__(self, channels, use_conv, dims=2, out_channels=None,padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial?]\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    \"\"\"A residual block that can optionally change the number of channels.\n",
        "    :param channels: the number of input channels.\n",
        "    :param emb_channels: the number of timestep embedding channels.\n",
        "    :param dropout: the rate of dropout.\n",
        "    :param out_channels: if specified, the number of out channels.\n",
        "    :param use_conv: if True and out_channels is specified, use a spatial convolution instead of a smaller 1x1 convolution to change the channels in the skip connection.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param up: if True, use this block for upsampling.\n",
        "    :param down: if True, use this block for downsampling.\"\"\"\n",
        "    def __init__(self, channels, emb_channels, dropout, out_channels=None, use_conv=False, use_scale_shift_norm=False, dims=2, up=False, down=False,):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels), nn.SiLU(), conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        self.updown = up or down\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(), nn.Linear(emb_channels, 2 * self.out_channels if use_scale_shift_norm else self.out_channels,),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(self.out_channels), nn.SiLU(), nn.Dropout(p=dropout),\n",
        "            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels: self.skip_connection = nn.Identity()\n",
        "        elif use_conv: self.skip_connection = conv_nd(dims, channels, self.out_channels, 3, padding=1)\n",
        "        else: self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, emb_ch]\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x) # norm, act\n",
        "            h = self.h_upd(h) # up/ down sample\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h) # conv\n",
        "        else:\n",
        "            h = self.in_layers(x) # norm, act, conv\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype) # act, lin\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h # [N, C, ...]\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\"\"\"\n",
        "    def __init__(self, channels, num_heads=1, num_head_channels=-1,):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1: self.num_heads = num_heads\n",
        "        else:\n",
        "            assert (channels % num_head_channels == 0), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.norm = normalization(channels)\n",
        "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
        "        # self.qkv = Conv1d(channels, channels * 3, 1)\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "        # self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "# import math\n",
        "# class QKVAttentionLegacy(nn.Module):\n",
        "#     \"\"\"A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\"\"\"\n",
        "#     def __init__(self, n_heads):\n",
        "#         super().__init__()\n",
        "#         self.n_heads = n_heads\n",
        "\n",
        "#     def forward(self, qkv): # [N, H*3*C, T]\n",
        "#         bs, width, length = qkv.shape\n",
        "#         assert width % (3 * self.n_heads) == 0\n",
        "#         ch = width // (3 * self.n_heads)\n",
        "\n",
        "#         q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1) # [N*H, C, T]\n",
        "#         scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "#         weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)  # More stable with f16 than dividing afterwards\n",
        "#         weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "#         a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
        "\n",
        "#         return a.reshape(bs, -1, length) # [N, H*C, T]\n",
        "\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv): # [N, 3*H*C, T]\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=1) # [N, H*C, T]\n",
        "        scale = ch**(-1/4) # scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),)  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "\n",
        "        return a.reshape(bs, -1, length) # [N, H*C, T]\n",
        "\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"The full UNet model with attention and timestep embedding.\n",
        "    :param model_channels: base channel count for the model.\n",
        "    :param num_res_blocks: number of residual blocks per downsample.\n",
        "    :param attention_resolutions: a collection of downsample rates at which\n",
        "        attention will take place. May be a set, list, or tuple.\n",
        "        For example, if this contains 4, then at 4x downsampling, attention\n",
        "        will be used.\n",
        "    :param channel_mult: channel multiplier for each level of the UNet.\n",
        "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
        "        downsampling.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param num_classes: if specified (as an int), then this model will be\n",
        "        class-conditional with `num_classes` classes.\n",
        "    :param num_heads: the number of attention heads in each attention layer.\n",
        "    :param num_heads_channels: if specified, ignore num_heads and instead use a fixed channel width per attention head.\n",
        "    :param num_heads_upsample: works with num_heads to set a different number\n",
        "                               of heads for upsampling. Deprecated.\n",
        "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
        "    :param resblock_updown: use residual blocks for up/downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_classes=None,\n",
        "        num_heads=-1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_spatial_transformer=False,    # custom transformer support\n",
        "        transformer_depth=1,              # custom transformer support\n",
        "        context_dim=None,                 # custom transformer support\n",
        "        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n",
        "        legacy=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if use_spatial_transformer:\n",
        "            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n",
        "\n",
        "        if context_dim is not None:\n",
        "            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n",
        "            from omegaconf.listconfig import ListConfig\n",
        "            if type(context_dim) == ListConfig:\n",
        "                context_dim = list(context_dim)\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        if num_heads == -1:\n",
        "            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n",
        "\n",
        "        if num_head_channels == -1:\n",
        "            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_classes = num_classes\n",
        "        self.dtype = torch.float16 # float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "        self.predict_codebook_ids = n_embed is not None\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])\n",
        "        self._feature_size = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult): # 1,2,4,8\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    if num_head_channels == -1:\n",
        "                        dim_head = ch // num_heads\n",
        "                    else:\n",
        "                        num_heads = ch // num_head_channels\n",
        "                        dim_head = num_head_channels\n",
        "                    if legacy:\n",
        "                        #num_heads = 1\n",
        "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,)\n",
        "                        if not use_spatial_transformer else SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim)\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1: # last level no res\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, down=True,)\n",
        "                        if resblock_updown\n",
        "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        if num_head_channels == -1:\n",
        "            dim_head = ch // num_heads\n",
        "        else:\n",
        "            num_heads = ch // num_head_channels\n",
        "            dim_head = num_head_channels\n",
        "        if legacy:\n",
        "            #num_heads = 1\n",
        "            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,)\n",
        "            if not use_spatial_transformer else SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    if num_head_channels == -1:\n",
        "                        dim_head = ch // num_heads\n",
        "                    else:\n",
        "                        num_heads = ch // num_head_channels\n",
        "                        dim_head = num_head_channels\n",
        "                    if legacy:\n",
        "                        #num_heads = 1\n",
        "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads_upsample, num_head_channels=dim_head,)\n",
        "                        if not use_spatial_transformer else SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim)\n",
        "                    )\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, up=True,)\n",
        "                        if resblock_updown else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n",
        "        )\n",
        "        if self.predict_codebook_ids:\n",
        "            self.id_predictor = nn.Sequential(\n",
        "            normalization(ch), conv_nd(dims, model_channels, n_embed, 1),\n",
        "            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None, y=None,**kwargs):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "        :param x: an [N x C x ...] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :param context: conditioning plugged in via crossattn\n",
        "        :param y: an [N] Tensor of labels, if class-conditional.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        assert (y is not None) == (self.num_classes is not None), \"must specify y if and only if the model is class-conditional\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            assert y.shape == (x.shape[0],)\n",
        "            emb = emb + self.label_emb(y)\n",
        "\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "        for module in self.output_blocks:\n",
        "            h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "        h = h.type(x.dtype)\n",
        "        if self.predict_codebook_ids:\n",
        "            return self.id_predictor(h)\n",
        "        else:\n",
        "            return self.out(h)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"The full UNet model with attention and timestep embedding.\n",
        "    :param channel_mult: channel multiplier for each level of the UNet.\n",
        "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
        "    :param num_heads: the number of attention heads in each attention layer.\n",
        "    :param num_heads_channels: if specified, ignore num_heads and instead use a fixed channel width per attention head.\n",
        "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, model_channels=32, out_channels=None, num_res_blocks=1,\n",
        "        # attention_resolutions,\n",
        "        attention_resolutions=(1, 2, 4),#(1, 2, 4, 8),\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4),#(1, 2, 4, 8),\n",
        "        dims=2, num_heads=-1, use_scale_shift_norm=False,):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels # base channel count for the model\n",
        "        out_channels = out_channels or in_channels\n",
        "        self.num_res_blocks = num_res_blocks # number of residual blocks per downsample\n",
        "        self.attention_resolutions = attention_resolutions # collection of downsample rates at which attention will take place. May be a set, list, or tuple. For example, if this contains 4, then at 4x downsampling, attention will be used\n",
        "        transformer_depth=1\n",
        "        context_dim=16\n",
        "\n",
        "        # self.dtype = torch.float16 # float32\n",
        "        # self.n_heads = n_heads\n",
        "        # self.head_dim = d_model // n_heads\n",
        "        # self.num_heads = num_heads\n",
        "        # self.num_head_channels = model_channels // num_heads\n",
        "        self.num_head_channels = num_head_channels = 4\n",
        "        self.num_heads = model_channels // num_head_channels\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])\n",
        "        # input_block_chans = [model_channels]\n",
        "        ch = model_channels # track num channels\n",
        "        # ds = 1 # track down sampling ammount\n",
        "        for level, mult in enumerate(channel_mult): # 1,2,4,8\n",
        "            for _ in range(num_res_blocks):\n",
        "                print(level, mult, \"level, mult; res; ch:\", ch,model_channels * mult)\n",
        "                # print(level, mult, \"level, mult;res;ch\", ch)\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = mult * model_channels\n",
        "                # if ds in attention_resolutions:\n",
        "                # dim_head = ch // num_heads\n",
        "                num_heads = ch // num_head_channels\n",
        "                dim_head = num_head_channels\n",
        "\n",
        "                # print(ds, \"ds ;attn; ch\", ch)\n",
        "                layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,))\n",
        "                # layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                # input_block_chans.append(ch)\n",
        "            # if level != len(channel_mult) - 1: # while not last level; last level no down sample\n",
        "            # out_ch = ch\n",
        "            # self.input_blocks.append(TimestepEmbedSequential(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, updown='down',)))\n",
        "            print(level, \"lv;down;ch\", ch)\n",
        "            # layers.append(Downsample(ch, dims=dims, out_channels=out_ch),)\n",
        "            layers.append(Downsample(ch, dims=dims, out_channels=ch),)\n",
        "            # ch = out_ch\n",
        "            # input_block_chans.append(ch)\n",
        "            # ds *= 2\n",
        "\n",
        "# 3,64,64 conv (res attn down) (res attn down) (res attn)\n",
        "# chn inc after res\n",
        "# (res attn res)\n",
        "\n",
        "        # dim_head = ch // num_heads\n",
        "        num_heads = ch // num_head_channels\n",
        "        dim_head = num_head_channels\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            Downsample(in_ch, dims=dims, out_channels=out_ch),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,),\n",
        "            # SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            Upsample(in_ch, dims=dims, out_channels=in_ch),\n",
        "        )\n",
        "        # print(\"unet init input_block_chans\", input_block_chans) # [64, 64, 64, 128, 128, 256]\n",
        "        print(\"UP\")\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            # for i in range(num_res_blocks + 1):\n",
        "            for i in range(num_res_blocks):\n",
        "                # ich = input_block_chans.pop()\n",
        "\n",
        "                # if level != len(channel_mult) - 1:\n",
        "                # out_ch = ch\n",
        "                # layers.append(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, updown='up',))\n",
        "                print(level, \"lv;up;ch\", ch)\n",
        "                # layers.append(Upsample(ch, dims=dims, out_channels=out_ch),)\n",
        "                layers.append(Upsample(ch, dims=dims, out_channels=ch),)\n",
        "                # ds //= 2\n",
        "\n",
        "                # print(level, mult, \"level, mult; res; ch,ich:\", ch,ich,ch + ich,model_channels * mult)\n",
        "                print(level, mult, \"level, mult; res; in,out:\", model_channels * mult*2,model_channels * mult)\n",
        "                # layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                # layers = [ResBlock(int(model_channels * mult*3/2), time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                layers = [ResBlock(model_channels * mult*2, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = model_channels * mult\n",
        "                # if ds in attention_resolutions:\n",
        "                # dim_head = ch // num_heads\n",
        "                num_heads = ch // num_head_channels\n",
        "                dim_head = num_head_channels\n",
        "                # print(ds, \"ds ;attn; ch\", ch)\n",
        "                layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=dim_head,))\n",
        "                # layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))\n",
        "                # # if level and i == num_res_blocks:\n",
        "                # # if level != num_res_blocks:\n",
        "                # if level != len(channel_mult) - 1:\n",
        "                #     out_ch = ch\n",
        "                #     # layers.append(ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, updown='up',))\n",
        "                #     print(level, \"lv;up;ch\", ch)\n",
        "                #     layers.append(Upsample(ch, dims=dims, out_channels=out_ch),)\n",
        "                #     ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"unet init\",dims, model_channels, out_channels) # 2 64 None\n",
        "        self.out = nn.Sequential(normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),)\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None): # [N, C, ...]\n",
        "        \"\"\":param timesteps: a 1-D batch of timesteps.\n",
        "        :param context: conditioning plugged in via crossattn\"\"\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        # h = x.type(self.dtype)\n",
        "        h = x\n",
        "        for module in self.input_blocks:\n",
        "            # print(\"unet fwd\", module)\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "        for i, module in enumerate(self.output_blocks):\n",
        "            print(\"unet fwd\", h.shape,hs[-1].shape)\n",
        "            # h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = torch.cat([h, hs[-i-1]], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "        # h = h.type(x.dtype)\n",
        "        return self.out(h)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderUNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The half UNet model with attention and timestep embedding.\n",
        "    For usage, see UNet.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        pool=\"adaptive\",\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.dtype = torch.float16 # float16 float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim), nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])\n",
        "        self._feature_size = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_scale_shift_norm=use_scale_shift_norm,)]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads, num_head_channels=num_head_channels,))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(TimestepEmbedSequential(\n",
        "                        ResBlock(ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm, down=True,)\n",
        "                        if resblock_updown\n",
        "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    ))\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "            AttentionBlock(ch, num_heads=num_heads, num_head_channels=num_head_channels,),\n",
        "            ResBlock(ch, time_embed_dim, dropout, dims=dims, use_scale_shift_norm=use_scale_shift_norm,),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "        self.pool = pool\n",
        "        if pool == \"adaptive\":\n",
        "            self.out = nn.Sequential(\n",
        "                normalization(ch), nn.SiLU(), nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
        "                nn.Flatten(),\n",
        "            )\n",
        "        elif pool == \"attention\":\n",
        "            assert num_head_channels != -1\n",
        "            self.out = nn.Sequential(\n",
        "                normalization(ch), nn.SiLU(),\n",
        "                AttentionPool2d((image_size // ds), ch, num_head_channels, out_channels),\n",
        "            )\n",
        "        elif pool == \"spatial\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048), nn.ReLU(),\n",
        "                nn.Linear(2048, self.out_channels),\n",
        "            )\n",
        "        elif pool == \"spatial_v2\":\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Linear(self._feature_size, 2048), normalization(2048), nn.SiLU(),\n",
        "                nn.Linear(2048, self.out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, timesteps): # [N, C, ...], [N]\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "        results = []\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            if self.pool.startswith(\"spatial\"):\n",
        "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "        h = self.middle_block(h, emb)\n",
        "        if self.pool.startswith(\"spatial\"):\n",
        "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
        "            h = torch.cat(results, axis=-1)\n",
        "        else: h = h.type(x.dtype)\n",
        "        return self.out(h) # [N, K]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bJsY-p6yyCkr"
      },
      "outputs": [],
      "source": [
        "# @title SpatialTransformer save\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py\n",
        "from inspect import isfunction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def uniq(arr):\n",
        "    return{el: True for el in arr}.keys()\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def max_neg_value(t):\n",
        "    return -torch.finfo(t.dtype).max\n",
        "\n",
        "\n",
        "def init_(tensor):\n",
        "    dim = tensor.shape[-1]\n",
        "    std = 1 / math.sqrt(dim)\n",
        "    tensor.uniform_(-std, std)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# feedforward\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = default(dim_out, dim)\n",
        "        project_in = nn.Sequential(\n",
        "            nn.Linear(dim, inner_dim),\n",
        "            nn.GELU()\n",
        "        ) if not glu else GEGLU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            project_in,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(inner_dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n",
        "        k = k.softmax(dim=-1)\n",
        "        context = torch.einsum('bhdn,bhen->bhde', k, v)\n",
        "        out = torch.einsum('bhde,bhdn->bhen', context, q)\n",
        "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class SpatialSelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = self.norm(x)\n",
        "        q, k, v = self.q(h_), self.k(h_), self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
        "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
        "\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
        "        w_ = rearrange(w_, 'b i j -> b j i')\n",
        "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
        "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
        "        h_ = self.proj_out(h_)\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout),)\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is a self-attention\n",
        "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
        "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is self-attn if context is none\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
        "\n",
        "    def _forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"Transformer block for image-like data.\n",
        "    First, project the input (aka embedding) and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\"\"\"\n",
        "    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.proj_in = nn.Conv2d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n",
        "        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim) for d in range(depth)])\n",
        "        self.proj_out = zero_module(nn.Conv2d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "    def forward(self, x, context=None): # note: if no context is given, cross-attention defaults to self-attention\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context=context)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wVirzVXjfM7",
        "outputId": "edc3d8b3-ac63-4e15-a2a0-0f2f13a3e2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title unet me attn\n",
        "# https://github.com/milesial/Pytorch-UNet/blob/master/unet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, mid_ch=None, bias=False):\n",
        "        super().__init__()\n",
        "        if mid_ch==None: mid_ch = out_ch\n",
        "        act = nn.ReLU(inplace=True) # ReLU GELU SiLU ELU\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(mid_ch), act,\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=bias), nn.BatchNorm2d(out_ch), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_ch, out_ch),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "                nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=1),)\n",
        "        else: self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)#, in_ch // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1) # [c,h,w]\n",
        "        diffX, diffY = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        return self.conv(torch.cat([x2, x1], dim=1))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py\n",
        "        wf=6 # width factor, 2^6 = 64 -> 1024\n",
        "        depth=4\n",
        "        self.inc = DoubleConv(in_ch, 2 ** wf)\n",
        "        self.down_list = nn.ModuleList([Down(2 ** (wf + i), 2 ** (wf + i+1)) for i in range(depth)])\n",
        "        self.up_list = nn.ModuleList([Up(2 ** (wf + i+1), 2 ** (wf + i)) for i in reversed(range(depth))])\n",
        "        self.outc = nn.Conv2d(2 ** wf, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        x = self.inc(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            blocks.append(x)\n",
        "            x = down(x)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "        return self.outc(x)\n",
        "\n",
        "\n",
        "unet = UNet(3, 5)\n",
        "x=torch.rand(4,3,64,64)\n",
        "# x=torch.rand(4,3,128,128)\n",
        "# x=torch.rand(4,3,28,28)\n",
        "out = unet(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYuDUKoWrvjl",
        "outputId": "e44ededd-e08d-4723-c9fb-b2bb2ef8e598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 32, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# @title test res, up, down\n",
        "\n",
        "in_ch, time_embed_dim, out_ch = 32,16,64\n",
        "res = ResBlock(in_ch, time_embed_dim, out_channels=out_ch, dims=2, use_scale_shift_norm=False)\n",
        "x=torch.rand((4,32,16,16))\n",
        "temb=torch.rand((4,16))\n",
        "out = res(x, temb)\n",
        "\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_embed_dim, context_dim, num_heads=None, dim_head=8, dims=2, use_scale_shift_norm=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        if num_heads==None: num_heads = out_ch // dim_head\n",
        "        layers = [\n",
        "            ResBlock(in_ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm),\n",
        "            AttentionBlock(out_ch, num_heads=num_heads, num_head_channels=dim_head,),\n",
        "            SpatialTransformer(out_ch, num_heads, dim_head, depth=1, context_dim=context_dim),\n",
        "            Downsample(out_ch, dims=dims, out_channels=out_ch),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, context=None):\n",
        "        return self.seq(x, temb, context)\n",
        "\n",
        "down=Down(32,32,16, 16)\n",
        "x=torch.rand((4,32,16,16))\n",
        "temb=torch.rand((4,16))\n",
        "cond=torch.rand((4,16))\n",
        "out = down(x, temb,cond)\n",
        "print(out.shape)\n",
        "# print(down)\n",
        "\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_embed_dim, context_dim, num_heads=None, dim_head=8, dims=2, use_scale_shift_norm=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        if num_heads==None: num_heads = out_ch // dim_head\n",
        "        layers = [\n",
        "            Upsample(in_ch, dims=dims, out_channels=in_ch),\n",
        "            ResBlock(in_ch, time_embed_dim, dropout, out_channels=out_ch, dims=dims, use_scale_shift_norm=use_scale_shift_norm),\n",
        "            SpatialTransformer(out_ch, num_heads, dim_head, depth=1, context_dim=context_dim),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, context=None):\n",
        "        return self.seq(x, temb, context)\n",
        "\n",
        "up=Up(64,32,16, 16)\n",
        "x=torch.rand((4,64,16,16))\n",
        "temb=torch.rand((4,16))\n",
        "cond=torch.rand((4,16))\n",
        "out = up(x, temb,cond)\n",
        "print(out.shape)\n",
        "# print(down)\n",
        "\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, conv_dim=2, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            Downsample(in_ch, conv_dim=conv_dim, out_ch=in_ch),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch, conv_dim=conv_dim),\n",
        "            # AttentionBlock(out_ch, n_head=n_head, d_head=d_head),\n",
        "            SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, cond=None):\n",
        "        return self.seq(x, temb, cond)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, conv_dim=2):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            # Upsample(in_ch, conv_dim=conv_dim, out_ch=in_ch),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch, conv_dim=conv_dim),\n",
        "            SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            Upsample(out_ch, conv_dim=conv_dim, out_ch=out_ch),\n",
        "            ]\n",
        "        self.seq = TimestepEmbedSequential(*layers)\n",
        "\n",
        "    def forward(self, x, temb=None, cond=None):\n",
        "        return self.seq(x, temb, cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pscpq6FqF5M",
        "outputId": "aa518dcd-2320-4d8e-c4fe-c269591ffa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title RoPE\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "        rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "        x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "        rot_x = x * rot_emb\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "\n",
        "dim=16\n",
        "seq_len=512\n",
        "rope = RoPE(dim, seq_len, base=10000)\n",
        "x = torch.rand(4,64,dim)\n",
        "out = rope(x)\n",
        "t = torch.rand(4,1)\n",
        "\n",
        "# [batch] -> []\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "# rot_emb = rope.rot_emb\n",
        "# print(rot_emb.shape)\n",
        "# print(rot_emb[:7])\n",
        "# # rot_emb = rot_emb.reshape(seq_len, dim // 2, 2)\n",
        "# # print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3oBrwquUT1fV"
      },
      "outputs": [],
      "source": [
        "# @title 2D RoPE\n",
        "# https://github.com/naver-ai/rope-vit/blob/main/models/vit_rope.py\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/deit\n",
        "# https://github.com/meta-llama/codellama/blob/main/llama/model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "# from timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from deit.models_v2 import vit_models, Layer_scale_init_Block, Attention\n",
        "\n",
        "def init_random_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
        "    freqs_x = []\n",
        "    freqs_y = []\n",
        "    mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    for i in range(num_heads):\n",
        "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
        "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
        "        fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
        "        freqs_x.append(fx)\n",
        "        freqs_y.append(fy)\n",
        "    freqs_x = torch.stack(freqs_x, dim=0)\n",
        "    freqs_y = torch.stack(freqs_y, dim=0)\n",
        "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
        "    return freqs\n",
        "\n",
        "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
        "    N = t_x.shape[0]\n",
        "    depth = freqs.shape[1]\n",
        "    # No float 16 for this range\n",
        "    with torch.cuda.amp.autocast(enabled=False):\n",
        "        freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 100.0):\n",
        "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
        "\n",
        "    t_x, t_y = init_t_xy(end_x, end_y)\n",
        "    freqs_x = torch.outer(t_x, freqs_x)\n",
        "    freqs_y = torch.outer(t_y, freqs_y)\n",
        "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
        "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
        "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
        "\n",
        "def init_t_xy(end_x: int, end_y: int):\n",
        "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
        "    t_x = (t % end_x).float()\n",
        "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
        "    return t_x, t_y\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
        "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
        "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
        "\n",
        "\n",
        "class RoPEAttention(Attention):\n",
        "    \"\"\"Multi-head Attention block with rotary position embeddings.\"\"\"\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q[:, :, 1:], k[:, :, 1:] = apply_rotary_emb(q[:, :, 1:], k[:, :, 1:], freqs_cis=freqs_cis)\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class RoPE_Layer_scale_init_Block(Layer_scale_init_Block):\n",
        "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "    # with slight modifications\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs[\"Attention_block\"] = RoPEAttention\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), freqs_cis=freqs_cis))\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class rope_vit_models(vit_models):\n",
        "    def __init__(self, rope_theta=100.0, rope_mixed=False, use_ape=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        img_size = kwargs['img_size'] if 'img_size' in kwargs else 224\n",
        "        patch_size = kwargs['patch_size'] if 'patch_size' in kwargs else 16\n",
        "        num_heads = kwargs['num_heads'] if 'num_heads' in kwargs else 12\n",
        "        embed_dim = kwargs['embed_dim'] if 'embed_dim' in kwargs else 768\n",
        "        mlp_ratio = kwargs['mlp_ratio'] if 'mlp_ratio' in kwargs else 4.\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        self.use_ape = use_ape\n",
        "        if not self.use_ape:\n",
        "            self.pos_embed = None\n",
        "\n",
        "        self.rope_mixed = rope_mixed\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            self.compute_cis = partial(compute_mixed_cis, num_heads=self.num_heads)\n",
        "\n",
        "            freqs = []\n",
        "            for i, _ in enumerate(self.blocks):\n",
        "                freqs.append(\n",
        "                    init_random_2d_freqs(dim=embed_dim // num_heads, num_heads=num_heads, theta=rope_theta)\n",
        "                )\n",
        "            freqs = torch.stack(freqs, dim=1).view(2, len(self.blocks), -1)\n",
        "            self.freqs = nn.Parameter(freqs.clone(), requires_grad=True)\n",
        "\n",
        "            t_x, t_y = init_t_xy(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.register_buffer('freqs_t_x', t_x)\n",
        "            self.register_buffer('freqs_t_y', t_y)\n",
        "        else:\n",
        "            self.compute_cis = partial(compute_axial_cis, dim=embed_dim//num_heads, theta=rope_theta)\n",
        "\n",
        "            freqs_cis = self.compute_cis(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
        "            self.freqs_cis = freqs_cis\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'freqs'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.use_ape:\n",
        "            pos_embed = self.pos_embed\n",
        "            if pos_embed.shape[-2] != x.shape[-2]:\n",
        "                img_size = self.patch_embed.img_size\n",
        "                patch_size = self.patch_embed.patch_size\n",
        "                pos_embed = pos_embed.view(\n",
        "                    1, (img_size[1] // patch_size[1]), (img_size[0] // patch_size[0]), self.embed_dim\n",
        "                ).permute(0, 3, 1, 2)\n",
        "                pos_embed = F.interpolate(\n",
        "                    pos_embed, size=(H // patch_size[1], W // patch_size[0]), mode='bicubic', align_corners=False\n",
        "                )\n",
        "                pos_embed = pos_embed.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            x = x + pos_embed\n",
        "\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        if self.rope_mixed:\n",
        "            if self.freqs_t_x.shape[0] != x.shape[1] - 1:\n",
        "                t_x, t_y = init_t_xy(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "                t_x, t_y = t_x.to(x.device), t_y.to(x.device)\n",
        "            else:\n",
        "                t_x, t_y = self.freqs_t_x, self.freqs_t_y\n",
        "            freqs_cis = self.compute_cis(self.freqs, t_x, t_y)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis[i])\n",
        "        else:\n",
        "            if self.freqs_cis.shape[0] != x.shape[1] - 1:\n",
        "                freqs_cis = self.compute_cis(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
        "            else:\n",
        "                freqs_cis = self.freqs_cis\n",
        "            freqs_cis = freqs_cis.to(x.device)\n",
        "\n",
        "            for i , blk in enumerate(self.blocks):\n",
        "                x = blk(x, freqs_cis=freqs_cis)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def adjust_pos_embed_size(model, state_dict):\n",
        "    # interpolate position embedding\n",
        "    if 'pos_embed' in state_dict:\n",
        "        pos_embed_checkpoint = state_dict['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "        # only the position tokens are interpolated\n",
        "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "        pos_tokens = torch.nn.functional.interpolate(\n",
        "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "        state_dict['pos_embed'] = new_pos_embed\n",
        "\n",
        "    return state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wdRZxhI7-Xuh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock down\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\"\"\"\n",
        "    def __init__(self, in_ch, n_head=1, d_head=8):\n",
        "        super().__init__()\n",
        "        # self.n_head = n_head\n",
        "        self.n_head = in_ch // num_head_in_ch\n",
        "        self.norm = normalization(in_ch)\n",
        "        self.qkv = conv_nd(1, in_ch, in_ch * 3, 1)\n",
        "        # self.qkv = Conv1D(in_ch, in_ch * 3, 1)\n",
        "        self.attention = QKVAttention(self.n_head)\n",
        "        # self.proj_out = zero_module(conv_nd(1, in_ch, in_ch, 1))\n",
        "        self.proj_out = conv_nd(1, in_ch, in_ch, 1)\n",
        "        nn.init.zeros_(self.proj_out.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv): # [N, 3*H*C, T]\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "\n",
        "\n",
        "        # from einops import rearrange\n",
        "        # from einops.layers.torch import Rearrange\n",
        "        # q = rearrange(q, 'b n (h d) -> b h n d', h = self.n_heads)\n",
        "        # q = rearrange(q, 'n (3 h c) t -> 3 n (h c) t', h = self.n_heads, c = ch)\n",
        "\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=1) # [N, H*C, T]\n",
        "        scale = ch**(-1/4)\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),)  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight, dim=-1)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "\n",
        "        return a.reshape(bs, -1, length) # [N, H*C, T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K821B-KoHISD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test inspect.signature\n",
        "# model = Downsample(8)\n",
        "# model = ResBlock(32,8)\n",
        "# model = SpatialTransformer(32,32,32)\n",
        "\n",
        "import inspect\n",
        "# # class Seq(nn.Sequential, TimestepBlock):\n",
        "class Seq(nn.Sequential):\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            args = [x]\n",
        "            if 'emb' in params: args.append(emb)\n",
        "            if 'cond' in params: args.append(cond)\n",
        "            x = layer(*args)\n",
        "            # print('x' in sig.parameters.keys())\n",
        "            # if isinstance(layer, TimestepBlock): x = layer(x, emb)\n",
        "            # elif isinstance(layer, SpatialTransformer): x = layer(x, cond)\n",
        "            # else: x = layer(x)\n",
        "            # pass\n",
        "        return x\n",
        "# sig = inspect.signature(model.forward)\n",
        "# print(sig)\n",
        "# print(sig.parameters.items())\n",
        "# print(sig.parameters.keys())\n",
        "# print('x' in sig.parameters.keys())\n",
        "# # print(type(sig))\n",
        "\n",
        "model = Seq(\n",
        "    Downsample(8),\n",
        "    ResBlock(32,8),\n",
        "    SpatialTransformer(32,32,32),\n",
        ")\n",
        "x = torch.rand(4)\n",
        "emb = torch.rand(4)\n",
        "cond = torch.rand(4)\n",
        "out = model(x, emb,cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIxTx8tT-nKH",
        "outputId": "ef51c676-a1c1-46bd-8d65-a114a9029756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48.8 ns ± 0.365 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
          ]
        }
      ],
      "source": [
        "# model = Downsample(8)\n",
        "model = ResBlock(32,8)\n",
        "# model = SpatialTransformer(32,32,32)\n",
        "# %timeit inspect.signature(model.forward).parameters.keys()\n",
        "# %timeit isinstance(model, TimestepBlock)\n",
        "%timeit model.emb\n",
        "# 14.4 µs 14.4 µs 16.7 µs; 80.5 ns 79.9 ns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUyK5FbdKFFV",
        "outputId": "8d81ff71-5511-4c81-a14c-9ae596c471f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def fun(a,b):\n",
            "    return a*b \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def fun(a,b):\n",
        "    return a*b\n",
        "print(inspect.getsource(fun))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}