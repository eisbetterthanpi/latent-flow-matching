{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/latent-flow-model/blob/main/latent_flow_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOiDHI7taUKF"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B8Pm-Fw6jn4A"
      },
      "outputs": [],
      "source": [
        "# @title mha me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.lin(out) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.self(self.norm1(x))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        return x.transpose(1,2).reshape(*bchw)\n",
        "\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# model = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = model(x, cond, mask)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j9wUA7Vk0Y03"
      },
      "outputs": [],
      "source": [
        "# @title rope & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "        if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "        return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "\n",
        "# rotemb = RotEmb(10)\n",
        "# seq_len=10\n",
        "# pos = torch.linspace(0,1,seq_len).to(device)#.unsqueeze(-1)\n",
        "# rot_emb = rotemb(pos)\n",
        "# print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B4R77KQvNgj4"
      },
      "outputs": [],
      "source": [
        "# @title unet me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        # self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim, cond_dim, n_head=None, d_head=8, updown=False, r=2):\n",
        "        super().__init__()\n",
        "        if updown=='down': in_ch = in_ch*r**2\n",
        "        elif updown=='up': out_ch = out_ch*r**2\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1))\n",
        "        # self.init_conv = CrossEmbedLayer(in_ch, dim_out=d_model, kernel_sizes=(3, 7, 15), stride=1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7//2)\n",
        "\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = ch_list[-1]*2**2 # 512\n",
        "        self.middle_block = Seq(\n",
        "            nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x) if self.in_ch!=self.d_model else x\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x) #if self.out_ch!=self.d_model else x\n",
        "\n",
        "\n",
        "\n",
        "# # # 64,64 -vae-> 16,16 -unet->\n",
        "# batch = 64\n",
        "# cond_dim=10\n",
        "# in_ch = 3\n",
        "# model = UNet(in_ch=in_ch, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 5311187\n",
        "# # # print(model)\n",
        "\n",
        "# # x=torch.rand((batch,in_ch,16,16),device=device)\n",
        "# x=torch.rand((batch,in_ch,64,64),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# # # cond_emb = nn.Embedding(10, cond_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtEfz1rRx1t8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "16d12657-f9b8-4230-dbf8-9beae1788e7b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b3eef7b448c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters(): print(name, param.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JxGF8bYeGDA2"
      },
      "outputs": [],
      "source": [
        "# @title UIB\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential( # norm,act,conv\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, in_ch, kernel, 1, kernel//2, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, mult*in_ch, kernel, 1, kernel//2, groups=mult*in_ch, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, out_ch, 1, bias=False),\n",
        "            # nn.BatchNorm2d(mult*in_ch), act, zero_module(nn.Conv2d(mult*in_ch, out_ch, 1, bias=False)),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# # in_ch, out_ch = 16,3\n",
        "# in_ch, out_ch = 3,16\n",
        "# model = UIB(in_ch, out_ch)\n",
        "# x = torch.rand(128, in_ch, 64, 64)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2nu4Dzma_cD5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        # self.block = nn.Sequential(\n",
        "        self.block = Seq(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            # nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "            nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        return self.block(x, emb) + self.res_conv(x)\n",
        "\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "\n",
        "# class Res(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#     def forward(self, x): return x + self.model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "form",
        "id": "GjvZZswH1_KR"
      },
      "outputs": [],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1, emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, emb_dim), nn.PixelShuffle(r))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, emb_dim))\n",
        "        # # elif in_ch!=out_ch: self.net = ResBlock(in_ch, out_ch)\n",
        "        # else: self.net = ResBlock(in_ch, out_ch, emb_dim)\n",
        "        if self.r>1: self.net = Seq(ResBlock(in_ch, out_ch*r**2, emb_dim), nn.PixelShuffle(r))\n",
        "        elif self.r<1: self.net = Seq(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, emb_dim))\n",
        "        else: self.net = Seq(ResBlock(in_ch, out_ch, emb_dim))\n",
        "\n",
        "    def forward(self, x, emb=None):\n",
        "        return self.net(x, emb)\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1, emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w]\n",
        "        out = self.block(x, emb)\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        return out + shortcut\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "f0E7ovklT83O",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262e365c-8be4-4c1a-ea55-57d75195c314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16, 32, 48, 64]\n",
            "U_DiT down torch.Size([64, 16, 64, 64])\n",
            "U_DiT down torch.Size([64, 32, 64, 64])\n",
            "U_DiT mid1 torch.Size([64, 48, 32, 32])\n",
            "U_DiT mid2 torch.Size([64, 48, 32, 32])\n",
            "U_DiT up torch.Size([64, 48, 32, 32])\n",
            "U_DiT up torch.Size([64, 32, 64, 64])\n",
            "torch.Size([64, 3, 64, 64])\n",
            "1105203\n"
          ]
        }
      ],
      "source": [
        "# @title U-DiT next\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, cond=None):\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             if 'cond' in layer._fwdparams: args.append(cond)\n",
        "#             x = layer(*args)\n",
        "#         return x\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = x #+ self.layer(x)\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "# conv res pixeldown\n",
        "\n",
        "\n",
        "class DownSample_Attn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.scale = d_head**-.5 # v1\n",
        "\n",
        "        # self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # b, _, h, w = x.size()\n",
        "        bchw = x.shape\n",
        "        # x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        x = x.transpose(-2,-1).reshape(bchw)\n",
        "        return self.lin(x)\n",
        "\n",
        "# # me\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_mult=1):\n",
        "        super().__init__()\n",
        "        # d_model = dim*ff_mult\n",
        "        self.project_in = nn.Sequential(nn.BatchNorm2d(d_model), nn.GELU())\n",
        "        self.dwconv = nn.ModuleList([\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 3//2),\n",
        "            nn.Conv2d(d_model, d_model, 1, 1, 1//2),\n",
        "        ])\n",
        "        # self.project_out = nn.Sequential(nn.GELU(), nn.Conv2d(d_model, dim, kernel_size=1)) # act, SEblock\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.project_in(x) # no\n",
        "        x = x + sum([conv(h) for conv in self.dwconv])\n",
        "        # x = self.project_out(out)\n",
        "        return x\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, d_model, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.rot_emb = RotEmb(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, d_model), nn.SiLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "        )\n",
        "    def forward(self, t): return self.mlp(self.rot_emb(t))\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    \"\"\"A IPT block with adaptive layer norm zero (adaLN-Zero) conIPTioning.\"\"\"\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = DownSample_Attn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        # self.mlp = FeedForward(d_model)\n",
        "        # self.mlp = UIB(d_model, mult=4)\n",
        "        self.mlp = ResBlock(out_ch)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(cond_dim, 6 * d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, d_model, out_ch):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Conv2d(d_model, d_model, kernel_size=3, stride=1, padding=1)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(d_model, 2*d_model))\n",
        "        )\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.out_proj = zero_module(nn.Conv2d(d_model, out_ch, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        shift, scale = self.adaLN_modulation(cond)[...,None,None].chunk(2, dim=1)\n",
        "        # print('FinalLayer', x.shape, shift.shape, scale.shape)\n",
        "        x = self.in_proj(x)\n",
        "        x = (1 + scale) * self.norm(x) + shift\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        if n_heads==None: n_heads = out_ch // d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            # *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.t_embedder = TimestepEmbedder(d_model*3)\n",
        "        self.y_embedder = nn.Linear(cond_dim, d_model*3)\n",
        "\n",
        "        self.x_embedder = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 3\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            # U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads)\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        self.final_layer = FinalLayer(d_model, in_ch)\n",
        "        # # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        x = self.x_embedder(x) # (N, C, H, W)\n",
        "        c123 = self.t_embedder(t) + self.y_embedder(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "\n",
        "        x = self.final_layer(x, cond=cond[0]) # (N, T, patch_size ** 2 * out_ch)\n",
        "        return x\n",
        "        # return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFXiQfSn1i3u",
        "outputId": "8799a8ab-0021-4f84-da9e-8c0ff6cb8454",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16, 32, 48, 64]\n",
            "1085395\n",
            "torch.Size([64, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title unet to dit\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        if n_heads==None: n_heads = out_ch // d_head\n",
        "        # if updown=='down': in_ch = in_ch*r**2\n",
        "        # elif updown=='up': out_ch = out_ch*r**2\n",
        "        # print('lvl', in_ch, out_ch, r)\n",
        "        # self.seq = Seq(\n",
        "        #     nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "        #     ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "        #     AttentionBlock(out_ch, d_head, cond_dim),\n",
        "        #     # U_DiTBlock(out_ch, cond_dim, n_heads),\n",
        "        #     nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "        # )\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            # *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_heads=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_heads = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "\n",
        "        # ch = ch_list[-1]*2#**2 # 512\n",
        "        # self.middle_block = Seq(\n",
        "        #     # nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "        #     AttentionBlock(ch, d_head, cond_dim),\n",
        "        #     # U_DiTBlock(ch, cond_dim, n_heads),\n",
        "        #     # ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        # )\n",
        "\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            AttentionBlock(ch_list[depth], d_head, cond_dim),\n",
        "            # U_DiTBlock(ch_list[depth], cond_dim, n_heads)\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x) if self.in_ch!=self.d_model else x\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print(\"unet fwd down\", x.shape)\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        # print(\"unet fwd mid1\", x.shape)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        # print(\"unet fwd mid2\", x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            # print(\"unet fwd up\", x.shape)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x) #if self.out_ch!=self.d_model else x\n",
        "\n",
        "\n",
        "\n",
        "# # 64,64 -vae-> 16,16 -unet->\n",
        "batch = 64\n",
        "cond_dim=10\n",
        "in_ch = 3\n",
        "model = UNet(in_ch=in_ch, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 5311187\n",
        "# # print(model)\n",
        "\n",
        "# x=torch.rand((batch,in_ch,16,16),device=device)\n",
        "x=torch.rand((batch,in_ch,64,64),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# # cond_emb = nn.Embedding(10, cond_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "n1Nfr2mmxuzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8447a9-3039-433e-b6bd-ef954df2ddbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "146512\n",
            "261712\n"
          ]
        }
      ],
      "source": [
        "# for name, param in model.named_parameters(): print(name, param.numel())\n",
        "print(sum(p.numel() for p in model.down_list.parameters() if p.requires_grad)) # 19683\n",
        "print(sum(p.numel() for p in model.up_list.parameters() if p.requires_grad)) # 19683\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kJOnXmd2jGga"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/DiT\n",
        "https://github.com/facebookresearch/DiT/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
        "\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#               Embedding Layers for Timesteps and Class Labels                 #\n",
        "#################################################################################\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size), nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    @staticmethod\n",
        "    def timestep_embedding(t, dim, max_period=10000):\n",
        "        \"\"\"\n",
        "        Create sinusoidal timestep embeddings.\n",
        "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "                          These may be fractional.\n",
        "        :param dim: the dimension of the output.\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        :return: an (N, D) Tensor of positional embeddings.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=t.device)\n",
        "        args = t[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                 Core DiT Model                                #\n",
        "#################################################################################\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    def __init__(self, input_size=32, patch_size=2, in_channels=4, hidden_size=1152, depth=28,\n",
        "        num_heads=16, mlp_ratio=4.0, class_dropout_prob=0.1, num_classes=1000, learn_sigma=True):\n",
        "        super().__init__()\n",
        "        # self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        # self.patch_size = patch_size\n",
        "        # self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Initialize label embedding table:\n",
        "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], [b], [b]\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)                   # (N, D)\n",
        "        y = self.y_embedder(y, self.training)    # (N, D)\n",
        "        c = t + y                                # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)                      # (N, T, D)\n",
        "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
        "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
        "        return x\n",
        "\n",
        "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "        model_out = self.forward(combined, t, y)\n",
        "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
        "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
        "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
        "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "# def DiT_XL_2/4/8(**kwargs): return DiT(depth=28, hidden_size=1152, patch_size=2/4/8, num_heads=16, **kwargs)\n",
        "# def DiT_L_2(**kwargs): return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "# def DiT_B_2(**kwargs): return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "# def DiT_S_2(**kwargs): return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/baofff/U-ViT/blob/main/libs/uvit.py#L138\n"
      ],
      "metadata": {
        "id": "IWmZcwoEft7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),) # do not normalise! want img in [0,1)\n",
        "test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 128 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ubkj5kR6Or8V"
      },
      "outputs": [],
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bHFHA_cXOvA5"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train_data = BufferDataset(buffer) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 512 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bf2bipgghY7O"
      },
      "outputs": [],
      "source": [
        "# @title LogitNormal\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogitNormal(dist.Distribution):\n",
        "    def __init__(self, mu=0, std=.5):\n",
        "        super().__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "        self._normal = dist.Normal(mu, std) # https://pytorch.org/docs/stable/distributions.html#normal\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        eps = self._normal.rsample(sample_shape)\n",
        "        return torch.sigmoid(eps) # https://en.wikipedia.org/wiki/Logit-normal_distribution\n",
        "\n",
        "logit_normal = LogitNormal()\n",
        "# samples = logit_normal.rsample((10,))\n",
        "# print(samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QRPa3VWuQDIW"
      },
      "outputs": [],
      "source": [
        "# @title sampling timestep\n",
        "# def InverseSigmoid(x): return torch.log(x/(1-x))\n",
        "# def Normal(x, mu=0, std=.5): return torch.exp(-.5*((x-mu)/std)**2)/(std*(2*torch.pi)**2)\n",
        "# def LogitNormalPDF(x, mu=0, std=.5): return torch.nan_to_num(Normal(logit(x), mu, std) * 1/(x*(1-x)))\n",
        "\n",
        "# def invlogit(x): return torch.exp(x)/(1+torch.exp(x))\n",
        "# def InvLogitNormalCDF(x, mu=0, std=.5):\n",
        "#     cdf = invlogit(torch.erfinv(2*x-1)*(2**.5*std)+mu)\n",
        "#     cdf[x==1.] = 1 # lol, replace nan with 1 when x=1\n",
        "#     return cdf\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "def Cosine(x): return .5*(-torch.cos(torch.pi*x)+1) # _/-\n",
        "def Polynomial(x): return -2*x**3 + 3*x**2 # -2x^3 + 3x^2 # _/-\n",
        "def ACosine(x): return torch.acos(1-2*x)/torch.pi # /-/ # x = acos(1-2y)/pi\n",
        "def InvertCubic(x): return (x-1)**3+1 # /-\n",
        "def InvertExp(x, a=4): return (1-torch.exp(-a*x)) / (1-torch.exp(torch.tensor(-a))) # /-\n",
        "\n",
        "\n",
        "a, b = .0, 0\n",
        "def bezier(t, x0=0,y0=0, x1=a,y1=b, x2=1-a,y2=1-b, x3=1,y3=1):\n",
        "    # print(x1,y1)\n",
        "    # return ((1-t)*((1-t)*((1-t)*x0+t*x1)+t*((1-t)*x1+t*x2))+t*((1-t)*((1-t)*x1+t*x2)+t*((1-t)*x2+t*x3)), (1-t)*((1-t)*((1-t)*y0+t*y1) +t*((1-t)*y1+t*y2))+t*((1-t)*((1-t)*y1+t*y2) +t*((1-t)*y2+t*y3)))\n",
        "    return (1-t)*((1-t)*((1-t)*x0+t*x1)+t*((1-t)*x1+t*x2))+t*((1-t)*((1-t)*x1+t*x2)+t*((1-t)*x2+t*x3))\n",
        "    # return (1-t)*((1-t)*((1-t)*y0+t*y1) +t*((1-t)*y1+t*y2))+t*((1-t)*((1-t)*y1+t*y2) +t*((1-t)*y2+t*y3))\n",
        "\n",
        "\n",
        "# x = torch.linspace(0, 1, 30)\n",
        "# y=x\n",
        "# y = LogitNormalCDF(x, mu=0, std=3) # _/- for std<1.8; /-/ for std>1.8\n",
        "# y = Cosine(x) # _/-\n",
        "# y = ACosine(x) # /-/\n",
        "# y = Polynomial(x) # _/-\n",
        "# y = InvertCubic(x) # /-\n",
        "# y = InvertExp(x) # /-\n",
        "# y = bezier(x) # _/-\n",
        "# print(x, y)\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(x, y)\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OFc76OzlFnE1"
      },
      "outputs": [],
      "source": [
        "# @title Sampling\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def reverse_flow(unet, cond, timesteps=25): # [n_samples, cond_dim]\n",
        "    unet.eval()\n",
        "    i = torch.linspace(0, 1, timesteps+1)\n",
        "    # y = i # linear\n",
        "    # y = LogitNormalCDF(i, mu=0, std=3.) # .5 _/- for std<1.8; 3 /-/ for std>1.8\n",
        "    y = LogitNormalCDF(i, mu=-.0, std=2.7) # .5 _/- for std<1.8; 3 /-/ for std>1.8\n",
        "    # y = Cosine(i) # _/-\n",
        "    # y = ACosine(i) # /-/\n",
        "    # y = Polynomial(i) # _/-\n",
        "    # y = InvertCubic(i) # /-\n",
        "    # y = InvertExp(i) # /-\n",
        "    # y = bezier(i) # _/-\n",
        "\n",
        "    dt = y[1:]-y[:-1]\n",
        "    num_samples = cond.shape[0]\n",
        "    # x = torch.randn((num_samples, unet.in_ch, 16,16), device=device)\n",
        "    x = torch.randn((num_samples, 3, 64,64), device=device)\n",
        "    # cond = cond.repeat(num_samples,1) # [n_samples, cond_dim]\n",
        "    for y, dt in zip(y, dt):\n",
        "        # print(y, dt)\n",
        "        t = torch.full((num_samples,), y, device=device)  # Current time # [num_samples] 1. # torch.tensor(i * dt, device=device).repeat(n_samples)\n",
        "        with torch.no_grad():\n",
        "            model = lambda y,t: -unet(y, t, cond)\n",
        "            v = model(x, t)\n",
        "            x = x - dt * v # Euler update # 25steps:1sec\n",
        "\n",
        "            # k1 = model(x, t)\n",
        "            # k2 = model(x - 0.5 * dt * k1, t - 0.5 * dt)\n",
        "            # k3 = model(x - 0.5 * dt * k2, t - 0.5 * dt)\n",
        "            # k4 = model(x - dt * k3, t - dt)\n",
        "            # x = x - (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4) # RK4 update # 25steps:4.5sec\n",
        "    return x\n",
        "\n",
        "\n",
        "# # cond = F.one_hot(torch.tensor([3]*16, device=device), num_classes=10).to(torch.float)\n",
        "# cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "# # img_ = reverse_flow(model, cond, timesteps=10)\n",
        "# img_ = model.sample(cond)\n",
        "# # # plt.imshow(img_.cpu().squeeze())\n",
        "# # # plt.show()\n",
        "# imshow(torchvision.utils.make_grid(img_.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbSq3Zx7aRL-"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "iJ1hbnSC12tA",
        "outputId": "06b6233e-ba21-4175-fb09-deeda7034c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1007872\n",
            "torch.Size([4, 65536, 16, 16])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [256, 32, 7, 7], expected input[4, 4096, 64, 64] to have 32 channels, but got 4096 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f55ed1538279>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f55ed1538279>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 32, 7, 7], expected input[4, 4096, 64, 64] to have 32 channels, but got 4096 channels instead"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=256, out_ch=None, kernels=[7,5], mult=[1]):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.d_model, self.out_ch = in_ch, d_model, out_ch\n",
        "        d_list=[d_model*m for m in mult]\n",
        "        in_list, out_list = [in_ch, *d_list[:-1]], [*d_list[:-1], out_ch]\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=1/2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act,) for i, (in_dim, out_dim, kernel) in enumerate(zip(in_list, out_list, kernels))], # conv,norm,act except for last layer: no norm\n",
        "            # PixelShuffleConv(in_ch, d_list[0], 7, r=1/2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConv(d_list[0], out_ch, 5, r=1/2), act,\n",
        "            # nn.Conv2d(in_ch, out_ch, 7, 2, 7//2), nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # nn.PixelUnshuffle(4), init_conv(nn.Conv2d(3*4**2, d_list[0]*1**2, 7, 1, padding=7//2), out_r=1, in_r=4),\n",
        "            # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3*2**2, d_list[0]*2**2, 7, 1, padding=7//2), out_r=2, in_r=2), nn.PixelUnshuffle(2),\n",
        "            init_conv(nn.Conv2d(3*1**2, d_list[0]*4**2, 7, 1, padding=7//2), out_r=4, in_r=1), nn.PixelUnshuffle(4),\n",
        "        )\n",
        "            # nn.PixelUnshuffle(4), ResBlock(in_ch*4**2, out_ch, emb_dim=out_ch), AttentionBlock(out_ch),\n",
        "\n",
        "        # nn.init.zeros_(self.encoder.parameters()[-1])\n",
        "        # self.encoder[-2].apply(self.zero_conv_)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), *(nn.BatchNorm2d(out_dim), act) if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # PixelShuffleConv(out_ch, d_list[0], 5, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConv(d_list[0], in_ch, 7, r=2),\n",
        "            # nn.Upsample(scale_factor=2), nn.ConvTranspose2d(out_ch, in_ch, 7, 2, 7//2, output_padding=1),# nn.BatchNorm2d(d_list[1]), nn.SiLU(),\n",
        "\n",
        "            # init_conv(nn.Conv2d(2*1**2, d_list[0]*4**2, 7, 1, padding=7//2), out_r=4, in_r=1), nn.PixelShuffle(4),\n",
        "            # nn.PixelShuffle(2), init_conv(nn.Conv2d(2*2**2, d_list[0]*2**2, 7, 1, padding=7//2), out_r=2, in_r=2), nn.PixelShuffle(2),\n",
        "            nn.PixelShuffle(4), init_conv(nn.Conv2d(2*4**2, d_list[0]*1**2, 7, 1, padding=7//2), out_r=1, in_r=4),\n",
        "        )\n",
        "            # ResBlock(out_ch, in_ch*4**2, emb_dim=in_ch*4**2), AttentionBlock(in_ch*4**2), nn.PixelShuffle(4),\n",
        "\n",
        "        # for param in self.parameters():\n",
        "        # nn.init.zeros_(self.decoder.parameters()[-1])\n",
        "    #     self.decoder[-1].apply(self.zero_conv_)\n",
        "\n",
        "    # def zero_conv_(self, conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "    #     if isinstance(conv, nn.Conv2d):\n",
        "    #         nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x): return self.encoder(x)\n",
        "    def decode(self, x): return self.decoder(x)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "# model_ch=16\n",
        "d_list=[16, 16] # [16,32]\n",
        "k_list=[7,5] # [7,5]\n",
        "# model = PixelAE(in_ch=in_ch, d_list=d_list, k_list=k_list)\n",
        "model = PixelAE(in_ch=in_ch, out_ch=16).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,in_ch,64,64), device=device)\n",
        "enc = model.encode(input)\n",
        "print(enc.shape)\n",
        "out = model.decode(enc)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q43yKETuk3P"
      },
      "outputs": [],
      "source": [
        "model = PixelAE()\n",
        "# nn.init.zeros_(model.decoder.parameters()[-1])\n",
        "# print(model.decoder.parameters())\n",
        "# print(model.decoder[-2])\n",
        "# nn.init.zeros_(model.decoder[-2])\n",
        "# model.decoder[-2].apply(nn.init.zeros_)\n",
        "\n",
        "def init_conv_(conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "    if isinstance(conv, nn.Conv2d):\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "model.decoder[-2].apply(init_conv_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nvBRca98P-GW"
      },
      "outputs": [],
      "source": [
        "# @title latent flow model\n",
        "\n",
        "class LFM(nn.Module): # latent flow model\n",
        "    def __init__(self, in_ch=3, d_list=[16, 3], d_model=16, cond_dim=16, depth=3, ae=None):\n",
        "        super().__init__()\n",
        "        self.ae = ae or PixelAE(in_ch, d_model)\n",
        "        self.unet = UNet(in_ch=in_ch, d_model=d_model, cond_dim=cond_dim, depth=3)\n",
        "\n",
        "    def loss(self, img, cond): # [b,c,h,w], [b,cond_dim]\n",
        "        x1 = self.ae.encode(img)\n",
        "        img_ = self.ae.decode(x1)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        fm_loss = otfm_loss(self.unet, x1.detach(), cond)\n",
        "        return ae_loss, fm_loss\n",
        "        # loss = ae_loss + fm_loss\n",
        "        # return loss\n",
        "\n",
        "    def ae_loss(self, img):\n",
        "        img_ = self.ae(img)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        return ae_loss\n",
        "\n",
        "\n",
        "    # self.unet(x, t=None, cond=None)\n",
        "    # def forward(self, cond, timesteps=10): # [N, C, ...]\n",
        "    def sample(self, cond=None, timesteps=10):\n",
        "        self.eval()\n",
        "        # cond = F.one_hot(torch.tensor([4]*16, device=device), num_classes=10).to(torch.float)\n",
        "        # if cond is None: cond = F.one_hot(torch.arange(16, dtype=torch.float, device=device)%10, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x1_ = reverse_flow(self.unet, cond, timesteps=timesteps)\n",
        "            img_ = self.ae.decode(x1_)\n",
        "        return img_\n",
        "\n",
        "model = LFM(d_list=[16, 16], d_model=16, cond_dim=10, depth=3).to(device)\n",
        "# model = LFM(ae=model.ae, d_list=[16, 16], d_model=16, cond_dim=10, depth=3).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=3e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfsabyM8P7q3"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def otfm_loss(model, x1, cond, sig_min = 0.001, eps = 1e-5): # UNetModel, [b,c,h,w], [b,cond_dim] # https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "    batch = x1.size(0)\n",
        "    # t = torch.rand((batch,), device=device) % (1 - eps)\n",
        "    t = logit_normal.rsample((batch,)).to(device) # in [0,1] [batch,1]\n",
        "    t_ = t[...,None,None,None]\n",
        "    x0 = torch.randn_like(x1)\n",
        "    psi_t = (1 - (1-sig_min)*t_)*x0 + t_*x1 # ψt(x) = (1 − (1 − σmin)t)x + tx1, (22)\n",
        "    v_psi = model(psi_t, t, cond) # vt(ψt(x0))\n",
        "    d_psi = x1 - (1 - sig_min) * x0 #\n",
        "    return F.mse_loss(v_psi, d_psi) # LCFM(θ)\n",
        "\n",
        "\n",
        "def ae_train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "    # for i, img in enumerate(dataloader):\n",
        "    #     img = img.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "            # loss = otfm_loss(model, x1, cond) # unet\n",
        "            # img = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # loss = model.ae_loss(img)\n",
        "\n",
        "            x1 = model.ae.encode(img)\n",
        "            img_ = model.ae.decode(x1)\n",
        "            ae_loss = F.mse_loss(img_, img)\n",
        "            loss = ae_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i,loss.item())\n",
        "            # print(i,ae_loss.item(), fm_loss.item())\n",
        "        if i % 200 == 0:\n",
        "            imshow(torchvision.utils.make_grid(img[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "            imshow(torchvision.utils.make_grid(x1[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "            imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "\n",
        "        try: wandb.log({\"ae_loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "\n",
        "def train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "    # for i, img in enumerate(dataloader):\n",
        "    #     img = img.to(device)\n",
        "        # # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "            img = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            # x1 = model.ae.encode(img)\n",
        "            # img_ = model.ae.decode(x1)\n",
        "            # loss = F.mse_loss(img_, img)\n",
        "\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # with torch.no_grad():\n",
        "            #     x1 = model.ae.encode(img)\n",
        "            #     img_ = model.ae.decode(x1)\n",
        "            # fm_loss = otfm_loss(model.unet, x1.detach(), cond)\n",
        "            loss = otfm_loss(model, img, cond)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i,loss.item())\n",
        "            # print(i,ae_loss.item(), fm_loss.item())\n",
        "        if i % 200 == 0:\n",
        "\n",
        "            cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "            img_ = reverse_flow(model, cond, timesteps=10) # unet\n",
        "            # img_ = model.sample(cond, timesteps=10) # ldm\n",
        "            # x1_ = reverse_flow(model.unet, cond, timesteps=10)\n",
        "            # print(x1.dtype)\n",
        "            # imshow(torchvision.utils.make_grid(x1_[:4].cpu(), nrow=4))\n",
        "\n",
        "            # img_ = model.ae.decode(x1_.float())\n",
        "            # with torch.no_grad(): img_ = model.ae.decode(x1_)\n",
        "            imshow(torchvision.utils.make_grid(img_[:4].cpu(), nrow=4))\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "for epoch in range(1):\n",
        "# for epoch in range(40):\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    # for x,y in test_loader: break\n",
        "    # img = F.interpolate(x.to(device), size=(64,64)).repeat(1,3,1,1)\n",
        "    # x1 = model.ae.encode(img)\n",
        "    # img_ = model.ae.decode(x1)\n",
        "    # imshow(torchvision.utils.make_grid(img[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "    # imshow(torchvision.utils.make_grid(x1[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "    # imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # # # cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).expand(16,-1).to(torch.float)\n",
        "        cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "        img_ = reverse_flow(model, cond, timesteps=10) # unet\n",
        "        # img_ = model.sample(cond, timesteps=10) # ldm\n",
        "        imshow(torchvision.utils.make_grid(img_.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ecSX2oJJ4_2F"
      },
      "outputs": [],
      "source": [
        "# @title test func\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "        # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # loss = ae_loss + fm_loss\n",
        "            x1 = F.interpolate(x1, size=(16,16))#.repeat(1,3,1,1)\n",
        "            loss = otfm_loss(model, x1, cond) # unet\n",
        "        total_loss+=loss\n",
        "    print(total_loss/len(dataloader))\n",
        "    # if i % 10 == 0: print(i,ae_loss.item(),fm_loss.item())\n",
        "    try: wandb.log({\"test_loss\": loss.item()})\n",
        "    # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "    except: pass\n",
        "\n",
        "\n",
        "# test(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"lfm\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGN0e0Mxe5UI"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'lfm.pkl', map_location=device).values()\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrFxTtw4eFSq"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'lfm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVSdLd7um-Rs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# High Fidelity Visualization of What Your Self-Supervised Representation Knows About aug 2022\n",
        "# https://arxiv.org/pdf/2112.09164\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFhmfVnrOiQd"
      },
      "outputs": [],
      "source": [
        "# # dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# # print(x)\n",
        "# print(x.shape)\n",
        "# x=x.unsqueeze(0)\n",
        "# # x1 = F.interpolate(x, size=(16,16))#.repeat(1,3,1,1)\n",
        "# x1 = F.interpolate(x, size=(64,64)).repeat(1,3,1,1)\n",
        "# imshow(torchvision.utils.make_grid(x1.cpu(), nrow=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQlJ4XVLrOSx"
      },
      "source": [
        "## save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G5l1b6dUu6MJ"
      },
      "outputs": [],
      "source": [
        "# @title U-DiT\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = self.layer(x) + x # down_shortcut\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "\n",
        "# conv res pixeldown\n",
        "\n",
        "class DownSample_Attn(nn.Module):\n",
        "    def __init__(self, dim, num_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = num_heads\n",
        "        self.d_head = dim//num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.r = r\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(head_dim, seq_len=512, base=10000)\n",
        "        self.scale = head_dim**-.5 # v1\n",
        "        # v2\n",
        "        # self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
        "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1))), requires_grad=True)\n",
        "        # self.logit_scale = nn.Parameter(10 * torch.ones((num_heads, 1, 1)))\n",
        "\n",
        "        self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # b, _, h, w = x.size()\n",
        "        x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # attn = (q @ k.transpose(-2, -1)) * self.scale # v1 attention\n",
        "        # attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)) * torch.clamp(self.logit_scale, max=4.6052).exp() # v2 attention\n",
        "        # # attn = attn * torch.clamp(self.logit_scale, max=100)\n",
        "        # x = attn.softmax(dim=-1) @ v\n",
        "\n",
        "        x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, dim, ff_mult=1):\n",
        "#         super().__init__()\n",
        "#         d_model = dim*ff_mult\n",
        "#         self.project_in = nn.Sequential(nn.Conv2d(dim, d_model, kernel_size=1), nn.GELU())\n",
        "#         self.dwconv = nn.ModuleList([\n",
        "#             # nn.Conv2d(d_model, d_model, 5, 1, 5//2, groups=d_model),\n",
        "#             nn.Conv2d(d_model, d_model, 3, 1, 3//2, groups=d_model),\n",
        "#             nn.Conv2d(d_model, d_model, 1, 1, 1//2, groups=d_model),\n",
        "#         ])\n",
        "#         self.project_out = nn.Sequential(nn.Conv2d(d_model, d_model//2, kernel_size=1),\n",
        "#             nn.Conv2d(d_model//2, dim, kernel_size=1))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.project_in(x) # no\n",
        "#         x = x + sum([conv(x) for conv in self.dwconv])\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class SEBlock(nn.Module): # https://github.com/DingXiaoH/RepVGG/blob/main/se_block.py#L7\n",
        "#     def __init__(self, in_ch, dim=None):\n",
        "#         super().__init__()\n",
        "#         dim = dim or in_ch//4\n",
        "#         self.se = nn.Sequential( # [b,c,h,w] -> [b,c,1,1]\n",
        "#             nn.AdaptiveAvgPool2d((1,1)),\n",
        "#             nn.Conv2d(in_ch, dim, 1, 1), nn.ReLU(),\n",
        "#             nn.Conv2d(dim, in_ch, 1, 1), nn.Sigmoid(),\n",
        "#         )\n",
        "#     def forward(self, x): # [b,c,h,w]\n",
        "#         return x * self.se(x) # [b,c,h,w]\n",
        "\n",
        "# # # https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py\n",
        "# # # https://github.com/DingXiaoH/RepVGG/blob/main/repvggplus.py#L28\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, in_ch, out_ch=None, ff_mult=1):\n",
        "#         super().__init__()\n",
        "#         # d_model = dim*ff_mult\n",
        "#         out_ch = out_ch or in_ch\n",
        "#         self.dwconv = nn.ModuleList([\n",
        "#             nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, 1, 3//2), nn.BatchNorm2d(out_ch)),\n",
        "#             nn.Sequential(nn.Conv2d(in_ch, out_ch, 1, 1, 1//2), nn.BatchNorm2d(out_ch)),\n",
        "#         ])\n",
        "#         self.project_out = nn.Sequential(nn.GELU(), SEBlock(out_ch, out_ch//4)) # act, SEblock\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + sum([conv(x) for conv in self.dwconv])\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "# # me\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_mult=1):\n",
        "        super().__init__()\n",
        "        # d_model = dim*ff_mult\n",
        "        self.project_in = nn.Sequential(nn.BatchNorm2d(d_model), nn.GELU())\n",
        "        self.dwconv = nn.ModuleList([\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 3//2),\n",
        "            nn.Conv2d(d_model, d_model, 1, 1, 1//2),\n",
        "        ])\n",
        "        # self.project_out = nn.Sequential(nn.GELU(), nn.Conv2d(d_model, dim, kernel_size=1)) # act, SEblock\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.project_in(x) # no\n",
        "        x = x + sum([conv(h) for conv in self.dwconv])\n",
        "        # x = self.project_out(out)\n",
        "        return x\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, hidden_size, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.rot_emb = RotEmb(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden_size), nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        # nn.init.normal_(self.mlp.weight, std=0.02)\n",
        "        self.mlp.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "    def forward(self, t):\n",
        "        t_freq = self.rot_emb(t)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    \"\"\"A IPT block with adaptive layer norm zero (adaLN-Zero) conIPTioning.\"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm1 = LayerNorm2d(hidden_size)\n",
        "        self.attn = DownSample_Attn(hidden_size, num_heads=num_heads, r=down_factor)\n",
        "        # self.attn = AttentionBlock(d_model=hidden_size, d_head=hidden_size//num_heads)\n",
        "        self.norm2 = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm2 = LayerNorm2d(hidden_size)\n",
        "        self.mlp = FeedForward(hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(hidden_size, 6 * hidden_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond).chunk(6, dim=1)\n",
        "        # x = x + gate_msa[...,None,None] * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        # x = x + gate_mlp[...,None,None] * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "\n",
        "        return x\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale[...,None,None]) + shift[...,None,None]\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(hidden_size, 2*hidden_size))\n",
        "        )\n",
        "        self.norm_final = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm_final = LayerNorm2d(hidden_size)\n",
        "        self.out_proj = zero_module(nn.Conv2d(hidden_size, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = self.in_proj(x)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class UpDownsample(nn.Module):\n",
        "    def __init__(self, n_feat, r=1):\n",
        "        super().__init__()\n",
        "        if r>1: sample = nn.PixelShuffle(r)\n",
        "        elif r<1: sample = nn.PixelUnshuffle(int(1/r))\n",
        "        else: sample = nn.Identity()\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, int(n_feat*r), 3, 1, 1, bias=False), sample)\n",
        "    def forward(self, x): return self.body(x)\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, depth=1, updown=False, red=False):\n",
        "        super().__init__()\n",
        "\n",
        "        if updown=='down': level = [\n",
        "            UpDownsample(hidden_size, r=1/2),\n",
        "            *[U_DiTBlock(hidden_size*2, num_heads) for _ in range(depth)]\n",
        "        ]\n",
        "        elif updown=='up': level = [\n",
        "            # nn.Conv2d(hidden_size, hidden_size//2, kernel_size=1),\n",
        "            # *[U_DiTBlock(hidden_size//2, num_heads) for _ in range(depth)],\n",
        "            # UpDownsample(hidden_size//2, r=2)\n",
        "            nn.Conv2d(hidden_size*2, hidden_size, kernel_size=1),\n",
        "            *[U_DiTBlock(hidden_size, num_heads) for _ in range(depth)],\n",
        "            UpDownsample(hidden_size, r=2)\n",
        "            ]\n",
        "        else: level = [\n",
        "            nn.Conv2d(hidden_size*2, hidden_size, kernel_size=1) if red else nn.Identity(),\n",
        "            *[U_DiTBlock(hidden_size, num_heads) for _ in range(depth)]\n",
        "        ]\n",
        "        self.seq = Seq(*level)\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        return self.seq(x, cond)\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_size=96, depth=[2,5,8,5,2], num_heads=16, cond_dim=16):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels #* 2 if learn_sigma else in_channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = nn.Conv2d(in_channels, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.bias, 0)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size*(1+2+4))\n",
        "        self.y_embedder = nn.Linear(cond_dim, hidden_size*(1+2+4))\n",
        "\n",
        "        self.down_list = nn.ModuleList([\n",
        "            levelBlock(hidden_size, num_heads, depth=1, updown=False),\n",
        "            levelBlock(hidden_size, num_heads, depth=1, updown='down'),\n",
        "        ])\n",
        "        self.middle_block = Seq(\n",
        "            UpDownsample(hidden_size*2, r=1/2),\n",
        "            # levelBlock(hidden_size*4, num_heads, depth=1, updown=False),\n",
        "            *[U_DiTBlock(hidden_size*4, num_heads) for _ in range(depth[2])],\n",
        "            UpDownsample(hidden_size*4, r=2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([\n",
        "            levelBlock(hidden_size*2, num_heads, depth=1, updown='up'),\n",
        "            levelBlock(hidden_size*1, num_heads, depth=1, updown=False, red=True),\n",
        "        ])\n",
        "\n",
        "        self.final_layer = FinalLayer(hidden_size, self.out_channels)\n",
        "\n",
        "        self.apply(self._basic_init)\n",
        "    def _basic_init(self, module):\n",
        "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        x = self.x_embedder(x) # (N, C, H, W)\n",
        "        c123 = self.t_embedder(t) + self.y_embedder(y)\n",
        "        cond = c123.split([self.hidden_size, self.hidden_size*2, self.hidden_size*4], dim=1)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, cond[i])\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, cond[-1])\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond[-2-i]) # x = up(x, blocks[-i - 1])\n",
        "\n",
        "        x = self.final_layer(x, cond[0]) # (N, T, patch_size ** 2 * out_channels)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs):\n",
        "#     return U_DiT(down_factor=2, hidden_size=96, num_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(hidden_size=192, num_heads=8,\n",
        "# def U_DiT_L: U_DiT(hidden_size=384, num_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_channels=3, hidden_size=16, num_heads=4, depth=[2,3,3,3,2], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "# model(inputs, t, y)\n",
        "# out = model(inputs, t, y)\n",
        "# print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SlxB9mFM6abU"
      },
      "outputs": [],
      "source": [
        "# @title unet me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim, cond_dim, n_head=None, d_head=8, updown=False, r=2):\n",
        "        super().__init__()\n",
        "        if updown=='down': in_ch = in_ch*r**2\n",
        "        elif updown=='up': out_ch = out_ch*r**2\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "\n",
        "        tok_dim = d_model\n",
        "        # if tok_dim == None: tok_dim = d_model\n",
        "        self.num_time_tokens = 2\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            RotEmb(emb_dim, top=torch.pi, base=10000),\n",
        "            nn.Linear(emb_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + self.num_time_tokens * tok_dim),\n",
        "        )\n",
        "        self.emb_dim, self.tok_dim = emb_dim, tok_dim\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(cond_dim), nn.Linear(cond_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + tok_dim)\n",
        "        )\n",
        "        self.norm_cond = nn.LayerNorm(tok_dim)\n",
        "        cond_dim = tok_dim\n",
        "\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1))\n",
        "        # self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1), act)\n",
        "        # self.init_conv = CrossEmbedLayer(in_ch, dim_out=d_model, kernel_sizes=(3, 7, 15), stride=1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7//2)\n",
        "\n",
        "        dim_mults = [1,2,3,4,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in dim_mults] # [128, 256, 384, 512]\n",
        "        # in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        # for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = ch_list[-1]*2**2 # 512\n",
        "        self.middle_block = Seq(\n",
        "            nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "        # for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        # t_emb = self.rotemb(t)\n",
        "        # emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        t_hid, t_tok = self.time_mlp(t).split([self.emb_dim, self.num_time_tokens * self.tok_dim], dim=-1)\n",
        "        t_tok = t_tok.reshape(t_tok.shape[0], self.num_time_tokens, self.tok_dim)\n",
        "        c_hid, c_tok = self.cond_mlp(cond).split([self.emb_dim, self.tok_dim], dim=-1)\n",
        "        # print('unet fwd', t_hid.shape, t_tok.shape, t.shape)\n",
        "\n",
        "        emb = t_hid + c_hid # [batch, emb_dim]\n",
        "        cond = torch.cat((t_tok, c_tok.unsqueeze(1)), dim=-2) # [b, num_toks, tok_dim]\n",
        "        cond = self.norm_cond(cond)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# x=torch.rand((batch,3,16,16),device=device)\n",
        "x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# print(t)\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "# [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "print(x.shape,t.shape,cond.shape)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EkVBxwyjTpM-"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_list=[32, 64], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            PixelShuffleConv(in_ch, d_list[0], k_list[0], r=1/2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.Dropout2d(drop), PixelShuffleConv(d_list[0], d_list[1], k_list[1], r=1/2), act,\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x) # [batch, 3,64,64] -> [batch, c,h,w]\n",
        "\n",
        "class Deconv(nn.Module):\n",
        "    def __init__(self, out_ch=3, d_list=[32, 64], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            PixelShuffleConv(d_list[1], d_list[0], k_list[1], r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            PixelShuffleConv(d_list[0], out_ch, k_list[0], r=2),\n",
        "        )\n",
        "    def forward(self, x): return self.deconv(x) # [batch, c,h,w] -> [batch, 3,64,64]\n",
        "\n",
        "\n",
        "class PixelAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=256, out_ch=None, kernels=[7,5], mult=[1]):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.d_model, self.out_ch = in_ch, d_model, out_ch\n",
        "        d_list=[d_model*m for m in mult]\n",
        "        in_list, out_list = [in_ch, *d_list[:-1]], [*d_list[:-1], out_ch]\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=1/2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act,) for i, (in_dim, out_dim, kernel) in enumerate(zip(in_list, out_list, kernels))], # conv,norm,act except for last layer: no norm\n",
        "            # PixelShuffleConvDown(in_ch, d_list[0], 7, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConvDown(d_list[0], d_list[1], 5, r=2), act,\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), *(nn.BatchNorm2d(out_dim), act) if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # PixelShuffleConvUp(d_list[1], d_list[0], 5, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConvUp(d_list[0], in_ch, 7, r=2), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x): return self.encoder(x)\n",
        "    def decode(self, x): return self.decoder(x)\n",
        "\n",
        "\n",
        "# in_ch=3\n",
        "# model_ch=16\n",
        "# d_list=[16, 16] # [16,32]\n",
        "# k_list=[7,5] # [7,5]\n",
        "# conv = Conv(in_ch=in_ch, d_list=d_list, k_list=k_list, act=nn.ReLU()) # ReLU GELU SiLU\n",
        "\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "# input = torch.rand((4,3,64,64), device=device)\n",
        "# enc = conv(input)\n",
        "# print(enc.shape)\n",
        "# out = deconv(enc)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GMFVOgT4sJ-y"
      },
      "outputs": [],
      "source": [
        "# @title stable diffusion unet next\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1) # og\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding=1), act)\n",
        "\n",
        "    def forward(self, x): # [N,C,...]\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\") # if self.conv_dim == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        x = self.conv(x) # optional\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.op = nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1) # optional # stride = 2 if conv_dim != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        self.op = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1), act)\n",
        "        # self.op = avg_pool_nd(conv_dim, kernel_size=stride, stride=stride) # alternative\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial]\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, temb_dim, out_ch=None, scale_shift=False, updown=False, drop=0.):\n",
        "        super().__init__()\n",
        "        self.temb_dim = temb_dim # number of timestep embedding channels\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch = in_ch, out_ch\n",
        "        self.scale_shift = scale_shift\n",
        "        if updown=='up': self.h_upd, self.x_upd = Upsample(in_ch), Upsample(in_ch)\n",
        "        elif updown=='down': self.h_upd, self.x_upd = Downsample(in_ch), Downsample(in_ch)\n",
        "        else: self.h_upd = self.x_upd = nn.Identity()\n",
        "        self.in_layers = nn.Sequential(nn.BatchNorm2d(in_ch), nn.SiLU(), self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1),) # zero\n",
        "        # self.in_layers = nn.Sequential(self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1)) # no bn before FiLM\n",
        "        # self.in_layers = nn.Sequential(self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.SiLU(), self.h_upd, nn.Conv2d(out_ch, out_ch, 3, padding=1)) # no bn before FiLM\n",
        "\n",
        "        # Conv bn film act res\n",
        "        self.emb_layers = nn.Sequential(nn.SiLU(), nn.Linear(temb_dim, 2 * out_ch if scale_shift else out_ch),)\n",
        "        # self.emb_layers = nn.Sequential(nn.Linear(temb_dim, out_ch), nn.SiLU(), nn.Linear(out_ch, 2 * out_ch if scale_shift else out_ch),)\n",
        "        self.out_layers = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_ch), nn.SiLU(), nn.Dropout(drop), nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            # nn.BatchNorm2d(out_ch), nn.SiLU(), nn.Dropout(drop), zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if out_ch == in_ch: self.skip = nn.Identity() # no need to change chanels\n",
        "        else:\n",
        "            # self.skip = nn.Conv2d(in_ch, out_ch, 3, padding=1) # spatial convolution to change the channels in the skip connection\n",
        "            self.skip = nn.Conv2d(in_ch, out_ch, 1) # smaller 1x1 convolution to change the channels in the skip connection\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, temb_dim]\n",
        "        # print(\"res fwd x\", x.shape, self.in_ch, self.out_ch)\n",
        "        h = self.in_layers(x) # norm, act, h_upd, conv\n",
        "        x = self.x_upd(x)\n",
        "        emb_out = self.emb_layers(emb) # act, lin\n",
        "        # print(\"res fwd h emb_out\", h.shape, emb_out.shape)\n",
        "        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]\n",
        "        if self.scale_shift: # FiLM\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return h + self.skip(x) # [N, C, ...]\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, updown=False, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            # Downsample(in_ch, out_ch) if updown=='down' else nn.Identity(),\n",
        "            Downsample(in_ch) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch),\n",
        "            # SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            Upsample(out_ch) if updown=='up' else nn.Identity(),\n",
        "            # Upsample(in_ch, out_ch) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, model_ch=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.model_ch = model_ch # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = model_ch // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(model_ch)\n",
        "        temb_dim = model_ch# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(model_ch, temb_dim), nn.SiLU(), nn.Linear(temb_dim, temb_dim))\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1))\n",
        "        # self.in_block = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1), act)\n",
        "\n",
        "        ch_list = [model_ch*2**i for i in range(depth+1)] # [32, 64, 128, 256]\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = 2*ch_list[-1] # 512\n",
        "        self.middle_block = Seq(\n",
        "            Downsample(ch_list[-1]),\n",
        "            ResBlock(ch_list[-1], temb_dim, ch),\n",
        "            # SpatialTransformer(ch, ch//d_head, d_head, cond_dim=cond_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, temb_dim, ch_list[-1]),\n",
        "            Upsample(ch_list[-1]),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(model_ch), nn.SiLU(), nn.Conv2d(model_ch, out_ch, 3, padding=1)) # zero\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        # t_emb = timestep_embedding(t, self.model_ch, repeat_only=False)\n",
        "        emb = self.time_emb(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, temb_dim)\n",
        "        blocks = []\n",
        "        x = self.in_block(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, model_ch=16, cond_dim=cond_dim, depth=4).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# # x=torch.rand((batch,3,16,16),device=device)\n",
        "# x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# # print(t)\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# # [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "# print(x.shape,t.shape,cond.shape)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# optim = torch.optim.SGD(model.parameters(), lr=1e-4) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F5znfg9g-BOQ"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChanRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.gamma = nn.Parameter(torch.ones(dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = 1) * self.scale * self.gamma\n",
        "\n",
        "\n",
        "class Parallel(nn.Module):\n",
        "    def __init__(self, *fns):\n",
        "        super().__init__()\n",
        "        self.fns = nn.ModuleList(fns)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [fn(x) for fn in self.fns]\n",
        "        return sum(outputs)\n",
        "\n",
        "class GlobalContext(nn.Module): # Global Context (GC) block\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b, c, h, w]\n",
        "        context = self.to_k(x)\n",
        "        x, context = x.flatten(2), context.flatten(2) # [b, c, h*w] ,[b,1,h*w]\n",
        "        out = x @ context.softmax(dim=-1).transpose(1,2) # [b, c, 1]\n",
        "        out = out.unsqueeze(-1) # [b, c, 1, 1]\n",
        "        return self.net(out) # [b, dim_out, 1, 1]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, d_head = 64,n_heads = 8, context_dim = None, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_head\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, 1, d_head))\n",
        "        self.qkv = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, d_model + 2*d_head, bias=False))\n",
        "        self.q_scale, self.k_scale = nn.Parameter(torch.ones(d_head)), nn.Parameter(torch.ones(d_head))\n",
        "\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, 2*d_head)) if context_dim!=None else None\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_model, dim, bias = False), nn.LayerNorm(dim),)\n",
        "\n",
        "    def forward(self, x, cond = None, mask = None, attn_bias = None): # [batch, T, d_model] = [b, hw, c], [batch, num_tok, context_dim]\n",
        "        batch, T, _ = x.shape#[0]\n",
        "        q, k, v = self.qkv(x).split([self.d_model, self.d_head, self.d_head], dim=-1) # [batch, T, d_model], 2*[batch, T, d_head]\n",
        "        q = q.reshape(batch, T, self.n_heads, -1).transpose(1,2) # [batch, n_heads, T, d_head]\n",
        "\n",
        "        nk, nv = self.null_kv.expand((batch,-1,-1,-1)).unbind(dim=1) # [2,1,d_head]->[batch,2,1,d_head]->[batch,1,d_head]\n",
        "        k, v = torch.cat((nk, k), dim=-2), torch.cat((nv, v), dim=-2) # [batch, 1+T, d_head]\n",
        "\n",
        "        if self.to_context!=None:\n",
        "            ck, cv = self.to_context(cond).chunk(2, dim = -1) # [batch, num_tok, d_head]\n",
        "            k, v = torch.cat((ck, k), dim=-2), torch.cat((cv, v), dim=-2) # [batch, num_tok+1+T, d_head]\n",
        "\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1) # qk rmsnorm\n",
        "        q, k = q * self.q_scale, k * self.k_scale\n",
        "        k, v = k.unsqueeze(1), v.unsqueeze(1) # [batch, 1, num_tok+1+T, d_head]\n",
        "\n",
        "        sim = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T, num_tok+1+T]\n",
        "        if attn_bias!=None: sim = sim + attn_bias # relative positional encoding (T5 style)\n",
        "        if mask!=None:\n",
        "            mask = F.pad(mask, (1, 0), value=True).unsqueeze(1).unsqeeze(2) # [b j] -> [b 1 1 j]\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "        attn = sim.softmax(dim=-1) # attn = sim.softmax(dim=-1, dtype=torch.float32).to(sim.dtype)\n",
        "        out = (attn @ v).transpose(1,2).flatten(2) # [batch, n_heads, T, d_head] -> [batch, T, d_model]\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, context_dim = None, d_head = 64,n_heads = 8, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.n_heads =n_heads\n",
        "        d_model = d_head *n_heads\n",
        "        if context_dim==None: context_dim = dim\n",
        "        self.to_q = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, d_model, bias=False))\n",
        "        self.to_kv = nn.Linear(context_dim, d_model * 2, bias=False)\n",
        "        # self.to_kv = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, d_model * 2, bias=False))\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, 1, 1, d_head))\n",
        "        self.q_scale, self.k_scale = nn.Parameter(torch.ones(d_head)), nn.Parameter(torch.ones(d_head))\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_model, dim, bias = False), nn.LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x, cond, mask = None): # [batch, T, dim]=[b, hw, c], [batch, num_tok, context_dim]\n",
        "        batch, T, _ = x.shape#[0]\n",
        "        _, num_tok, _ = cond.shape\n",
        "        q = self.to_q(x).reshape(batch, T, self.n_heads, -1).transpose(1,2) # [batch, n_heads, T, d_head]\n",
        "        k, v = self.to_kv(cond).chunk(2, dim = -1) # [batch, num_tok, d_model]\n",
        "        k = k.reshape(batch, num_tok, self.n_heads, -1).transpose(1,2) # [batch, n_heads, num_tok, d_head]\n",
        "        v = v.reshape(batch, num_tok, self.n_heads, -1).transpose(1,2) # [batch, n_heads, num_tok, d_head]\n",
        "\n",
        "        nk, nv = self.null_kv.expand((batch,-1,self.n_heads,-1,-1)).unbind(dim=1) # [2,1,1,d_head]->[batch,2,n_heads,1,d_head]->[batch,n_heads,1,d_head]\n",
        "        k, v = torch.cat((nk, k), dim=-2), torch.cat((nv, v), dim=-2) # [batch, n_heads, 1+num_tok, d_head]\n",
        "\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1) # qk rmsnorm\n",
        "        q, k = q * self.q_scale, k * self.k_scale\n",
        "\n",
        "        sim = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T, 1+num_tok]\n",
        "        if mask!=None:\n",
        "            mask = F.pad(mask, (1, 0), value=True).unsqueeze(1).unsqeeze(2) # [b j] -> [b 1 1 j]\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "        attn = sim.softmax(dim=-1) # attn = sim.softmax(dim=-1, dtype=torch.float32).to(sim.dtype)\n",
        "        out = (attn @ v).transpose(1,2).flatten(2) # [batch, n_heads, T, d_head] -> [batch, T, d_model]\n",
        "        return self.to_out(out)\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(self, dim_in, kernel_sizes, dim_out = None, stride = 2):\n",
        "        super().__init__()\n",
        "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
        "        if dim_out==None: dim_out = dim_in\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, len(kernel_sizes))] # [64, 32]\n",
        "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)] # [64, 32, 32]\n",
        "        # 1/2 + 1/4 + 1/8 + ... + 1/2^num_kernels + 1/2^num_kernels of dim_out; smaller kernel allocated more channels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(dim_in, dim_scale, kernel, stride=stride, padding=(kernel-stride)//2) for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "\n",
        "\n",
        "def Upsample(in_ch, out_ch=None):\n",
        "    if out_ch==None: out_ch = in_ch\n",
        "    return nn.Sequential(nn.Interpolate(scale_factor = 2, mode = 'nearest'), nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "\n",
        "class PixelShuffleUpsample(nn.Module):\n",
        "    \"\"\"code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\"\"\"\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * 4, 1), nn.SiLU(), nn.PixelShuffle(2)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        self.init_conv_(self.net[0])\n",
        "\n",
        "    def init_conv_(self, conv):\n",
        "        o, i, h, w = conv.weight.shape\n",
        "        conv_weight = torch.empty(o//4, i, h, w)\n",
        "        nn.init.kaiming_uniform_(conv_weight)\n",
        "        conv_weight = conv_weight.repeat(4,1,1,1)\n",
        "        conv.weight.data.copy_(conv_weight)\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def Downsample(in_ch, out_ch=None): # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample, named SP-conv in the paper, but basically a pixel unshuffle\n",
        "    if out_ch==None: out_ch = in_ch\n",
        "    return nn.Sequential(nn.PixelUnshuffle(2), nn.Conv2d(in_ch * 4, out_ch, 1)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "\n",
        "\n",
        "def FeedForward(dim, mult = 2):\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim), nn.Linear(dim, hidden_dim, bias = False), nn.GELU(),\n",
        "        nn.LayerNorm(hidden_dim), nn.Linear(hidden_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, depth = 1,n_heads = 8, d_head = 32, ff_mult = 2, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim,n_heads =n_heads, d_head = d_head, context_dim = context_dim),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, cond = None):\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2) # [b, c, h, w] -> [b, h*w, c]\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, cond) + x\n",
        "            x = ff(x) + x\n",
        "        x = x.transpose(1, 2).reshape(*bchw) # [b, h*w, c] -> [b, c, h, w]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, tok_dim = None, emb_dim = None,n_heads=None, d_head=None):\n",
        "        super().__init__()\n",
        "        if tok_dim != None: self.cross_attn = CrossAttention(dim = dim_out, context_dim = tok_dim, n_heads=n_heads, d_head=d_head) # CrossAttention LinearCrossAttention\n",
        "        else: self.cross_attn = None\n",
        "        self.block1 = nn.Sequential(ChanRMSNorm(dim), nn.SiLU(), nn.Conv2d(dim, dim_out, 3, padding = 1))\n",
        "        self.block2 = Seq(ChanRMSNorm(dim_out), scale_shift(dim_out, emb_dim) if emb_dim != None else nn.Identity(), nn.SiLU(), nn.Conv2d(dim_out, dim_out, 3, padding = 1))\n",
        "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb = None, cond = None):\n",
        "        h = self.block1(x)\n",
        "        # print('ResnetBlock fwd', h.shape)\n",
        "        if self.cross_attn != None:\n",
        "            bhwc = h.shape\n",
        "            h = h.flatten(2).transpose(1, 2) # [b, h, w, c] -> [b, h*w, c]\n",
        "            h = self.cross_attn(h, cond=cond) + h\n",
        "            h = h.transpose(1, 2).reshape(*bhwc) # [b, h*w, c] -> [b, c, h, w]\n",
        "        h = self.block2(h, emb)\n",
        "        h = h * self.gca(h) # use_gca\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb = None): # [b,c,h,w], [b,emb_dim]\n",
        "        # print('scale_shift fwd', x.shape, emb.shape)\n",
        "        emb = self.time_mlp(emb)[..., None, None] # [b,t_dim] -> [b,2*x_dim,1,1]\n",
        "        scale, shift = emb.chunk(2, dim = 1) # [b,x_dim,1,1]\n",
        "        x = x * (scale + 1) + shift\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KpM1jlvFvD9i"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen next\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, d_model, # 512 # base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
        "        c_dim = 16, # cond vec dim\n",
        "        tok_dim = None, # token dim\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8), # (1, 2, 3, 4)\n",
        "        in_ch = 3, out_ch = None,\n",
        "        layer_attns = True, # (False, True, True, True)\n",
        "        layer_cross_attns = True, # (False, True, True, True)\n",
        "        # num_resnet_blocks = 1, # 3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        if out_ch == None: self.out_ch = in_ch\n",
        "        d_head = 64\n",
        "        n_heads = 8 # 8 # ideally at least 4 or 8\n",
        "\n",
        "        self.init_conv = CrossEmbedLayer(in_ch, dim_out = d_model, kernel_sizes = (3, 7, 15), stride = 1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7 // 2)\n",
        "        dims = [d_model] + [d_model * m for m in dim_mults] # [128, 128, 256, 384, 512]\n",
        "        # dims = [d_model * m for m in dim_mults] # [128, 256, 384, 512]\n",
        "\n",
        "        # time conditioning\n",
        "        if tok_dim == None: tok_dim = d_model\n",
        "\n",
        "        # embedding time for log(snr) noise from continuous version\n",
        "        emb_dim = 16\n",
        "        emb_dim = d_model * 4 #* (2 if lowres_cond else 1)\n",
        "\n",
        "        # pos_emb = RotEmb(emb_dim, top=torch.pi, base=10000)\n",
        "        # self.time_hiddens = nn.Sequential(pos_emb, nn.Linear(emb_dim, emb_dim), nn.SiLU())\n",
        "\n",
        "        # self.time_cond = nn.Sequential(nn.Linear(emb_dim, emb_dim))\n",
        "        # num_time_tokens = 2\n",
        "        # self.time_tokens = nn.Sequential(nn.Linear(emb_dim, tok_dim * num_time_tokens), Rearrange('b (r d) -> b r d', r = num_time_tokens))\n",
        "        # # self.time_tokens = nn.Sequential(nn.Linear(emb_dim, tok_dim))\n",
        "\n",
        "        self.num_time_tokens = 2\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            RotEmb(emb_dim, top=torch.pi, base=10000),\n",
        "            nn.Linear(emb_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + self.num_time_tokens * tok_dim),\n",
        "        )\n",
        "        self.emb_dim, self.tok_dim = emb_dim, tok_dim\n",
        "\n",
        "\n",
        "        # self.cond_tokens = nn.Linear(c_dim, tok_dim)\n",
        "        # self.cond_cond = nn.Sequential(\n",
        "        #     nn.LayerNorm(tok_dim), nn.Linear(tok_dim, emb_dim), nn.SiLU(),\n",
        "        #     nn.Linear(emb_dim, emb_dim)\n",
        "        # )\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.Linear(c_dim, tok_dim),\n",
        "            nn.LayerNorm(tok_dim), nn.Linear(tok_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + tok_dim)\n",
        "        )\n",
        "\n",
        "        self.norm_cond = nn.LayerNorm(tok_dim)\n",
        "\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        num_layers = len(in_out)\n",
        "        # num_layers = len(dims)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        # skip_connect_dims = [] # keep track of skip connection dimensions\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_layers - 1)\n",
        "            # print('down', dim_in, dim_out)\n",
        "\n",
        "            # skip_connect_dims.append(dim_in) # dim_out if memory_efficient\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                # Downsample(dim_in, dim_out), # memory_efficient pre_downsample; self.downs all dim_out # Downsample cross_embed_downsample CrossEmbedLayer(dim_in, dim_out, kernel_sizes = (2, 4))\n",
        "                ResnetBlock(dim_in, dim_in, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "                nn.ModuleList([ResnetBlock(dim_in, dim_in, emb_dim = emb_dim) for _ in range(0)]),\n",
        "                TransformerBlock(dim = dim_in, depth = 1, ff_mult = 2, context_dim = tok_dim, n_heads=n_heads, d_head=d_head), # trans/ lintrans/ id\n",
        "                Downsample(dim_in, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1)) # Downsample cross_embed_downsample CrossEmbedLayer(dim_in, dim_out, kernel_sizes = (2, 4))\n",
        "            ]))\n",
        "\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block = Seq(\n",
        "            ResnetBlock(mid_dim, mid_dim, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "            TransformerBlock(mid_dim, depth = 1, n_heads=n_heads, d_head=d_head), # True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "            ResnetBlock(mid_dim, mid_dim, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "        )\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (num_layers - 1)\n",
        "            # skip_connect_dim = skip_connect_dims.pop()\n",
        "            # print('up', dim_in, dim_out, skip_connect_dim)\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                ResnetBlock(dim_out + dim_in, dim_out, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "                nn.ModuleList([ResnetBlock(dim_out + dim_in, dim_out, emb_dim = emb_dim) for _ in range(0)]),\n",
        "                TransformerBlock(dim = dim_out, depth = 1, ff_mult = 2, context_dim = tok_dim, n_heads=n_heads, d_head=d_head), # trans/ lintrans/ id\n",
        "                PixelShuffleUpsample(dim_out, dim_in) if not is_last else nn.Identity() # PixelShuffleUpsample Upsample ; memory_efficient upscale at last too\n",
        "            ]))\n",
        "\n",
        "        self.final_res_block = ResnetBlock(d_model, d_model, emb_dim = emb_dim) #if final_resnet_block else None\n",
        "        self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3 // 2)\n",
        "        def zero_init_(m):\n",
        "            nn.init.zeros_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "        zero_init_(self.final_conv)\n",
        "\n",
        "    def forward(self, x, time, cond):\n",
        "        # cond_images = resize_image_to(cond_images, x.shape[-1], mode = 'nearest')\n",
        "        # x = torch.cat((cond_images, x), dim = 1)\n",
        "        x = self.init_conv(x)\n",
        "        # hiddens.append(x)\n",
        "\n",
        "        t, t_tok = self.time_mlp(time).split([self.emb_dim, self.num_time_tokens * self.tok_dim], dim=-1)\n",
        "        t_tok = t_tok.reshape(t_tok.shape[0], self.num_time_tokens, self.tok_dim)\n",
        "        cond_hid, c_tok = self.cond_mlp(cond).split([self.emb_dim, self.tok_dim], dim=-1)\n",
        "        # print('unet fwd', t_hid.shape, t_tok.shape, t.shape)\n",
        "\n",
        "        t = t + cond_hid # [batch, emb_dim]\n",
        "        c = torch.cat((t_tok, c_tok.unsqueeze(1)), dim=-2) # [b, num_toks, tok_dim]\n",
        "        c = self.norm_cond(c)\n",
        "\n",
        "        hiddens = []\n",
        "        # for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "        for init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "            # x = pre_downsample(x)\n",
        "            x = init_block(x, t, c)\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = resnet_block(x, t)\n",
        "                hiddens.append(x)\n",
        "            x = attn_block(x, c)\n",
        "            hiddens.append(x)\n",
        "            x = post_downsample(x)\n",
        "\n",
        "        # print('unet fwd', x.shape, t.shape, c.shape)\n",
        "        x = self.mid_block(x, t, c)\n",
        "\n",
        "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
        "            x = torch.cat((x, hiddens.pop() * 2**-.5), dim = 1)\n",
        "            x = init_block(x, t, c)\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = torch.cat((x, hiddens.pop() * 2**-.5), dim = 1)\n",
        "                x = resnet_block(x, t)\n",
        "            x = attn_block(x, c)\n",
        "            x = upsample(x)\n",
        "\n",
        "        # x = torch.cat((x, hiddens.pop()), dim = 1)\n",
        "        x = self.final_res_block(x, t)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "model = UNet(d_model=128, c_dim=10, in_ch=1, dim_mults = (1, 2, 3, 4)).to(device)\n",
        "batch = 1\n",
        "# x = torch.rand((batch, 3, 64, 64), device = device)\n",
        "x = torch.rand((batch, 1,16,16), device = device)\n",
        "t = torch.rand(batch, device = device)\n",
        "# img_cond = torch.rand((batch, 512, 64, 64), device = device)\n",
        "cond = torch.rand((batch, 10), device = device)\n",
        "out = model(x, t, cond)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1wCQ0gZ-Hz3i"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JYMQDoL578HQ"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module): # down shortcut [b,c,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch*r**2, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,2h,2w] -> [b,4c,h,w] -> [b,o,4c/o,h,w] -> [b,o,h,w]\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        x = self.samech(x)\n",
        "        return x\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch, out_ch*r**2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        x = self.samech(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch, out_ch*r**2)\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale r r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        # down\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        x = self.samech(x)\n",
        "\n",
        "        # up\n",
        "        x = self.samech(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r-cYioQer02v"
      },
      "outputs": [],
      "source": [
        "# @title CrossEmbedLayer PixelShuffleConv\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_sizes, stride=1):\n",
        "        super().__init__()\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "\n",
        "        r=2\n",
        "        balls = out_ch//r**2\n",
        "        mult = [1/(1.6**i) for i in range(len(kernel_sizes))]\n",
        "        mul = balls/sum(mult)\n",
        "        mult = [m*mul for m in mult]\n",
        "        dim_scales = [0]*len(kernel_sizes)\n",
        "        for i in range(balls):\n",
        "            ind = mult.index(max(mult))\n",
        "            dim_scales[ind] += 1\n",
        "            mult[ind] -= 1\n",
        "        dim_scales = [d*r**2 for d in dim_scales]\n",
        "        if 0 in dim_scales: print('dim_scales',dim_scales)\n",
        "        # 1/2 + 1/4 + 1/8 + ... + 1/2^num_kernels + 1/2^num_kernels of out_ch; smaller kernel allocated more channels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(in_ch, dim_scale, kernel, stride=stride, padding=(kernel-stride)//2) for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "        out = torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "        b,c,h,w = out.shape\n",
        "        out = out.reshape(b, -1, 4, h, w).transpose(1,2).reshape(b, c, h, w)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch = None, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch, self.r = in_ch, out_ch, r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        self.net.apply(self.init_conv_)\n",
        "\n",
        "    def init_conv_(self, conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "        if isinstance(conv, nn.Conv2d):\n",
        "            o, i, h, w = conv.weight.shape\n",
        "            conv_weight = torch.empty(self.out_ch, self.in_ch, h, w)\n",
        "            nn.init.kaiming_uniform_(conv_weight)\n",
        "            # print(conv.weight.shape, conv_weight.shape,max(self.r, int(1/self.r)), (0 if self.r>1 else 1))\n",
        "            conv.weight.data.copy_(conv_weight.repeat_interleave(max(self.r, int(1/self.r))**2, dim=(0 if self.r>1 else 1)))\n",
        "            nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# d=PixelShuffleConv(3, 16, 7, r=1/2)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = PixelShuffleConv(in_ch=3, r=2).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "# input = torch.rand((4,3,64,64), device=device)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n",
        "\n",
        "# model = PixelShuffleConv(in_ch=3, r=1/2).to(device)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IQlJ4XVLrOSx"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}