{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/latent-flow-matching/blob/main/latent_flow_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOiDHI7taUKF"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "B8Pm-Fw6jn4A"
      },
      "outputs": [],
      "source": [
        "# @title mha me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        Q, K = self.rope(Q), self.rope(K)\n",
        "\n",
        "        # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.lin(out) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.self(self.norm1(x))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        return x.transpose(1,2).reshape(*bchw)\n",
        "\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# model = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = model(x, cond, mask)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "j9wUA7Vk0Y03"
      },
      "outputs": [],
      "source": [
        "# @title rope & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "        if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "        return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, h=224, w=224, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.h, self.w = dim, h, w\n",
        "        # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "        # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "        theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "        y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "        y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "        # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "        # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "        self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "        # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "    def forward(self, img): #\n",
        "        # batch, dim, h, w = img.shape\n",
        "        # print(img.shape)\n",
        "        hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "        h=w=int(hw**.5)\n",
        "        if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "        # print(self.rot_emb.shape)\n",
        "        # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "        rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "        # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "        return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "        # return img * self.rot_emb\n",
        "\n",
        "\n",
        "# def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "#     # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#     theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#     y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#     y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#     rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#     # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#     return rot_emb\n",
        "\n",
        "\n",
        "# rotemb = RotEmb(10)\n",
        "# seq_len=10\n",
        "# pos = torch.linspace(0,1,seq_len).to(device)#.unsqueeze(-1)\n",
        "# rot_emb = rotemb(pos)\n",
        "# print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test rope\n",
        "dim=8\n",
        "# posemb = RoPE(dim, seq_len=512, base=10000)\n",
        "posemb = RoPE2D(dim, h=32, w=32, base=100)\n",
        "\n",
        "for x in posemb.rot_emb[0]:\n",
        "    print(x)\n",
        "\n",
        "\n",
        "def precompute_freqs_cis_2d(dim, end, theta = 10000.0, scale=1.0, use_cls=False):\n",
        "    H = int( end**.5 )\n",
        "    # assert  H * H == end\n",
        "    flat_patch_pos = torch.arange(0 if not use_cls else -1, end) # N = end\n",
        "    x_pos = flat_patch_pos % H # N\n",
        "    y_pos = flat_patch_pos // H # N\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim)) # Hc/4\n",
        "    x_freqs = torch.outer(x_pos, freqs).float() # N Hc/4\n",
        "    y_freqs = torch.outer(y_pos, freqs).float() # N Hc/4\n",
        "    x_cis = torch.polar(torch.ones_like(x_freqs), x_freqs)\n",
        "    y_cis = torch.polar(torch.ones_like(y_freqs), y_freqs)\n",
        "    freqs_cis = torch.cat([x_cis.unsqueeze(dim=-1), y_cis.unsqueeze(dim=-1)], dim=-1) # N,Hc/4,2\n",
        "    freqs_cis = freqs_cis.reshape(end if not use_cls else end + 1, -1)\n",
        "    # we need to think how to implement this for multi heads.\n",
        "    # freqs_cis = torch.cat([x_cis, y_cis], dim=-1) # N, Hc/2\n",
        "    return freqs_cis\n",
        "\n",
        "dim, end = 8, 25\n",
        "o = precompute_freqs_cis_2d(dim, end)\n",
        "print(o)\n"
      ],
      "metadata": {
        "id": "vtEM7L7KHri4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import numpy as np\n",
        "# abs = torch.tensor([1, 2], dtype=torch.float64)\n",
        "# angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n",
        "# z = torch.polar(abs, angle)\n",
        "# z\n",
        "\n",
        "\n",
        "# polar = torch.randn(4,3,2)\n",
        "\n",
        "# r = polar[:,:,0]\n",
        "# theta = polar[:,:,1]\n",
        "\n",
        "# x = r * torch.cos(theta)\n",
        "# y = r * torch.sin(theta)\n",
        "r*theta.sin()\n",
        "# cartesian = torch.stack([x,y], axis = -1)\n"
      ],
      "metadata": {
        "id": "c3Va1bed5qbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "2nu4Dzma_cD5"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        # self.block = nn.Sequential(\n",
        "        self.block = Seq(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            # nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "            nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        return self.block(x, emb) + self.res_conv(x)\n",
        "\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "\n",
        "# class Res(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#     def forward(self, x): return x + self.model(x)\n",
        "\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, cond=None):\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             if 'cond' in layer._fwdparams: args.append(cond)\n",
        "#             x = layer(*args)\n",
        "#         return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "GjvZZswH1_KR"
      },
      "outputs": [],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1, emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, emb_dim), nn.PixelShuffle(r))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, emb_dim))\n",
        "        # # elif in_ch!=out_ch: self.net = ResBlock(in_ch, out_ch)\n",
        "        # else: self.net = ResBlock(in_ch, out_ch, emb_dim)\n",
        "        if self.r>1: self.net = Seq(ResBlock(in_ch, out_ch*r**2, emb_dim), nn.PixelShuffle(r))\n",
        "        elif self.r<1: self.net = Seq(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, emb_dim))\n",
        "        else: self.net = Seq(ResBlock(in_ch, out_ch, emb_dim))\n",
        "\n",
        "    def forward(self, x, emb=None):\n",
        "        return self.net(x, emb)\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1, emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r, emb_dim=emb_dim)\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w]\n",
        "        out = self.block(x, emb)\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        return out + shortcut\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4kbIqclOK0t",
        "outputId": "9f646bf9-d349-4929-c6c6-d9c5de52b438",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[128, 256]\n",
            "torch.Size([64, 3, 64, 64])\n",
            "6014980\n"
          ]
        }
      ],
      "source": [
        "# @title U-DiT me\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b,c,h,w = x.shape\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "        # x = F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h,w] -> [c,b*r^2,h/r,w/r] -> [b,h/r*w/r,c]\n",
        "\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        x = x.transpose(-2,-1).reshape(b,c,h,w) # [batch, n_heads, T/num_tok, d_head] -> [batch, n_heads*d_head, T/num_tok] -> [b,c,h,w]\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = SelfAttn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        # self.mlp = ResBlock(d_model)\n",
        "        # self.mlp = UpDownBlock(d_model)\n",
        "        # self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 6*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 3*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        # shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        shift_msa, scale_msa, gate_msa = self.adaLN_modulation(cond)[...,None,None].chunk(3, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        # x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            # UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "            UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, emb_dim=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        emb_dim = emb_dim or d_model# * 4\n",
        "        self.time_emb = nn.Sequential(RotEmb(d_model), nn.Linear(d_model, d_model), nn.SiLU(), nn.Linear(d_model, 3*emb_dim))\n",
        "        self.cond_emb = nn.Linear(cond_dim, 3*d_model)\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 1\n",
        "        # mult = [1,1,1,1] # [1,1,1,1] [1,2,3,4] [1,2,2,2]\n",
        "        mult = [1,2,3,4] # [1,1,1,1] [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        print(ch_list)\n",
        "\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1) for i in range(depth-1)])\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1),\n",
        "            # AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            # U_DiTBlock(ch_list[0], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=1),\n",
        "        )\n",
        "        # self.middle_block = Seq(\n",
        "        #     # UpDownBlock(ch_list[0]),\n",
        "        #     U_DiTBlock(d_model, cond_dim=d_model, n_heads=d_model//d_head),\n",
        "        #     # UpDownBlock(ch_list[0]),\n",
        "        #     U_DiTBlock(d_model, cond_dim=d_model, n_heads=d_model//d_head),\n",
        "        # )\n",
        "\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1) for i in reversed(range(depth-1))])\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # # self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.out = nn.Conv2d(d_model, out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "        self.skip_scale = nn.Parameter(torch.tensor(2**-.5))\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        c123 = self.time_emb(t) + self.cond_emb(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "        x = self.in_block(x)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            # x = torch.cat([x, blocks[-i-1]*2**-.5], dim=1)\n",
        "            # print(len(self.up_list)-i)\n",
        "            x = torch.cat([x, blocks[-i-1]*self.skip_scale**(len(self.up_list)-i)], dim=1) # https://arxiv.org/pdf/2310.13545\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "        return self.out(x)\n",
        "\n",
        "# norm,act,zeroconv < finallyr\n",
        "# attnblk =< uditblk(no down)?\n",
        "\n",
        "# nope, posemb input, rope qk\n",
        "# adaln, crossattn\n",
        "\n",
        "# downattn decreases faster but plateaus\n",
        "# no UpDownBlock?\n",
        "\n",
        "# nomlp in uditblk\n",
        "# updown downscale saves vram: 6->3gb\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "# model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "model = U_DiT(in_ch=3, d_model=128, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Nfr2mmxuzO",
        "outputId": "3d8447a9-3039-433e-b6bd-ef954df2ddbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146512\n",
            "261712\n"
          ]
        }
      ],
      "source": [
        "# for name, param in model.named_parameters(): print(name, param.numel())\n",
        "print(sum(p.numel() for p in model.down_list.parameters() if p.requires_grad)) # 19683\n",
        "print(sum(p.numel() for p in model.up_list.parameters() if p.requires_grad)) # 19683\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),) # do not normalise! want img in [0,1)\n",
        "test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 128 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ubkj5kR6Or8V"
      },
      "outputs": [],
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bHFHA_cXOvA5"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train_data = BufferDataset(buffer) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 512 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2bipgghY7O",
        "outputId": "1862ff3b-5e7a-4d86-8c54-5303b3ed4a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py:56: UserWarning: <class '__main__.LogitNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title LogitNormal\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogitNormal(dist.Distribution):\n",
        "    def __init__(self, mu=0, std=.5):\n",
        "        super().__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "        self._normal = dist.Normal(mu, std) # https://pytorch.org/docs/stable/distributions.html#normal\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        eps = self._normal.rsample(sample_shape)\n",
        "        return torch.sigmoid(eps) # https://en.wikipedia.org/wiki/Logit-normal_distribution\n",
        "\n",
        "logit_normal = LogitNormal()\n",
        "# samples = logit_normal.rsample((10,))\n",
        "# print(samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "QRPa3VWuQDIW"
      },
      "outputs": [],
      "source": [
        "# @title sampling timestep\n",
        "# def InverseSigmoid(x): return torch.log(x/(1-x))\n",
        "# def Normal(x, mu=0, std=.5): return torch.exp(-.5*((x-mu)/std)**2)/(std*(2*torch.pi)**2)\n",
        "# def LogitNormalPDF(x, mu=0, std=.5): return torch.nan_to_num(Normal(logit(x), mu, std) * 1/(x*(1-x)))\n",
        "\n",
        "# def invlogit(x): return torch.exp(x)/(1+torch.exp(x))\n",
        "# def InvLogitNormalCDF(x, mu=0, std=.5):\n",
        "#     cdf = invlogit(torch.erfinv(2*x-1)*(2**.5*std)+mu)\n",
        "#     cdf[x==1.] = 1 # lol, replace nan with 1 when x=1\n",
        "#     return cdf\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "def Cosine(x): return .5*(-torch.cos(torch.pi*x)+1) # _/-\n",
        "def Polynomial(x): return -2*x**3 + 3*x**2 # -2x^3 + 3x^2 # _/-\n",
        "def ACosine(x): return torch.acos(1-2*x)/torch.pi # /-/ # x = acos(1-2y)/pi\n",
        "def InvertCubic(x): return (x-1)**3+1 # /-\n",
        "def InvertExp(x, a=4): return (1-torch.exp(-a*x)) / (1-torch.exp(torch.tensor(-a))) # /-\n",
        "\n",
        "\n",
        "a, b = .0, 0\n",
        "def bezier(t, x0=0,y0=0, x1=a,y1=b, x2=1-a,y2=1-b, x3=1,y3=1):\n",
        "    # print(x1,y1)\n",
        "    # return ((1-t)*((1-t)*((1-t)*x0+t*x1)+t*((1-t)*x1+t*x2))+t*((1-t)*((1-t)*x1+t*x2)+t*((1-t)*x2+t*x3)), (1-t)*((1-t)*((1-t)*y0+t*y1) +t*((1-t)*y1+t*y2))+t*((1-t)*((1-t)*y1+t*y2) +t*((1-t)*y2+t*y3)))\n",
        "    return (1-t)*((1-t)*((1-t)*x0+t*x1)+t*((1-t)*x1+t*x2))+t*((1-t)*((1-t)*x1+t*x2)+t*((1-t)*x2+t*x3))\n",
        "    # return (1-t)*((1-t)*((1-t)*y0+t*y1) +t*((1-t)*y1+t*y2))+t*((1-t)*((1-t)*y1+t*y2) +t*((1-t)*y2+t*y3))\n",
        "\n",
        "\n",
        "# x = torch.linspace(0, 1, 30)\n",
        "# y=x\n",
        "# y = LogitNormalCDF(x, mu=0, std=3) # _/- for std<1.8; /-/ for std>1.8\n",
        "# y = Cosine(x) # _/-\n",
        "# y = ACosine(x) # /-/\n",
        "# y = Polynomial(x) # _/-\n",
        "# y = InvertCubic(x) # /-\n",
        "# y = InvertExp(x) # /-\n",
        "# y = bezier(x) # _/-\n",
        "# print(x, y)\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(x, y)\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "OFc76OzlFnE1"
      },
      "outputs": [],
      "source": [
        "# @title Sampling\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def reverse_flow(unet, cond, timesteps=25): # [n_samples, cond_dim]\n",
        "    training = unet.training\n",
        "    unet.eval()\n",
        "    i = torch.linspace(0, 1, timesteps+1)\n",
        "    # y = i # linear\n",
        "    # y = LogitNormalCDF(i, mu=0, std=3.) # .5 _/- for std<1.8; 3 /-/ for std>1.8\n",
        "    y = LogitNormalCDF(i, mu=-.0, std=2.7) # .5 _/- for std<1.8; 3 /-/ for std>1.8\n",
        "    # y = Cosine(i) # _/-\n",
        "    # y = ACosine(i) # /-/\n",
        "    # y = Polynomial(i) # _/-\n",
        "    # y = InvertCubic(i) # /-\n",
        "    # y = InvertExp(i) # /-\n",
        "    # y = bezier(i) # _/-\n",
        "\n",
        "    dt = y[1:]-y[:-1]\n",
        "    num_samples = cond.shape[0]\n",
        "    # x = torch.randn((num_samples, unet.in_ch, 16,16), device=device)\n",
        "    x = torch.randn((num_samples, 3, 64,64), device=device)\n",
        "    # cond = cond.repeat(num_samples,1) # [n_samples, cond_dim]\n",
        "    for y, dt in zip(y, dt):\n",
        "        # print(y, dt)\n",
        "        t = torch.full((num_samples,), y, device=device)  # Current time # [num_samples] 1. # torch.tensor(i * dt, device=device).repeat(n_samples)\n",
        "        with torch.no_grad():\n",
        "            model = lambda y,t: -unet(y, t, cond)\n",
        "            v = model(x, t)\n",
        "            x = x - dt * v # Euler update # 25steps:1sec\n",
        "\n",
        "            # k1 = model(x, t)\n",
        "            # k2 = model(x - 0.5 * dt * k1, t - 0.5 * dt)\n",
        "            # k3 = model(x - 0.5 * dt * k2, t - 0.5 * dt)\n",
        "            # k4 = model(x - dt * k3, t - dt)\n",
        "            # x = x - (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4) # RK4 update # 25steps:4.5sec\n",
        "    if training: unet.train()\n",
        "    return x\n",
        "\n",
        "\n",
        "# # cond = F.one_hot(torch.tensor([3]*16, device=device), num_classes=10).to(torch.float)\n",
        "# cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "# # img_ = reverse_flow(model, cond, timesteps=10)\n",
        "# img_ = model.sample(cond)\n",
        "# # # plt.imshow(img_.cpu().squeeze())\n",
        "# # # plt.show()\n",
        "# imshow(torchvision.utils.make_grid(img_.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbSq3Zx7aRL-"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iJ1hbnSC12tA"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=256, out_ch=None, kernels=[7,5], mult=[1]):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.d_model, self.out_ch = in_ch, d_model, out_ch\n",
        "        d_list=[d_model*m for m in mult]\n",
        "        in_list, out_list = [in_ch, *d_list[:-1]], [*d_list[:-1], out_ch]\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=1/2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act,) for i, (in_dim, out_dim, kernel) in enumerate(zip(in_list, out_list, kernels))], # conv,norm,act except for last layer: no norm\n",
        "            # PixelShuffleConv(in_ch, d_list[0], 7, r=1/2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConv(d_list[0], out_ch, 5, r=1/2), act,\n",
        "            # nn.Conv2d(in_ch, out_ch, 7, 2, 7//2), nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # nn.PixelUnshuffle(4), init_conv(nn.Conv2d(3*4**2, d_list[0]*1**2, 7, 1, padding=7//2), out_r=1, in_r=4),\n",
        "            # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3*2**2, d_list[0]*2**2, 7, 1, padding=7//2), out_r=2, in_r=2), nn.PixelUnshuffle(2),\n",
        "            init_conv(nn.Conv2d(3*1**2, d_list[0]*4**2, 7, 1, padding=7//2), out_r=4, in_r=1), nn.PixelUnshuffle(4),\n",
        "        )\n",
        "            # nn.PixelUnshuffle(4), ResBlock(in_ch*4**2, out_ch, emb_dim=out_ch), AttentionBlock(out_ch),\n",
        "\n",
        "        # nn.init.zeros_(self.encoder.parameters()[-1])\n",
        "        # self.encoder[-2].apply(self.zero_conv_)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), *(nn.BatchNorm2d(out_dim), act) if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # PixelShuffleConv(out_ch, d_list[0], 5, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConv(d_list[0], in_ch, 7, r=2),\n",
        "            # nn.Upsample(scale_factor=2), nn.ConvTranspose2d(out_ch, in_ch, 7, 2, 7//2, output_padding=1),# nn.BatchNorm2d(d_list[1]), nn.SiLU(),\n",
        "\n",
        "            # init_conv(nn.Conv2d(2*1**2, d_list[0]*4**2, 7, 1, padding=7//2), out_r=4, in_r=1), nn.PixelShuffle(4),\n",
        "            # nn.PixelShuffle(2), init_conv(nn.Conv2d(2*2**2, d_list[0]*2**2, 7, 1, padding=7//2), out_r=2, in_r=2), nn.PixelShuffle(2),\n",
        "            nn.PixelShuffle(4), init_conv(nn.Conv2d(2*4**2, d_list[0]*1**2, 7, 1, padding=7//2), out_r=1, in_r=4),\n",
        "        )\n",
        "            # ResBlock(out_ch, in_ch*4**2, emb_dim=in_ch*4**2), AttentionBlock(in_ch*4**2), nn.PixelShuffle(4),\n",
        "\n",
        "        # for param in self.parameters():\n",
        "        # nn.init.zeros_(self.decoder.parameters()[-1])\n",
        "    #     self.decoder[-1].apply(self.zero_conv_)\n",
        "\n",
        "    # def zero_conv_(self, conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "    #     if isinstance(conv, nn.Conv2d):\n",
        "    #         nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x): return self.encoder(x)\n",
        "    def decode(self, x): return self.decoder(x)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "# model_ch=16\n",
        "d_list=[16, 16] # [16,32]\n",
        "k_list=[7,5] # [7,5]\n",
        "# model = PixelAE(in_ch=in_ch, d_list=d_list, k_list=k_list)\n",
        "model = PixelAE(in_ch=in_ch, out_ch=16).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,in_ch,64,64), device=device)\n",
        "enc = model.encode(input)\n",
        "print(enc.shape)\n",
        "out = model.decode(enc)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q43yKETuk3P"
      },
      "outputs": [],
      "source": [
        "model = PixelAE()\n",
        "# nn.init.zeros_(model.decoder.parameters()[-1])\n",
        "# print(model.decoder.parameters())\n",
        "# print(model.decoder[-2])\n",
        "# nn.init.zeros_(model.decoder[-2])\n",
        "# model.decoder[-2].apply(nn.init.zeros_)\n",
        "\n",
        "def init_conv_(conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "    if isinstance(conv, nn.Conv2d):\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "model.decoder[-2].apply(init_conv_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nvBRca98P-GW"
      },
      "outputs": [],
      "source": [
        "# @title latent flow model\n",
        "\n",
        "class LFM(nn.Module): # latent flow model\n",
        "    def __init__(self, in_ch=3, d_list=[16, 3], d_model=16, cond_dim=16, depth=3, ae=None):\n",
        "        super().__init__()\n",
        "        self.ae = ae or PixelAE(in_ch, d_model)\n",
        "        self.unet = UNet(in_ch=in_ch, d_model=d_model, cond_dim=cond_dim, depth=3)\n",
        "\n",
        "    def loss(self, img, cond): # [b,c,h,w], [b,cond_dim]\n",
        "        x1 = self.ae.encode(img)\n",
        "        img_ = self.ae.decode(x1)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        fm_loss = otfm_loss(self.unet, x1.detach(), cond)\n",
        "        return ae_loss, fm_loss\n",
        "        # loss = ae_loss + fm_loss\n",
        "        # return loss\n",
        "\n",
        "    def ae_loss(self, img):\n",
        "        img_ = self.ae(img)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        return ae_loss\n",
        "\n",
        "\n",
        "    # self.unet(x, t=None, cond=None)\n",
        "    # def forward(self, cond, timesteps=10): # [N, C, ...]\n",
        "    def sample(self, cond=None, timesteps=10):\n",
        "        self.eval()\n",
        "        # cond = F.one_hot(torch.tensor([4]*16, device=device), num_classes=10).to(torch.float)\n",
        "        # if cond is None: cond = F.one_hot(torch.arange(16, dtype=torch.float, device=device)%10, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x1_ = reverse_flow(self.unet, cond, timesteps=timesteps)\n",
        "            img_ = self.ae.decode(x1_)\n",
        "        return img_\n",
        "\n",
        "model = LFM(d_list=[16, 16], d_model=16, cond_dim=10, depth=3).to(device)\n",
        "# model = LFM(ae=model.ae, d_list=[16, 16], d_model=16, cond_dim=10, depth=3).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=3e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "rfsabyM8P7q3",
        "outputId": "f034452a-be33-4577-a0a3-0d6433163a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.1078898906707764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4628855..2.6372225].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 0.955610454082489\n",
            "20 0.7199699282646179\n",
            "30 0.46871012449264526\n",
            "40 0.2371058464050293\n",
            "50 0.10695777833461761\n",
            "60 0.07886366546154022\n",
            "70 0.05878458917140961\n",
            "80 0.05245543643832207\n",
            "90 0.05044519528746605\n",
            "100 0.04862472414970398\n",
            "110 0.0474155955016613\n",
            "120 0.04420723021030426\n",
            "130 0.04494550824165344\n",
            "140 0.043353743851184845\n",
            "150 0.04242460057139397\n",
            "160 0.03876667842268944\n",
            "170 0.04421078413724899\n",
            "180 0.04076073691248894\n",
            "190 0.039281152188777924\n",
            "200 0.03828150033950806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.318641..1.1535679].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210 0.036732517182826996\n",
            "220 0.03625468537211418\n",
            "230 0.03556510806083679\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def otfm_loss(model, x1, cond, sig_min = 0.001, eps = 1e-5): # UNetModel, [b,c,h,w], [b,cond_dim] # https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "    batch = x1.size(0)\n",
        "    # t = torch.rand((batch,), device=device) % (1 - eps)\n",
        "    t = logit_normal.rsample((batch,)).to(device) # in [0,1] [batch,1]\n",
        "    t_ = t[...,None,None,None]\n",
        "    x0 = torch.randn_like(x1)\n",
        "    psi_t = (1 - (1-sig_min)*t_)*x0 + t_*x1 # ψt(x) = (1 − (1 − σmin)t)x + tx1, (22)\n",
        "    v_psi = model(psi_t, t, cond) # vt(ψt(x0))\n",
        "    d_psi = x1 - (1 - sig_min) * x0 #\n",
        "    return F.mse_loss(v_psi, d_psi) # LCFM(θ)\n",
        "\n",
        "\n",
        "def ae_train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "    # for i, img in enumerate(dataloader):\n",
        "    #     img = img.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "            # loss = otfm_loss(model, x1, cond) # unet\n",
        "            # img = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # loss = model.ae_loss(img)\n",
        "\n",
        "            x1 = model.ae.encode(img)\n",
        "            img_ = model.ae.decode(x1)\n",
        "            ae_loss = F.mse_loss(img_, img)\n",
        "            loss = ae_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i,loss.item())\n",
        "            # print(i,ae_loss.item(), fm_loss.item())\n",
        "        if i % 200 == 0:\n",
        "            imshow(torchvision.utils.make_grid(img[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "            imshow(torchvision.utils.make_grid(x1[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "            imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "\n",
        "        try: wandb.log({\"ae_loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "\n",
        "def train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "    # for i, img in enumerate(dataloader):\n",
        "    #     img = img.to(device)\n",
        "        # # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "        # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "        img = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            # x1 = model.ae.encode(img)\n",
        "            # img_ = model.ae.decode(x1)\n",
        "            # loss = F.mse_loss(img_, img)\n",
        "\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # with torch.no_grad():\n",
        "            #     x1 = model.ae.encode(img)\n",
        "            #     img_ = model.ae.decode(x1)\n",
        "            # fm_loss = otfm_loss(model.unet, x1.detach(), cond)\n",
        "            loss = otfm_loss(model, img, cond)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i,loss.item())\n",
        "            # print(i,ae_loss.item(), fm_loss.item())\n",
        "        if i % 200 == 0:\n",
        "            with torch.no_grad():\n",
        "                cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "                img_ = reverse_flow(model, cond, timesteps=10) # unet\n",
        "                # img_ = model.sample(cond, timesteps=10) # ldm\n",
        "                # x1_ = reverse_flow(model.unet, cond, timesteps=10)\n",
        "                # print(x1.dtype)\n",
        "                # imshow(torchvision.utils.make_grid(x1_[:4].cpu(), nrow=4))\n",
        "\n",
        "                # img_ = model.ae.decode(x1_.float())\n",
        "                # with torch.no_grad(): img_ = model.ae.decode(x1_)\n",
        "                imshow(torchvision.utils.make_grid(img_[:4].cpu(), nrow=4))\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "for epoch in range(2):\n",
        "# for epoch in range(40):\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    # for x,y in test_loader: break\n",
        "    # img = F.interpolate(x.to(device), size=(64,64)).repeat(1,3,1,1)\n",
        "    # x1 = model.ae.encode(img)\n",
        "    # img_ = model.ae.decode(x1)\n",
        "    # imshow(torchvision.utils.make_grid(img[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "    # imshow(torchvision.utils.make_grid(x1[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "    # imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # # # cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).expand(16,-1).to(torch.float)\n",
        "        cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "        img_ = reverse_flow(model, cond, timesteps=10) # unet\n",
        "        # img_ = model.sample(cond, timesteps=10) # ldm\n",
        "        imshow(torchvision.utils.make_grid(img_.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "ecSX2oJJ4_2F"
      },
      "outputs": [],
      "source": [
        "# @title test func\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "        # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # loss = ae_loss + fm_loss\n",
        "            x1 = F.interpolate(x1, size=(16,16))#.repeat(1,3,1,1)\n",
        "            loss = otfm_loss(model, x1, cond) # unet\n",
        "        total_loss+=loss\n",
        "    print(total_loss/len(dataloader))\n",
        "    # if i % 10 == 0: print(i,ae_loss.item(),fm_loss.item())\n",
        "    try: wandb.log({\"test_loss\": loss.item()})\n",
        "    # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "    except: pass\n",
        "\n",
        "\n",
        "# test(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "3f941a4a-af71-4cbd-85a1-62f84be6b6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250328_075026-wmjec106</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/lfm/runs/wmjec106' target=\"_blank\">major-night-99</a></strong> to <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">https://wandb.ai/bobdole/lfm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/lfm/runs/wmjec106' target=\"_blank\">https://wandb.ai/bobdole/lfm/runs/wmjec106</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"lfm\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGN0e0Mxe5UI"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'lfm.pkl', map_location=device).values()\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrFxTtw4eFSq"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'lfm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVSdLd7um-Rs"
      },
      "outputs": [],
      "source": [
        "# @title WCDM\n",
        "# High Fidelity Visualization of What Your Self-Supervised Representation Knows About aug 2022\n",
        "# https://arxiv.org/pdf/2112.09164\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFhmfVnrOiQd"
      },
      "outputs": [],
      "source": [
        "# # dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# # print(x)\n",
        "# print(x.shape)\n",
        "# x=x.unsqueeze(0)\n",
        "# # x1 = F.interpolate(x, size=(16,16))#.repeat(1,3,1,1)\n",
        "# x1 = F.interpolate(x, size=(64,64)).repeat(1,3,1,1)\n",
        "# imshow(torchvision.utils.make_grid(x1.cpu(), nrow=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQlJ4XVLrOSx"
      },
      "source": [
        "## save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JxGF8bYeGDA2"
      },
      "outputs": [],
      "source": [
        "# @title UIB\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential( # norm,act,conv\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, in_ch, kernel, 1, kernel//2, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, mult*in_ch, kernel, 1, kernel//2, groups=mult*in_ch, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, out_ch, 1, bias=False),\n",
        "            # nn.BatchNorm2d(mult*in_ch), act, zero_module(nn.Conv2d(mult*in_ch, out_ch, 1, bias=False)),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# # in_ch, out_ch = 16,3\n",
        "# in_ch, out_ch = 3,16\n",
        "# model = UIB(in_ch, out_ch)\n",
        "# x = torch.rand(128, in_ch, 64, 64)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0E7ovklT83O",
        "outputId": "a509a3d7-73be-4efa-f8fa-fe1179af9093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 64, 64])\n",
            "116259\n"
          ]
        }
      ],
      "source": [
        "# @title U-DiT next\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = x #+ self.layer(x)\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "# conv res pixeldown\n",
        "\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.rope = RoPE2D(d_head, h=64, w=64, base=10000)\n",
        "        # print(d_head)\n",
        "        self.scale = d_head**-.5\n",
        "        # self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        bchw = x.shape\n",
        "        # x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        # q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        x = x.transpose(-2,-1).reshape(bchw) # [batch, n_heads, T/num_tok, d_head] -> [batch, n_heads*d_head, T/num_tok] -> [b,c,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "# class TimeEmb(nn.Module):\n",
        "#     def __init__(self, d_model, emb_dim):\n",
        "#         super().__init__()\n",
        "#         self.rot_emb = RotEmb(d_model)\n",
        "#         self.mlp = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "#     def forward(self, t): return self.mlp(self.rot_emb(t))\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = SelfAttn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 6*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class CombineAttnBlk(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = SelfAttn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        # if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 6*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        if self.cond_dim!=None: x = x + gate_mca * self.cross((1 + scale_mca) * self.norm(x) + shift_mca, cond, mask)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, emb_dim=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        emb_dim = emb_dim or d_model# * 4\n",
        "        # self.time_emb = TimeEmb(d_model, 3*emb_dim)\n",
        "        self.time_emb = nn.Sequential(RotEmb(d_model), nn.Linear(d_model, d_model), nn.SiLU(), nn.Linear(d_model, 3*emb_dim))\n",
        "\n",
        "        self.cond_emb = nn.Linear(cond_dim, 3*d_model)\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 1\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        # print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            # AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # # self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.out = nn.Conv2d(d_model, out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        c123 = self.time_emb(t) + self.cond_emb(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "        x = self.in_block(x)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "        return self.out(x)\n",
        "\n",
        "# norm,act,zeroconv < finallyr\n",
        "# attnblk =< uditblk(no down)?\n",
        "\n",
        "# nope, posemb input, rope qk\n",
        "\n",
        "# adaln, crossattn\n",
        "\n",
        "# downattn\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# uib<res\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oFXiQfSn1i3u"
      },
      "outputs": [],
      "source": [
        "# @title unet to dit\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            # *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, emb_dim=None, cond_dim=16, depth=4, num_res_blocks=1, n_heads=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_heads = d_model // d_head\n",
        "\n",
        "        emb_dim = emb_dim or d_model# * 4\n",
        "        self.time_emb = nn.Sequential(RotEmb(d_model), nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "        # self.time_emb = TimeEmb(d_model, emb_dim)\n",
        "        self.cond_emb = nn.Linear(cond_dim, d_model)\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "        # self.pos_embed = nn.Parameter(torch.randn(1, self.extras + num_patches, embed_dim)*0.02)\n",
        "\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            # U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=n_heads),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [b,c,h,w], [b], [b, cond_dim]\n",
        "        emb = self.time_emb(t) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "        cond = self.cond_emb(cond)\n",
        "        x = self.in_block(x) if self.in_ch!=self.d_model else x\n",
        "\n",
        "        # x = x + self.pos_embed\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print(\"unet fwd down\", x.shape)\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        # print(\"unet fwd mid1\", x.shape)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        # print(\"unet fwd mid2\", x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            # print(\"unet fwd up\", x.shape)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x) #if self.out_ch!=self.d_model else x\n",
        "\n",
        "\n",
        "\n",
        "# # 64,64 -vae-> 16,16 -unet->\n",
        "batch = 64\n",
        "cond_dim=10\n",
        "in_ch = 3\n",
        "model = UNet(in_ch=in_ch, d_model=16, cond_dim=cond_dim, depth=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 5311187\n",
        "# # print(model)\n",
        "\n",
        "# x=torch.rand((batch,in_ch,16,16),device=device)\n",
        "x=torch.rand((batch,in_ch,64,64),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# # cond_emb = nn.Embedding(10, cond_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r4htE9Mz-JiO"
      },
      "outputs": [],
      "source": [
        "# @title U-DiT next\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = x #+ self.layer(x)\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "# conv res pixeldown\n",
        "\n",
        "\n",
        "class DownSample_Attn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.scale = d_head**-.5 # v1\n",
        "\n",
        "        # self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # b, _, h, w = x.size()\n",
        "        bchw = x.shape\n",
        "        # x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        x = x.transpose(-2,-1).reshape(bchw) # [batch, n_heads, T/num_tok, d_head] -> [batch, n_heads*d_head, T/num_tok] -> [b,c,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class TimeEmb(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.rot_emb = RotEmb(emb_dim)\n",
        "        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "    def forward(self, t): return self.mlp(self.rot_emb(t))\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    \"\"\"A IPT block with adaptive layer norm zero (adaLN-Zero) conIPTioning.\"\"\"\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = DownSample_Attn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        # self.mlp = FeedForward(d_model)\n",
        "        self.mlp = UIB(d_model, mult=4)\n",
        "        # self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(cond_dim, 6 * d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, d_model, out_ch):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Conv2d(d_model, d_model, kernel_size=3, stride=1, padding=1)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(d_model, 2*d_model))\n",
        "        )\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.out_proj = zero_module(nn.Conv2d(d_model, out_ch, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        shift, scale = self.adaLN_modulation(cond)[...,None,None].chunk(2, dim=1)\n",
        "        # print('FinalLayer', x.shape, shift.shape, scale.shape)\n",
        "        x = self.in_proj(x)\n",
        "        x = (1 + scale) * self.norm(x) + shift\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.time_emb = TimeEmb(d_model*3)\n",
        "        self.cond_emb = nn.Linear(cond_dim, d_model*3)\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 3\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        # print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            # AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # self.out = FinalLayer(d_model, in_ch) # udit\n",
        "        # # self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.out = nn.Conv2d(d_model, out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        x = self.in_block(x) # (N, C, H, W)\n",
        "        c123 = self.time_emb(t) + self.cond_emb(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "\n",
        "        # x = self.final_layer(x, cond=cond[0]) # (N, T, patch_size ** 2 * out_ch)\n",
        "        return self.out(x)\n",
        "        # return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# uib<res\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B4R77KQvNgj4"
      },
      "outputs": [],
      "source": [
        "# @title unet me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        # self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim, cond_dim, n_head=None, d_head=8, updown=False, r=2):\n",
        "        super().__init__()\n",
        "        if updown=='down': in_ch = in_ch*r**2\n",
        "        elif updown=='up': out_ch = out_ch*r**2\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1))\n",
        "        # self.init_conv = CrossEmbedLayer(in_ch, dim_out=d_model, kernel_sizes=(3, 7, 15), stride=1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7//2)\n",
        "\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = ch_list[-1]*2**2 # 512\n",
        "        self.middle_block = Seq(\n",
        "            nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x) if self.in_ch!=self.d_model else x\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x) #if self.out_ch!=self.d_model else x\n",
        "\n",
        "\n",
        "\n",
        "# # # 64,64 -vae-> 16,16 -unet->\n",
        "batch = 64\n",
        "cond_dim=10\n",
        "in_ch = 3\n",
        "model = UNet(in_ch=in_ch, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 5311187\n",
        "# # # print(model)\n",
        "\n",
        "# # x=torch.rand((batch,in_ch,16,16),device=device)\n",
        "# x=torch.rand((batch,in_ch,64,64),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# # # cond_emb = nn.Embedding(10, cond_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kJOnXmd2jGga"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/DiT\n",
        "# https://github.com/facebookresearch/DiT/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L58\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size), nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    @staticmethod\n",
        "    def timestep_embedding(t, dim, max_period=10000):\n",
        "        \"\"\"\n",
        "        Create sinusoidal timestep embeddings.\n",
        "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "                          These may be fractional.\n",
        "        :param dim: the dimension of the output.\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        :return: an (N, D) Tensor of positional embeddings.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=t.device)\n",
        "        args = t[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    def __init__(self, input_size=32, patch_size=2, in_channels=4, hidden_size=1152, depth=28,\n",
        "        num_heads=16, mlp_ratio=4.0, class_dropout_prob=0.1, num_classes=1000, learn_sigma=True):\n",
        "        super().__init__()\n",
        "        # self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        # self.patch_size = patch_size\n",
        "        # self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Initialize label embedding table:\n",
        "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], [b], [b]\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)                   # (N, D)\n",
        "        y = self.y_embedder(y, self.training)    # (N, D)\n",
        "        c = t + y                                # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)                      # (N, T, D)\n",
        "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
        "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
        "        return x\n",
        "\n",
        "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "        model_out = self.forward(combined, t, y)\n",
        "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
        "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
        "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
        "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "# def DiT_XL_2/4/8(**kwargs): return DiT(depth=28, hidden_size=1152, patch_size=2/4/8, num_heads=16, **kwargs)\n",
        "# def DiT_L_2(**kwargs): return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "# def DiT_B_2(**kwargs): return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "# def DiT_S_2(**kwargs): return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IWmZcwoEft7y"
      },
      "outputs": [],
      "source": [
        "# @title baofff/U-ViT\n",
        "# https://github.com/baofff/U-ViT/blob/main/libs/uvit.py#L138\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from .timm import trunc_normal_, Mlp\n",
        "import einops\n",
        "import torch.utils.checkpoint\n",
        "\n",
        "if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "    ATTENTION_MODE = 'flash'\n",
        "else:\n",
        "    try:\n",
        "        import xformers\n",
        "        import xformers.ops\n",
        "        ATTENTION_MODE = 'xformers'\n",
        "    except:\n",
        "        ATTENTION_MODE = 'math'\n",
        "print(f'attention mode is {ATTENTION_MODE}')\n",
        "\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def patchify(imgs, patch_size):\n",
        "    x = einops.rearrange(imgs, 'B C (h p1) (w p2) -> B (h w) (p1 p2 C)', p1=patch_size, p2=patch_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "def unpatchify(x, channels=3):\n",
        "    patch_size = int((x.shape[2] // channels) ** 0.5)\n",
        "    h = w = int(x.shape[1] ** .5)\n",
        "    assert h * w == x.shape[1] and patch_size ** 2 * channels == x.shape[2]\n",
        "    x = einops.rearrange(x, 'B (h w) (p1 p2 C) -> B C (h p1) (w p2)', h=h, p1=patch_size, p2=patch_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        if ATTENTION_MODE == 'flash':\n",
        "            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads).float()\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D\n",
        "            x = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
        "            x = einops.rearrange(x, 'B H L D -> B L (H D)')\n",
        "        elif ATTENTION_MODE == 'xformers':\n",
        "            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B L H D', K=3, H=self.num_heads)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]  # B L H D\n",
        "            x = xformers.ops.memory_efficient_attention(q, k, v)\n",
        "            x = einops.rearrange(x, 'B L H D -> B L (H D)', H=self.num_heads)\n",
        "        elif ATTENTION_MODE == 'math':\n",
        "            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D\n",
        "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "            attn = attn.softmax(dim=-1)\n",
        "            attn = self.attn_drop(attn)\n",
        "            x = (attn @ v).transpose(1, 2).reshape(B, L, C)\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip=False, use_checkpoint=False):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
        "        self.skip_linear = nn.Linear(2 * dim, dim) if skip else None\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        if self.use_checkpoint:\n",
        "            return torch.utils.checkpoint.checkpoint(self._forward, x, skip)\n",
        "        else:\n",
        "            return self._forward(x, skip)\n",
        "\n",
        "    def _forward(self, x, skip=None):\n",
        "        if self.skip_linear is not None:\n",
        "            x = self.skip_linear(torch.cat([x, skip], dim=-1))\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H % self.patch_size == 0 and W % self.patch_size == 0\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "                 qkv_bias=False, qk_scale=None, norm_layer=nn.LayerNorm, mlp_time_embed=False, num_classes=-1,\n",
        "                 use_checkpoint=False, conv=True, skip=True):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.num_classes = num_classes\n",
        "        self.in_chans = in_chans\n",
        "\n",
        "        self.patch_embed = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "        ) if mlp_time_embed else nn.Identity()\n",
        "\n",
        "        if self.num_classes > 0:\n",
        "            self.label_emb = nn.Embedding(self.num_classes, embed_dim)\n",
        "            self.extras = 2\n",
        "        else:\n",
        "            self.extras = 1\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.extras + num_patches, embed_dim))\n",
        "\n",
        "        self.in_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                norm_layer=norm_layer, use_checkpoint=use_checkpoint)\n",
        "            for _ in range(depth // 2)])\n",
        "\n",
        "        self.mid_block = Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                norm_layer=norm_layer, use_checkpoint=use_checkpoint)\n",
        "\n",
        "        self.out_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                norm_layer=norm_layer, skip=skip, use_checkpoint=use_checkpoint)\n",
        "            for _ in range(depth // 2)])\n",
        "\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        self.patch_dim = patch_size ** 2 * in_chans\n",
        "        self.decoder_pred = nn.Linear(embed_dim, self.patch_dim, bias=True)\n",
        "        self.final_layer = nn.Conv2d(self.in_chans, self.in_chans, 3, padding=1) if conv else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed'}\n",
        "\n",
        "    def forward(self, x, timesteps, y=None):\n",
        "        x = self.patch_embed(x)\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        time_token = self.time_embed(timestep_embedding(timesteps, self.embed_dim))\n",
        "        time_token = time_token.unsqueeze(dim=1)\n",
        "        x = torch.cat((time_token, x), dim=1)\n",
        "        if y is not None:\n",
        "            label_emb = self.label_emb(y)\n",
        "            label_emb = label_emb.unsqueeze(dim=1)\n",
        "            x = torch.cat((label_emb, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        skips = []\n",
        "        for blk in self.in_blocks:\n",
        "            x = blk(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        x = self.mid_block(x)\n",
        "\n",
        "        for blk in self.out_blocks:\n",
        "            x = blk(x, skips.pop())\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.decoder_pred(x)\n",
        "        assert x.size(1) == self.extras + L\n",
        "        x = x[:, self.extras:, :]\n",
        "        x = unpatchify(x, self.in_chans)\n",
        "        x = self.final_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G5l1b6dUu6MJ"
      },
      "outputs": [],
      "source": [
        "# @title U-DiT\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = self.layer(x) + x # down_shortcut\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "\n",
        "# conv res pixeldown\n",
        "\n",
        "class DownSample_Attn(nn.Module):\n",
        "    def __init__(self, dim, num_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = num_heads\n",
        "        self.d_head = dim//num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.r = r\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(head_dim, seq_len=512, base=10000)\n",
        "        self.scale = head_dim**-.5 # v1\n",
        "        # v2\n",
        "        # self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
        "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1))), requires_grad=True)\n",
        "        # self.logit_scale = nn.Parameter(10 * torch.ones((num_heads, 1, 1)))\n",
        "\n",
        "        self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # b, _, h, w = x.size()\n",
        "        x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # attn = (q @ k.transpose(-2, -1)) * self.scale # v1 attention\n",
        "        # attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)) * torch.clamp(self.logit_scale, max=4.6052).exp() # v2 attention\n",
        "        # # attn = attn * torch.clamp(self.logit_scale, max=100)\n",
        "        # x = attn.softmax(dim=-1) @ v\n",
        "\n",
        "        x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, dim, ff_mult=1):\n",
        "#         super().__init__()\n",
        "#         d_model = dim*ff_mult\n",
        "#         self.project_in = nn.Sequential(nn.Conv2d(dim, d_model, kernel_size=1), nn.GELU())\n",
        "#         self.dwconv = nn.ModuleList([\n",
        "#             # nn.Conv2d(d_model, d_model, 5, 1, 5//2, groups=d_model),\n",
        "#             nn.Conv2d(d_model, d_model, 3, 1, 3//2, groups=d_model),\n",
        "#             nn.Conv2d(d_model, d_model, 1, 1, 1//2, groups=d_model),\n",
        "#         ])\n",
        "#         self.project_out = nn.Sequential(nn.Conv2d(d_model, d_model//2, kernel_size=1),\n",
        "#             nn.Conv2d(d_model//2, dim, kernel_size=1))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.project_in(x) # no\n",
        "#         x = x + sum([conv(x) for conv in self.dwconv])\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class SEBlock(nn.Module): # https://github.com/DingXiaoH/RepVGG/blob/main/se_block.py#L7\n",
        "#     def __init__(self, in_ch, dim=None):\n",
        "#         super().__init__()\n",
        "#         dim = dim or in_ch//4\n",
        "#         self.se = nn.Sequential( # [b,c,h,w] -> [b,c,1,1]\n",
        "#             nn.AdaptiveAvgPool2d((1,1)),\n",
        "#             nn.Conv2d(in_ch, dim, 1, 1), nn.ReLU(),\n",
        "#             nn.Conv2d(dim, in_ch, 1, 1), nn.Sigmoid(),\n",
        "#         )\n",
        "#     def forward(self, x): # [b,c,h,w]\n",
        "#         return x * self.se(x) # [b,c,h,w]\n",
        "\n",
        "# # # https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py\n",
        "# # # https://github.com/DingXiaoH/RepVGG/blob/main/repvggplus.py#L28\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, in_ch, out_ch=None, ff_mult=1):\n",
        "#         super().__init__()\n",
        "#         # d_model = dim*ff_mult\n",
        "#         out_ch = out_ch or in_ch\n",
        "#         self.dwconv = nn.ModuleList([\n",
        "#             nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, 1, 3//2), nn.BatchNorm2d(out_ch)),\n",
        "#             nn.Sequential(nn.Conv2d(in_ch, out_ch, 1, 1, 1//2), nn.BatchNorm2d(out_ch)),\n",
        "#         ])\n",
        "#         self.project_out = nn.Sequential(nn.GELU(), SEBlock(out_ch, out_ch//4)) # act, SEblock\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + sum([conv(x) for conv in self.dwconv])\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "# # me\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_mult=1):\n",
        "        super().__init__()\n",
        "        # d_model = dim*ff_mult\n",
        "        self.project_in = nn.Sequential(nn.BatchNorm2d(d_model), nn.GELU())\n",
        "        self.dwconv = nn.ModuleList([\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 3//2),\n",
        "            nn.Conv2d(d_model, d_model, 1, 1, 1//2),\n",
        "        ])\n",
        "        # self.project_out = nn.Sequential(nn.GELU(), nn.Conv2d(d_model, dim, kernel_size=1)) # act, SEblock\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.project_in(x) # no\n",
        "        x = x + sum([conv(h) for conv in self.dwconv])\n",
        "        # x = self.project_out(out)\n",
        "        return x\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, hidden_size, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.rot_emb = RotEmb(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden_size), nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        # nn.init.normal_(self.mlp.weight, std=0.02)\n",
        "        self.mlp.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "    def forward(self, t):\n",
        "        t_freq = self.rot_emb(t)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    \"\"\"A IPT block with adaptive layer norm zero (adaLN-Zero) conIPTioning.\"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm1 = LayerNorm2d(hidden_size)\n",
        "        self.attn = DownSample_Attn(hidden_size, num_heads=num_heads, r=down_factor)\n",
        "        # self.attn = AttentionBlock(d_model=hidden_size, d_head=hidden_size//num_heads)\n",
        "        self.norm2 = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm2 = LayerNorm2d(hidden_size)\n",
        "        self.mlp = FeedForward(hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(hidden_size, 6 * hidden_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond).chunk(6, dim=1)\n",
        "        # x = x + gate_msa[...,None,None] * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        # x = x + gate_mlp[...,None,None] * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "\n",
        "        return x\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale[...,None,None]) + shift[...,None,None]\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(hidden_size, 2*hidden_size))\n",
        "        )\n",
        "        self.norm_final = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm_final = LayerNorm2d(hidden_size)\n",
        "        self.out_proj = zero_module(nn.Conv2d(hidden_size, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = self.in_proj(x)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class UpDownsample(nn.Module):\n",
        "    def __init__(self, n_feat, r=1):\n",
        "        super().__init__()\n",
        "        if r>1: sample = nn.PixelShuffle(r)\n",
        "        elif r<1: sample = nn.PixelUnshuffle(int(1/r))\n",
        "        else: sample = nn.Identity()\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, int(n_feat*r), 3, 1, 1, bias=False), sample)\n",
        "    def forward(self, x): return self.body(x)\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, depth=1, updown=False, red=False):\n",
        "        super().__init__()\n",
        "\n",
        "        if updown=='down': level = [\n",
        "            UpDownsample(hidden_size, r=1/2),\n",
        "            *[U_DiTBlock(hidden_size*2, num_heads) for _ in range(depth)]\n",
        "        ]\n",
        "        elif updown=='up': level = [\n",
        "            # nn.Conv2d(hidden_size, hidden_size//2, kernel_size=1),\n",
        "            # *[U_DiTBlock(hidden_size//2, num_heads) for _ in range(depth)],\n",
        "            # UpDownsample(hidden_size//2, r=2)\n",
        "            nn.Conv2d(hidden_size*2, hidden_size, kernel_size=1),\n",
        "            *[U_DiTBlock(hidden_size, num_heads) for _ in range(depth)],\n",
        "            UpDownsample(hidden_size, r=2)\n",
        "            ]\n",
        "        else: level = [\n",
        "            nn.Conv2d(hidden_size*2, hidden_size, kernel_size=1) if red else nn.Identity(),\n",
        "            *[U_DiTBlock(hidden_size, num_heads) for _ in range(depth)]\n",
        "        ]\n",
        "        self.seq = Seq(*level)\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        return self.seq(x, cond)\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_size=96, depth=[2,5,8,5,2], num_heads=16, cond_dim=16):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels #* 2 if learn_sigma else in_channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = nn.Conv2d(in_channels, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.bias, 0)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size*(1+2+4))\n",
        "        self.y_embedder = nn.Linear(cond_dim, hidden_size*(1+2+4))\n",
        "\n",
        "        self.down_list = nn.ModuleList([\n",
        "            levelBlock(hidden_size, num_heads, depth=1, updown=False),\n",
        "            levelBlock(hidden_size, num_heads, depth=1, updown='down'),\n",
        "        ])\n",
        "        self.middle_block = Seq(\n",
        "            UpDownsample(hidden_size*2, r=1/2),\n",
        "            # levelBlock(hidden_size*4, num_heads, depth=1, updown=False),\n",
        "            *[U_DiTBlock(hidden_size*4, num_heads) for _ in range(depth[2])],\n",
        "            UpDownsample(hidden_size*4, r=2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([\n",
        "            levelBlock(hidden_size*2, num_heads, depth=1, updown='up'),\n",
        "            levelBlock(hidden_size*1, num_heads, depth=1, updown=False, red=True),\n",
        "        ])\n",
        "\n",
        "        self.final_layer = FinalLayer(hidden_size, self.out_channels)\n",
        "\n",
        "        self.apply(self._basic_init)\n",
        "    def _basic_init(self, module):\n",
        "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        x = self.x_embedder(x) # (N, C, H, W)\n",
        "        c123 = self.t_embedder(t) + self.y_embedder(y)\n",
        "        cond = c123.split([self.hidden_size, self.hidden_size*2, self.hidden_size*4], dim=1)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, cond[i])\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, cond[-1])\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond[-2-i]) # x = up(x, blocks[-i - 1])\n",
        "\n",
        "        x = self.final_layer(x, cond[0]) # (N, T, patch_size ** 2 * out_channels)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs):\n",
        "#     return U_DiT(down_factor=2, hidden_size=96, num_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(hidden_size=192, num_heads=8,\n",
        "# def U_DiT_L: U_DiT(hidden_size=384, num_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_channels=3, hidden_size=16, num_heads=4, depth=[2,3,3,3,2], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "# model(inputs, t, y)\n",
        "# out = model(inputs, t, y)\n",
        "# print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SlxB9mFM6abU"
      },
      "outputs": [],
      "source": [
        "# @title unet me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim, cond_dim, n_head=None, d_head=8, updown=False, r=2):\n",
        "        super().__init__()\n",
        "        if updown=='down': in_ch = in_ch*r**2\n",
        "        elif updown=='up': out_ch = out_ch*r**2\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "\n",
        "        tok_dim = d_model\n",
        "        # if tok_dim == None: tok_dim = d_model\n",
        "        self.num_time_tokens = 2\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            RotEmb(emb_dim, top=torch.pi, base=10000),\n",
        "            nn.Linear(emb_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + self.num_time_tokens * tok_dim),\n",
        "        )\n",
        "        self.emb_dim, self.tok_dim = emb_dim, tok_dim\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(cond_dim), nn.Linear(cond_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + tok_dim)\n",
        "        )\n",
        "        self.norm_cond = nn.LayerNorm(tok_dim)\n",
        "        cond_dim = tok_dim\n",
        "\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1))\n",
        "        # self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1), act)\n",
        "        # self.init_conv = CrossEmbedLayer(in_ch, dim_out=d_model, kernel_sizes=(3, 7, 15), stride=1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7//2)\n",
        "\n",
        "        dim_mults = [1,2,3,4,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in dim_mults] # [128, 256, 384, 512]\n",
        "        # in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        # for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = ch_list[-1]*2**2 # 512\n",
        "        self.middle_block = Seq(\n",
        "            nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "        # for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        # t_emb = self.rotemb(t)\n",
        "        # emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        t_hid, t_tok = self.time_mlp(t).split([self.emb_dim, self.num_time_tokens * self.tok_dim], dim=-1)\n",
        "        t_tok = t_tok.reshape(t_tok.shape[0], self.num_time_tokens, self.tok_dim)\n",
        "        c_hid, c_tok = self.cond_mlp(cond).split([self.emb_dim, self.tok_dim], dim=-1)\n",
        "        # print('unet fwd', t_hid.shape, t_tok.shape, t.shape)\n",
        "\n",
        "        emb = t_hid + c_hid # [batch, emb_dim]\n",
        "        cond = torch.cat((t_tok, c_tok.unsqueeze(1)), dim=-2) # [b, num_toks, tok_dim]\n",
        "        cond = self.norm_cond(cond)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# x=torch.rand((batch,3,16,16),device=device)\n",
        "x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# print(t)\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "# [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "print(x.shape,t.shape,cond.shape)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EkVBxwyjTpM-"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_list=[32, 64], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            PixelShuffleConv(in_ch, d_list[0], k_list[0], r=1/2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.Dropout2d(drop), PixelShuffleConv(d_list[0], d_list[1], k_list[1], r=1/2), act,\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x) # [batch, 3,64,64] -> [batch, c,h,w]\n",
        "\n",
        "class Deconv(nn.Module):\n",
        "    def __init__(self, out_ch=3, d_list=[32, 64], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            PixelShuffleConv(d_list[1], d_list[0], k_list[1], r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            PixelShuffleConv(d_list[0], out_ch, k_list[0], r=2),\n",
        "        )\n",
        "    def forward(self, x): return self.deconv(x) # [batch, c,h,w] -> [batch, 3,64,64]\n",
        "\n",
        "\n",
        "class PixelAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=256, out_ch=None, kernels=[7,5], mult=[1]):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.d_model, self.out_ch = in_ch, d_model, out_ch\n",
        "        d_list=[d_model*m for m in mult]\n",
        "        in_list, out_list = [in_ch, *d_list[:-1]], [*d_list[:-1], out_ch]\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=1/2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act,) for i, (in_dim, out_dim, kernel) in enumerate(zip(in_list, out_list, kernels))], # conv,norm,act except for last layer: no norm\n",
        "            # PixelShuffleConvDown(in_ch, d_list[0], 7, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConvDown(d_list[0], d_list[1], 5, r=2), act,\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), *(nn.BatchNorm2d(out_dim), act) if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # PixelShuffleConvUp(d_list[1], d_list[0], 5, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConvUp(d_list[0], in_ch, 7, r=2), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x): return self.encoder(x)\n",
        "    def decode(self, x): return self.decoder(x)\n",
        "\n",
        "\n",
        "# in_ch=3\n",
        "# model_ch=16\n",
        "# d_list=[16, 16] # [16,32]\n",
        "# k_list=[7,5] # [7,5]\n",
        "# conv = Conv(in_ch=in_ch, d_list=d_list, k_list=k_list, act=nn.ReLU()) # ReLU GELU SiLU\n",
        "\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "# input = torch.rand((4,3,64,64), device=device)\n",
        "# enc = conv(input)\n",
        "# print(enc.shape)\n",
        "# out = deconv(enc)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GMFVOgT4sJ-y"
      },
      "outputs": [],
      "source": [
        "# @title stable diffusion unet next\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1) # og\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding=1), act)\n",
        "\n",
        "    def forward(self, x): # [N,C,...]\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\") # if self.conv_dim == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        x = self.conv(x) # optional\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.op = nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1) # optional # stride = 2 if conv_dim != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        self.op = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1), act)\n",
        "        # self.op = avg_pool_nd(conv_dim, kernel_size=stride, stride=stride) # alternative\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial]\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, temb_dim, out_ch=None, scale_shift=False, updown=False, drop=0.):\n",
        "        super().__init__()\n",
        "        self.temb_dim = temb_dim # number of timestep embedding channels\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch = in_ch, out_ch\n",
        "        self.scale_shift = scale_shift\n",
        "        if updown=='up': self.h_upd, self.x_upd = Upsample(in_ch), Upsample(in_ch)\n",
        "        elif updown=='down': self.h_upd, self.x_upd = Downsample(in_ch), Downsample(in_ch)\n",
        "        else: self.h_upd = self.x_upd = nn.Identity()\n",
        "        self.in_layers = nn.Sequential(nn.BatchNorm2d(in_ch), nn.SiLU(), self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1),) # zero\n",
        "        # self.in_layers = nn.Sequential(self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1)) # no bn before FiLM\n",
        "        # self.in_layers = nn.Sequential(self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.SiLU(), self.h_upd, nn.Conv2d(out_ch, out_ch, 3, padding=1)) # no bn before FiLM\n",
        "\n",
        "        # Conv bn film act res\n",
        "        self.emb_layers = nn.Sequential(nn.SiLU(), nn.Linear(temb_dim, 2 * out_ch if scale_shift else out_ch),)\n",
        "        # self.emb_layers = nn.Sequential(nn.Linear(temb_dim, out_ch), nn.SiLU(), nn.Linear(out_ch, 2 * out_ch if scale_shift else out_ch),)\n",
        "        self.out_layers = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_ch), nn.SiLU(), nn.Dropout(drop), nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            # nn.BatchNorm2d(out_ch), nn.SiLU(), nn.Dropout(drop), zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if out_ch == in_ch: self.skip = nn.Identity() # no need to change chanels\n",
        "        else:\n",
        "            # self.skip = nn.Conv2d(in_ch, out_ch, 3, padding=1) # spatial convolution to change the channels in the skip connection\n",
        "            self.skip = nn.Conv2d(in_ch, out_ch, 1) # smaller 1x1 convolution to change the channels in the skip connection\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, temb_dim]\n",
        "        # print(\"res fwd x\", x.shape, self.in_ch, self.out_ch)\n",
        "        h = self.in_layers(x) # norm, act, h_upd, conv\n",
        "        x = self.x_upd(x)\n",
        "        emb_out = self.emb_layers(emb) # act, lin\n",
        "        # print(\"res fwd h emb_out\", h.shape, emb_out.shape)\n",
        "        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]\n",
        "        if self.scale_shift: # FiLM\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return h + self.skip(x) # [N, C, ...]\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, updown=False, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            # Downsample(in_ch, out_ch) if updown=='down' else nn.Identity(),\n",
        "            Downsample(in_ch) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch),\n",
        "            # SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            Upsample(out_ch) if updown=='up' else nn.Identity(),\n",
        "            # Upsample(in_ch, out_ch) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, model_ch=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.model_ch = model_ch # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = model_ch // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(model_ch)\n",
        "        temb_dim = model_ch# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(model_ch, temb_dim), nn.SiLU(), nn.Linear(temb_dim, temb_dim))\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1))\n",
        "        # self.in_block = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1), act)\n",
        "\n",
        "        ch_list = [model_ch*2**i for i in range(depth+1)] # [32, 64, 128, 256]\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = 2*ch_list[-1] # 512\n",
        "        self.middle_block = Seq(\n",
        "            Downsample(ch_list[-1]),\n",
        "            ResBlock(ch_list[-1], temb_dim, ch),\n",
        "            # SpatialTransformer(ch, ch//d_head, d_head, cond_dim=cond_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, temb_dim, ch_list[-1]),\n",
        "            Upsample(ch_list[-1]),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(model_ch), nn.SiLU(), nn.Conv2d(model_ch, out_ch, 3, padding=1)) # zero\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        # t_emb = timestep_embedding(t, self.model_ch, repeat_only=False)\n",
        "        emb = self.time_emb(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, temb_dim)\n",
        "        blocks = []\n",
        "        x = self.in_block(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, model_ch=16, cond_dim=cond_dim, depth=4).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# # x=torch.rand((batch,3,16,16),device=device)\n",
        "# x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# # print(t)\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# # [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "# print(x.shape,t.shape,cond.shape)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# optim = torch.optim.SGD(model.parameters(), lr=1e-4) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F5znfg9g-BOQ"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChanRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.gamma = nn.Parameter(torch.ones(dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = 1) * self.scale * self.gamma\n",
        "\n",
        "\n",
        "class Parallel(nn.Module):\n",
        "    def __init__(self, *fns):\n",
        "        super().__init__()\n",
        "        self.fns = nn.ModuleList(fns)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [fn(x) for fn in self.fns]\n",
        "        return sum(outputs)\n",
        "\n",
        "class GlobalContext(nn.Module): # Global Context (GC) block\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b, c, h, w]\n",
        "        context = self.to_k(x)\n",
        "        x, context = x.flatten(2), context.flatten(2) # [b, c, h*w] ,[b,1,h*w]\n",
        "        out = x @ context.softmax(dim=-1).transpose(1,2) # [b, c, 1]\n",
        "        out = out.unsqueeze(-1) # [b, c, 1, 1]\n",
        "        return self.net(out) # [b, dim_out, 1, 1]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, d_head = 64,n_heads = 8, context_dim = None, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_head\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, 1, d_head))\n",
        "        self.qkv = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, d_model + 2*d_head, bias=False))\n",
        "        self.q_scale, self.k_scale = nn.Parameter(torch.ones(d_head)), nn.Parameter(torch.ones(d_head))\n",
        "\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, 2*d_head)) if context_dim!=None else None\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_model, dim, bias = False), nn.LayerNorm(dim),)\n",
        "\n",
        "    def forward(self, x, cond = None, mask = None, attn_bias = None): # [batch, T, d_model] = [b, hw, c], [batch, num_tok, context_dim]\n",
        "        batch, T, _ = x.shape#[0]\n",
        "        q, k, v = self.qkv(x).split([self.d_model, self.d_head, self.d_head], dim=-1) # [batch, T, d_model], 2*[batch, T, d_head]\n",
        "        q = q.reshape(batch, T, self.n_heads, -1).transpose(1,2) # [batch, n_heads, T, d_head]\n",
        "\n",
        "        nk, nv = self.null_kv.expand((batch,-1,-1,-1)).unbind(dim=1) # [2,1,d_head]->[batch,2,1,d_head]->[batch,1,d_head]\n",
        "        k, v = torch.cat((nk, k), dim=-2), torch.cat((nv, v), dim=-2) # [batch, 1+T, d_head]\n",
        "\n",
        "        if self.to_context!=None:\n",
        "            ck, cv = self.to_context(cond).chunk(2, dim = -1) # [batch, num_tok, d_head]\n",
        "            k, v = torch.cat((ck, k), dim=-2), torch.cat((cv, v), dim=-2) # [batch, num_tok+1+T, d_head]\n",
        "\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1) # qk rmsnorm\n",
        "        q, k = q * self.q_scale, k * self.k_scale\n",
        "        k, v = k.unsqueeze(1), v.unsqueeze(1) # [batch, 1, num_tok+1+T, d_head]\n",
        "\n",
        "        sim = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T, num_tok+1+T]\n",
        "        if attn_bias!=None: sim = sim + attn_bias # relative positional encoding (T5 style)\n",
        "        if mask!=None:\n",
        "            mask = F.pad(mask, (1, 0), value=True).unsqueeze(1).unsqeeze(2) # [b j] -> [b 1 1 j]\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "        attn = sim.softmax(dim=-1) # attn = sim.softmax(dim=-1, dtype=torch.float32).to(sim.dtype)\n",
        "        out = (attn @ v).transpose(1,2).flatten(2) # [batch, n_heads, T, d_head] -> [batch, T, d_model]\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, context_dim = None, d_head = 64,n_heads = 8, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.n_heads =n_heads\n",
        "        d_model = d_head *n_heads\n",
        "        if context_dim==None: context_dim = dim\n",
        "        self.to_q = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, d_model, bias=False))\n",
        "        self.to_kv = nn.Linear(context_dim, d_model * 2, bias=False)\n",
        "        # self.to_kv = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, d_model * 2, bias=False))\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, 1, 1, d_head))\n",
        "        self.q_scale, self.k_scale = nn.Parameter(torch.ones(d_head)), nn.Parameter(torch.ones(d_head))\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_model, dim, bias = False), nn.LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x, cond, mask = None): # [batch, T, dim]=[b, hw, c], [batch, num_tok, context_dim]\n",
        "        batch, T, _ = x.shape#[0]\n",
        "        _, num_tok, _ = cond.shape\n",
        "        q = self.to_q(x).reshape(batch, T, self.n_heads, -1).transpose(1,2) # [batch, n_heads, T, d_head]\n",
        "        k, v = self.to_kv(cond).chunk(2, dim = -1) # [batch, num_tok, d_model]\n",
        "        k = k.reshape(batch, num_tok, self.n_heads, -1).transpose(1,2) # [batch, n_heads, num_tok, d_head]\n",
        "        v = v.reshape(batch, num_tok, self.n_heads, -1).transpose(1,2) # [batch, n_heads, num_tok, d_head]\n",
        "\n",
        "        nk, nv = self.null_kv.expand((batch,-1,self.n_heads,-1,-1)).unbind(dim=1) # [2,1,1,d_head]->[batch,2,n_heads,1,d_head]->[batch,n_heads,1,d_head]\n",
        "        k, v = torch.cat((nk, k), dim=-2), torch.cat((nv, v), dim=-2) # [batch, n_heads, 1+num_tok, d_head]\n",
        "\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1) # qk rmsnorm\n",
        "        q, k = q * self.q_scale, k * self.k_scale\n",
        "\n",
        "        sim = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T, 1+num_tok]\n",
        "        if mask!=None:\n",
        "            mask = F.pad(mask, (1, 0), value=True).unsqueeze(1).unsqeeze(2) # [b j] -> [b 1 1 j]\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "        attn = sim.softmax(dim=-1) # attn = sim.softmax(dim=-1, dtype=torch.float32).to(sim.dtype)\n",
        "        out = (attn @ v).transpose(1,2).flatten(2) # [batch, n_heads, T, d_head] -> [batch, T, d_model]\n",
        "        return self.to_out(out)\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(self, dim_in, kernel_sizes, dim_out = None, stride = 2):\n",
        "        super().__init__()\n",
        "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
        "        if dim_out==None: dim_out = dim_in\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, len(kernel_sizes))] # [64, 32]\n",
        "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)] # [64, 32, 32]\n",
        "        # 1/2 + 1/4 + 1/8 + ... + 1/2^num_kernels + 1/2^num_kernels of dim_out; smaller kernel allocated more channels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(dim_in, dim_scale, kernel, stride=stride, padding=(kernel-stride)//2) for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "\n",
        "\n",
        "def Upsample(in_ch, out_ch=None):\n",
        "    if out_ch==None: out_ch = in_ch\n",
        "    return nn.Sequential(nn.Interpolate(scale_factor = 2, mode = 'nearest'), nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "\n",
        "class PixelShuffleUpsample(nn.Module):\n",
        "    \"\"\"code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\"\"\"\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * 4, 1), nn.SiLU(), nn.PixelShuffle(2)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        self.init_conv_(self.net[0])\n",
        "\n",
        "    def init_conv_(self, conv):\n",
        "        o, i, h, w = conv.weight.shape\n",
        "        conv_weight = torch.empty(o//4, i, h, w)\n",
        "        nn.init.kaiming_uniform_(conv_weight)\n",
        "        conv_weight = conv_weight.repeat(4,1,1,1)\n",
        "        conv.weight.data.copy_(conv_weight)\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def Downsample(in_ch, out_ch=None): # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample, named SP-conv in the paper, but basically a pixel unshuffle\n",
        "    if out_ch==None: out_ch = in_ch\n",
        "    return nn.Sequential(nn.PixelUnshuffle(2), nn.Conv2d(in_ch * 4, out_ch, 1)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "\n",
        "\n",
        "def FeedForward(dim, mult = 2):\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim), nn.Linear(dim, hidden_dim, bias = False), nn.GELU(),\n",
        "        nn.LayerNorm(hidden_dim), nn.Linear(hidden_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, depth = 1,n_heads = 8, d_head = 32, ff_mult = 2, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim,n_heads =n_heads, d_head = d_head, context_dim = context_dim),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, cond = None):\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2) # [b, c, h, w] -> [b, h*w, c]\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, cond) + x\n",
        "            x = ff(x) + x\n",
        "        x = x.transpose(1, 2).reshape(*bchw) # [b, h*w, c] -> [b, c, h, w]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, tok_dim = None, emb_dim = None,n_heads=None, d_head=None):\n",
        "        super().__init__()\n",
        "        if tok_dim != None: self.cross_attn = CrossAttention(dim = dim_out, context_dim = tok_dim, n_heads=n_heads, d_head=d_head) # CrossAttention LinearCrossAttention\n",
        "        else: self.cross_attn = None\n",
        "        self.block1 = nn.Sequential(ChanRMSNorm(dim), nn.SiLU(), nn.Conv2d(dim, dim_out, 3, padding = 1))\n",
        "        self.block2 = Seq(ChanRMSNorm(dim_out), scale_shift(dim_out, emb_dim) if emb_dim != None else nn.Identity(), nn.SiLU(), nn.Conv2d(dim_out, dim_out, 3, padding = 1))\n",
        "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb = None, cond = None):\n",
        "        h = self.block1(x)\n",
        "        # print('ResnetBlock fwd', h.shape)\n",
        "        if self.cross_attn != None:\n",
        "            bhwc = h.shape\n",
        "            h = h.flatten(2).transpose(1, 2) # [b, h, w, c] -> [b, h*w, c]\n",
        "            h = self.cross_attn(h, cond=cond) + h\n",
        "            h = h.transpose(1, 2).reshape(*bhwc) # [b, h*w, c] -> [b, c, h, w]\n",
        "        h = self.block2(h, emb)\n",
        "        h = h * self.gca(h) # use_gca\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb = None): # [b,c,h,w], [b,emb_dim]\n",
        "        # print('scale_shift fwd', x.shape, emb.shape)\n",
        "        emb = self.time_mlp(emb)[..., None, None] # [b,t_dim] -> [b,2*x_dim,1,1]\n",
        "        scale, shift = emb.chunk(2, dim = 1) # [b,x_dim,1,1]\n",
        "        x = x * (scale + 1) + shift\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KpM1jlvFvD9i"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen next\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, d_model, # 512 # base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
        "        c_dim = 16, # cond vec dim\n",
        "        tok_dim = None, # token dim\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8), # (1, 2, 3, 4)\n",
        "        in_ch = 3, out_ch = None,\n",
        "        layer_attns = True, # (False, True, True, True)\n",
        "        layer_cross_attns = True, # (False, True, True, True)\n",
        "        # num_resnet_blocks = 1, # 3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        if out_ch == None: self.out_ch = in_ch\n",
        "        d_head = 64\n",
        "        n_heads = 8 # 8 # ideally at least 4 or 8\n",
        "\n",
        "        self.init_conv = CrossEmbedLayer(in_ch, dim_out = d_model, kernel_sizes = (3, 7, 15), stride = 1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7 // 2)\n",
        "        dims = [d_model] + [d_model * m for m in dim_mults] # [128, 128, 256, 384, 512]\n",
        "        # dims = [d_model * m for m in dim_mults] # [128, 256, 384, 512]\n",
        "\n",
        "        # time conditioning\n",
        "        if tok_dim == None: tok_dim = d_model\n",
        "\n",
        "        # embedding time for log(snr) noise from continuous version\n",
        "        emb_dim = 16\n",
        "        emb_dim = d_model * 4 #* (2 if lowres_cond else 1)\n",
        "\n",
        "        # pos_emb = RotEmb(emb_dim, top=torch.pi, base=10000)\n",
        "        # self.time_hiddens = nn.Sequential(pos_emb, nn.Linear(emb_dim, emb_dim), nn.SiLU())\n",
        "\n",
        "        # self.time_cond = nn.Sequential(nn.Linear(emb_dim, emb_dim))\n",
        "        # num_time_tokens = 2\n",
        "        # self.time_tokens = nn.Sequential(nn.Linear(emb_dim, tok_dim * num_time_tokens), Rearrange('b (r d) -> b r d', r = num_time_tokens))\n",
        "        # # self.time_tokens = nn.Sequential(nn.Linear(emb_dim, tok_dim))\n",
        "\n",
        "        self.num_time_tokens = 2\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            RotEmb(emb_dim, top=torch.pi, base=10000),\n",
        "            nn.Linear(emb_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + self.num_time_tokens * tok_dim),\n",
        "        )\n",
        "        self.emb_dim, self.tok_dim = emb_dim, tok_dim\n",
        "\n",
        "\n",
        "        # self.cond_tokens = nn.Linear(c_dim, tok_dim)\n",
        "        # self.cond_cond = nn.Sequential(\n",
        "        #     nn.LayerNorm(tok_dim), nn.Linear(tok_dim, emb_dim), nn.SiLU(),\n",
        "        #     nn.Linear(emb_dim, emb_dim)\n",
        "        # )\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.Linear(c_dim, tok_dim),\n",
        "            nn.LayerNorm(tok_dim), nn.Linear(tok_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + tok_dim)\n",
        "        )\n",
        "\n",
        "        self.norm_cond = nn.LayerNorm(tok_dim)\n",
        "\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        num_layers = len(in_out)\n",
        "        # num_layers = len(dims)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        # skip_connect_dims = [] # keep track of skip connection dimensions\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_layers - 1)\n",
        "            # print('down', dim_in, dim_out)\n",
        "\n",
        "            # skip_connect_dims.append(dim_in) # dim_out if memory_efficient\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                # Downsample(dim_in, dim_out), # memory_efficient pre_downsample; self.downs all dim_out # Downsample cross_embed_downsample CrossEmbedLayer(dim_in, dim_out, kernel_sizes = (2, 4))\n",
        "                ResnetBlock(dim_in, dim_in, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "                nn.ModuleList([ResnetBlock(dim_in, dim_in, emb_dim = emb_dim) for _ in range(0)]),\n",
        "                TransformerBlock(dim = dim_in, depth = 1, ff_mult = 2, context_dim = tok_dim, n_heads=n_heads, d_head=d_head), # trans/ lintrans/ id\n",
        "                Downsample(dim_in, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1)) # Downsample cross_embed_downsample CrossEmbedLayer(dim_in, dim_out, kernel_sizes = (2, 4))\n",
        "            ]))\n",
        "\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block = Seq(\n",
        "            ResnetBlock(mid_dim, mid_dim, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "            TransformerBlock(mid_dim, depth = 1, n_heads=n_heads, d_head=d_head), # True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "            ResnetBlock(mid_dim, mid_dim, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "        )\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (num_layers - 1)\n",
        "            # skip_connect_dim = skip_connect_dims.pop()\n",
        "            # print('up', dim_in, dim_out, skip_connect_dim)\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                ResnetBlock(dim_out + dim_in, dim_out, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "                nn.ModuleList([ResnetBlock(dim_out + dim_in, dim_out, emb_dim = emb_dim) for _ in range(0)]),\n",
        "                TransformerBlock(dim = dim_out, depth = 1, ff_mult = 2, context_dim = tok_dim, n_heads=n_heads, d_head=d_head), # trans/ lintrans/ id\n",
        "                PixelShuffleUpsample(dim_out, dim_in) if not is_last else nn.Identity() # PixelShuffleUpsample Upsample ; memory_efficient upscale at last too\n",
        "            ]))\n",
        "\n",
        "        self.final_res_block = ResnetBlock(d_model, d_model, emb_dim = emb_dim) #if final_resnet_block else None\n",
        "        self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3 // 2)\n",
        "        def zero_init_(m):\n",
        "            nn.init.zeros_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "        zero_init_(self.final_conv)\n",
        "\n",
        "    def forward(self, x, time, cond):\n",
        "        # cond_images = resize_image_to(cond_images, x.shape[-1], mode = 'nearest')\n",
        "        # x = torch.cat((cond_images, x), dim = 1)\n",
        "        x = self.init_conv(x)\n",
        "        # hiddens.append(x)\n",
        "\n",
        "        t, t_tok = self.time_mlp(time).split([self.emb_dim, self.num_time_tokens * self.tok_dim], dim=-1)\n",
        "        t_tok = t_tok.reshape(t_tok.shape[0], self.num_time_tokens, self.tok_dim)\n",
        "        cond_hid, c_tok = self.cond_mlp(cond).split([self.emb_dim, self.tok_dim], dim=-1)\n",
        "        # print('unet fwd', t_hid.shape, t_tok.shape, t.shape)\n",
        "\n",
        "        t = t + cond_hid # [batch, emb_dim]\n",
        "        c = torch.cat((t_tok, c_tok.unsqueeze(1)), dim=-2) # [b, num_toks, tok_dim]\n",
        "        c = self.norm_cond(c)\n",
        "\n",
        "        hiddens = []\n",
        "        # for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "        for init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "            # x = pre_downsample(x)\n",
        "            x = init_block(x, t, c)\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = resnet_block(x, t)\n",
        "                hiddens.append(x)\n",
        "            x = attn_block(x, c)\n",
        "            hiddens.append(x)\n",
        "            x = post_downsample(x)\n",
        "\n",
        "        # print('unet fwd', x.shape, t.shape, c.shape)\n",
        "        x = self.mid_block(x, t, c)\n",
        "\n",
        "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
        "            x = torch.cat((x, hiddens.pop() * 2**-.5), dim = 1)\n",
        "            x = init_block(x, t, c)\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = torch.cat((x, hiddens.pop() * 2**-.5), dim = 1)\n",
        "                x = resnet_block(x, t)\n",
        "            x = attn_block(x, c)\n",
        "            x = upsample(x)\n",
        "\n",
        "        # x = torch.cat((x, hiddens.pop()), dim = 1)\n",
        "        x = self.final_res_block(x, t)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "model = UNet(d_model=128, c_dim=10, in_ch=1, dim_mults = (1, 2, 3, 4)).to(device)\n",
        "batch = 1\n",
        "# x = torch.rand((batch, 3, 64, 64), device = device)\n",
        "x = torch.rand((batch, 1,16,16), device = device)\n",
        "t = torch.rand(batch, device = device)\n",
        "# img_cond = torch.rand((batch, 512, 64, 64), device = device)\n",
        "cond = torch.rand((batch, 10), device = device)\n",
        "out = model(x, t, cond)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1wCQ0gZ-Hz3i"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JYMQDoL578HQ"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module): # down shortcut [b,c,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch*r**2, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,2h,2w] -> [b,4c,h,w] -> [b,o,4c/o,h,w] -> [b,o,h,w]\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        x = self.samech(x)\n",
        "        return x\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch, out_ch*r**2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        x = self.samech(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch, out_ch*r**2)\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale r r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        # down\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        x = self.samech(x)\n",
        "\n",
        "        # up\n",
        "        x = self.samech(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r-cYioQer02v"
      },
      "outputs": [],
      "source": [
        "# @title CrossEmbedLayer PixelShuffleConv\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_sizes, stride=1):\n",
        "        super().__init__()\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "\n",
        "        r=2\n",
        "        balls = out_ch//r**2\n",
        "        mult = [1/(1.6**i) for i in range(len(kernel_sizes))]\n",
        "        mul = balls/sum(mult)\n",
        "        mult = [m*mul for m in mult]\n",
        "        dim_scales = [0]*len(kernel_sizes)\n",
        "        for i in range(balls):\n",
        "            ind = mult.index(max(mult))\n",
        "            dim_scales[ind] += 1\n",
        "            mult[ind] -= 1\n",
        "        dim_scales = [d*r**2 for d in dim_scales]\n",
        "        if 0 in dim_scales: print('dim_scales',dim_scales)\n",
        "        # 1/2 + 1/4 + 1/8 + ... + 1/2^num_kernels + 1/2^num_kernels of out_ch; smaller kernel allocated more channels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(in_ch, dim_scale, kernel, stride=stride, padding=(kernel-stride)//2) for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "        out = torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "        b,c,h,w = out.shape\n",
        "        out = out.reshape(b, -1, 4, h, w).transpose(1,2).reshape(b, c, h, w)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch = None, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch, self.r = in_ch, out_ch, r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        self.net.apply(self.init_conv_)\n",
        "\n",
        "    def init_conv_(self, conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "        if isinstance(conv, nn.Conv2d):\n",
        "            o, i, h, w = conv.weight.shape\n",
        "            conv_weight = torch.empty(self.out_ch, self.in_ch, h, w)\n",
        "            nn.init.kaiming_uniform_(conv_weight)\n",
        "            # print(conv.weight.shape, conv_weight.shape,max(self.r, int(1/self.r)), (0 if self.r>1 else 1))\n",
        "            conv.weight.data.copy_(conv_weight.repeat_interleave(max(self.r, int(1/self.r))**2, dim=(0 if self.r>1 else 1)))\n",
        "            nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# d=PixelShuffleConv(3, 16, 7, r=1/2)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = PixelShuffleConv(in_ch=3, r=2).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "# input = torch.rand((4,3,64,64), device=device)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n",
        "\n",
        "# model = PixelShuffleConv(in_ch=3, r=1/2).to(device)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}